{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "origX = pd.read_csv(\"../Datasets/train_x.csv\", sep=\",\", header=None).as_matrix().reshape(-1,64,64)\n",
    "origY = pd.read_csv(\"../Datasets/train_y.csv\", sep=\",\", header=None).as_matrix() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 64, 64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#origX = origX.reshape(-1, 4096) # reshape \n",
    "#origY = origY.reshape(-1)\n",
    "origX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_tresholding(image):\n",
    "    for i in range(len(image)):\n",
    "        for j in range(len(image)):\n",
    "            if image[i, j] < 254:\n",
    "                image[i, j] = 0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binarize all train images\n",
    "images_train = np.zeros(origX.shape)\n",
    "for i in range(origX.shape[0]):\n",
    "    images_train[i] = binary_tresholding(origX[i])\n",
    "    \n",
    "#Binarize all test images\n",
    "images_test = np.zeros(origTestX.shape)\n",
    "for i in range(origTestX.shape[0]):\n",
    "    images_test[i] = binary_tresholding(origTestX[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_largest_digit_center(image):\n",
    "    thresh = binary_tresholding(image)\n",
    "    thresh = thresh.astype('uint8')\n",
    "\n",
    "    im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    Ws = [] \n",
    "    Hs = []\n",
    "    for c in contours:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        Ws.append(w)\n",
    "        Hs.append(h)\n",
    "    max_h = max(Hs)\n",
    "    max_w = max(Ws)\n",
    "    max_l = max(max_h,max_w)\n",
    "\n",
    "    # Find the index of the contour with largest width\n",
    "    if max_l == max_h:\n",
    "        max_index = np.argmax(Hs)\n",
    "    else:\n",
    "        max_index = np.argmax(Ws)\n",
    "    x,y,w,h = cv2.boundingRect(contours[max_index])\n",
    "\n",
    "    #to create a filter to remove all other digits and keep just the biggest one\n",
    "    filter_ = np.zeros(image.shape)\n",
    "    filter_ = cv2.rectangle(filter_,(x,y),(x+max_l,y+max_l),(255,255,255),-1)\n",
    "    \n",
    "    #find center of the component\n",
    "    px = x+max_l/2\n",
    "    py = y+max_l/2\n",
    "\n",
    "    preprocessed_img = np.zeros(origX[0].shape)\n",
    "    preprocessed_img = thresh*filter_\n",
    "\n",
    "    # To shift the largest component to the center of an image\n",
    "    preprocessed_img = preprocessed_img.astype('uint8')\n",
    "    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(preprocessed_img, connectivity=4)\n",
    "    sizes = stats[:, -1]\n",
    "    max_label = 1\n",
    "    max_size = sizes[1]\n",
    "    for i in range(2, nb_components):\n",
    "        if sizes[i] > max_size:\n",
    "            max_label = i\n",
    "            max_size = sizes[i]\n",
    "        \n",
    "    img2 = np.zeros(output.shape)\n",
    "    img2[output == max_label] = 255.\n",
    "    \n",
    "    # To shift the largest component to the center of an image\n",
    "    px = 32 - centroids[max_label][0]\n",
    "    py = 32 - centroids[max_label][1]\n",
    "    M = np.float32([[1,0,px],[0,1,py]])\n",
    "    dst = cv2.warpAffine(img2,M,(64,64))\n",
    "    \n",
    "    return dst[18:46,18:46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the biggest component in all dataset and shift it to center\n",
    "prepTrainX = np.zeros([50000, 784])\n",
    "for i in range(len(origX)):\n",
    "    img1 = find_the_largest_digit_center(images_train[i, :])\n",
    "    img1 = img1.reshape((1, 784))\n",
    "    prepTrainX[i, :] = img1\n",
    "    \n",
    "prepTestX = np.zeros([10000, 784])\n",
    "for i in range(len(origTestX)):\n",
    "    img2 = find_the_largest_digit_center(images_test[i, :])\n",
    "    img2 = img2.reshape((1, 784))\n",
    "    prepTestX[i, :] = img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a numpy 1-dimensional array and writes each value in a different row on the file\n",
    "def write_solution(filename, y_pred):\n",
    "    file = open(filename,\"w\")\n",
    "    file.write(\"Id,Label\" + \"\\n\")\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        file.write(repr(i) + \",\" + repr(int(y_pred[i])) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepTrainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cross validation parameter tweaking\n",
    "x_train1 = prepTrainX[10000:]\n",
    "y_train1 = origY[10000:]\n",
    "x_valid1 = prepTrainX[:10000]\n",
    "y_valid1 = origY[:10000]\n",
    "\n",
    "x_train2 = np.concatenate((prepTrainX[:10000], prepTrainX[20000:]))\n",
    "y_train2 = np.concatenate((origY[:10000], origY[20000:]))\n",
    "x_valid2 = prepTrainX[10000:20000]\n",
    "y_valid2 = origY[10000:20000]\n",
    "\n",
    "x_train3 = np.concatenate((prepTrainX[:20000], prepTrainX[30000:]))\n",
    "y_train3 = np.concatenate((origY[:20000], origY[30000:]))\n",
    "x_valid3 = prepTrainX[20000:30000]\n",
    "y_valid3 = origY[20000:30000]\n",
    "\n",
    "x_train4 = np.concatenate((prepTrainX[:30000], prepTrainX[40000:]))\n",
    "y_train4 = np.concatenate((origY[:30000], origY[40000:]))\n",
    "x_valid4 = prepTrainX[30000:40000]\n",
    "y_valid4 = origY[30000:40000]\n",
    "\n",
    "x_train5 = prepTrainX[:40000]\n",
    "y_train5 = origY[:40000]\n",
    "x_valid5 = prepTrainX[40000:]\n",
    "y_valid5 = origY[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.divide(x_train1,255.0) # normalize pixels\n",
    "x_v = np.divide(x_valid1,255.0) # normalize pixels\n",
    "y = y_train1\n",
    "y_v = y_valid1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to tweak\n",
    "batchSize = 1\n",
    "epochs = 100\n",
    "learning_rate = .005\n",
    "numHiddenLayers = 6\n",
    "hiddenLayerNodes = [2, 4, 7, 3, 8, 5]\n",
    "if len(hiddenLayerNodes) != numHiddenLayers:\n",
    "    print(\"PROBLEM IN INPUT!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights and bias\n",
    "weights = []\n",
    "biases = []\n",
    "for i in range(numHiddenLayers+1):\n",
    "    if i == 0:\n",
    "        w = np.random.randn(784, hiddenLayerNodes[i])\n",
    "        b = np.random.randn(1, hiddenLayerNodes[i])\n",
    "    elif i != numHiddenLayers:\n",
    "        w = np.random.randn(hiddenLayerNodes[i-1], hiddenLayerNodes[i])\n",
    "        b = np.random.randn(1, hiddenLayerNodes[i])\n",
    "    else:\n",
    "        w = np.random.randn(hiddenLayerNodes[i-1], 10) # 10 output nodes\n",
    "        b = np.random.randn(1, 10)\n",
    "    weights.append(w)\n",
    "    biases.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [x[k: k + batchSize] for k in range(0, len(x), batchSize)]\n",
    "yBatches = [y[k: k + batchSize] for k in range(0, len(y), batchSize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z, deriv):\n",
    "    sig = 1.0 / (1.0 + np.exp(-z))\n",
    "    if deriv:\n",
    "        return sig * (1 - sig)\n",
    "    else:\n",
    "        return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "# NN algorithm\n",
    "\n",
    "for ep in range(epochs):\n",
    "    for i in range(len(batches)):\n",
    "        xb = batches[i]\n",
    "        yb = yBatches[i]\n",
    "        \n",
    "        currentBatchSize = len(xb)\n",
    "\n",
    "        # feed forward\n",
    "        a = xb\n",
    "        As = [a] # activations (right side of node)\n",
    "        Zs = [] # (left side of node)\n",
    "        for l in range(len(weights)): # for x total layers (including input and output) this will iterate x-1 times\n",
    "            z = np.dot(a, weights[l]) + biases[l]\n",
    "            a = sigmoid(z, False)\n",
    "            Zs.append(z)\n",
    "            As.append(a)\n",
    "\n",
    "        # Back propagation\n",
    "\n",
    "        e = np.zeros((currentBatchSize, 10)) # Derivatives of quadratic deviation\n",
    "        for j in range(len(As[-1])): # j represents image index in batch\n",
    "            e[j] = deepcopy(As[-1][j]) # delta = prediction - 0 assuming label is 0, deepcopy to pass by value instead of reference\n",
    "            e[j, int(yb[j])] -= 1 # delta = prediction - 1 because label is 1 at index yb\n",
    "\n",
    "        deltas = [] # list of delta layers\n",
    "        # delta[0] represents the first layer (starting from right) of deltas for all images in batch\n",
    "        # delta[0][0, :] represents the first layer (starting from right) of deltas for the first image\n",
    "        # delta[0][0, 0] represents the delta of the first node of the first (starting from right) layer of deltas for the first image\n",
    "\n",
    "        #Calculate deltas in output layer                \n",
    "        D = [np.diag(sigmoid(Zs[-1][j], True)) for j in range(currentBatchSize)] # Derivatives stored in output layer\n",
    "        d = np.array([np.dot(D[j], e[j]) for j in range(currentBatchSize)]) # output layer deltas\n",
    "        deltas.append(d)\n",
    "\n",
    "        bGradients = [np.zeros((currentBatchSize, b.shape[1])) for b in biases]\n",
    "        bGradients[-1] = d\n",
    "        wGradients = [np.zeros(w.shape) for w in weights]\n",
    "        wGradients[-1] = -learning_rate * np.dot(As[-2].T, d)\n",
    "\n",
    "        # calculating deltas\n",
    "        for k in range(len(weights)-1, 0, -1):            \n",
    "            D = [np.diag(sigmoid(Zs[k-1][j], True)) for j in range(currentBatchSize)] # Derivatives stored in next hidden layer\n",
    "            d = np.array([np.dot(np.dot(D[j], weights[k]), d[j]) for j in range(currentBatchSize)]) # next hidden layer deltas\n",
    "            bGradients[k-1] = d\n",
    "            wGradients[k-1] = -learning_rate * np.dot(As[k-1].T, d)\n",
    "            deltas.append(d)\n",
    "\n",
    "        # bGradients[0] represents first layer\n",
    "        # bGradients[0][i, j] represents for first layer, mini batch i and bias value j\n",
    "\n",
    "        # wGradients[0] represents first layer\n",
    "        # wGradients[0][i, j] represents for first layer sum of weight edges from node i to j for all mini batches (images) in batch\n",
    "\n",
    "        # divide gradients for avg\n",
    "        wGradientsAvg = [np.zeros(w.shape) for w in weights]\n",
    "        for l in range(len(wGradients)):\n",
    "            wGradientsAvg[l] = wGradients[l] / currentBatchSize\n",
    "\n",
    "        bGradientsAvg = [np.zeros(b.shape) for b in biases]\n",
    "        for l in range(len(bGradients)):\n",
    "            bGradientsAvg[l][0,:] = sum(bGradients[l])\n",
    "            bGradientsAvg[l] /= currentBatchSize\n",
    "\n",
    "        # update weights\n",
    "        for l in range(len(weights)):\n",
    "            weights[l] += wGradientsAvg[l]\n",
    "            biases[l] += bGradientsAvg[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating training data\n",
    "\n",
    "# Last feed forward for evaluation\n",
    "a = x\n",
    "for l in range(len(weights)): # for x total layers (including input and output) this will iterate x-1 times\n",
    "    z = np.dot(a, weights[l]) + biases[l]\n",
    "    a = sigmoid(z, False)\n",
    "\n",
    "values = [np.argmax(a_i) for a_i in a]\n",
    "binaryResults = []\n",
    "for i in range(len(values)):\n",
    "    binaryResults.append(int(values[i] == y[i]))\n",
    "    \n",
    "score = sum(binaryResults) / len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(values[:100])\n",
    "print(np.unique(values))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating validation data\n",
    "\n",
    "# Last feed forward for evaluation\n",
    "a = x_v\n",
    "for l in range(len(weights)): # for x total layers (including input and output) this will iterate x-1 times\n",
    "    z = np.dot(a, weights[l]) + biases[l]\n",
    "    a = sigmoid(z, False)\n",
    "\n",
    "values = [np.argmax(a_i) for a_i in a]\n",
    "binaryResults = []\n",
    "for i in range(len(values)):\n",
    "    binaryResults.append(int(values[i] == y_v[i]))\n",
    "    \n",
    "score = sum(binaryResults) / len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(values[:100])\n",
    "print(np.unique(values))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
