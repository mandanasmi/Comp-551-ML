{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 Real life dataset\n",
    "Mandana Samiei \n",
    "ID: 260779555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from pandas import read_csv\n",
    "from numpy.linalg import inv\n",
    "from numpy import genfromtxt\n",
    "from math import exp\n",
    "import random\n",
    "%matplotlib inline  \n",
    "\n",
    "def load_data(file_name):\n",
    "    my_data = genfromtxt(file_name, delimiter=',')\n",
    "    np.random.shuffle(my_data)\n",
    "    #print my_data.shape\n",
    "    X = my_data[:,0:127]\n",
    "    #print X.shape\n",
    "    Y = my_data[:,127]\n",
    "    #print Y.shape\n",
    "    return X,Y\n",
    "\n",
    "def load_new_data(file_name):\n",
    "    my_data = genfromtxt(file_name, delimiter=',')\n",
    "    np.random.shuffle(my_data)\n",
    "    X = my_data[:,0:121]\n",
    "    Y = my_data[:,122]\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill the missing values with the mean of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_data(X):\n",
    "    no_missing_features = []\n",
    "    features_mean = []\n",
    "    print \"\\n\\n**********************************PART 1: Fill the missing values****************************\\n\\n\"\n",
    "    print \"_____________________________Features Specification________________________\"\n",
    "    for i in range(X.shape[1]):\n",
    "        print \"\\n____________________________feature number:{}____________________________\" .format(i)\n",
    "        features = X[:,i]\n",
    "        print \"size of feature {} before delete NaNs:{}\" .format(i,features.size)\n",
    "        print \"Before Delete NaNs:{}\" .format(features)\n",
    "        feat_nan_indeices = np.argwhere(np.isnan(features))\n",
    "        print \"number of nan for this feature:{}\" .format(feat_nan_indeices.size)\n",
    "        new_features = np.delete(features,feat_nan_indeices)\n",
    "        print \"Size of feature {} column after delete NaNs:{}\" .format(i,new_features.size)\n",
    "        print \"After Delete NaNs:{}\" .format(new_features)\n",
    "        mean = np.mean(new_features)\n",
    "        if (np.isnan(mean)):\n",
    "            mean = 0.0\n",
    "        features_mean.append(mean)\n",
    "        features[feat_nan_indeices] = mean\n",
    "        X[:,i] = features\n",
    "        #Finding featuresh without any missing \n",
    "        if (features.size == new_features.size):\n",
    "            no_missing_features.append(i)\n",
    "    print \"\\nThe number of all no-missing features: {}\" .format(len(no_missing_features))\n",
    "    print \"\\n__________________Data after replacing all nan elements:__________________\\n\\n{}\".format(X)\n",
    "    print \"\\nData shape:{}\" .format(X.shape)\n",
    "    print \"\\nTo check if there is any other Nan(exisiting Nan indices):{}\" .format(np.argwhere(np.isnan(X)))\n",
    "    if (np.argwhere(np.isnan(X)).size == 0):\n",
    "         print \"\\n^^^^^^^^^^^^^Congratulation!^^^^^^^^^^^^\\n\\n-------There is no other NaN element in the dataset!--------\"\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data and Save in 5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(X,fold):\n",
    "    print \"\\n\\n*******************PART 2 - Split Data and Save each split as a dataset for train and validation*****************\\n\\n\"\n",
    "    #remove 5 non-predictive \n",
    "    new_data = X[:,4:]\n",
    "    splitted_data = {}\n",
    "    k = 0 \n",
    "    for i in range(fold):\n",
    "        j = 399*(i+1)\n",
    "        split = new_data[k:j,:]\n",
    "        k = j\n",
    "        splitted_data[i] = split\n",
    "    print \"\\n_______________________________________DONE! Data is splitted_____________________________________\\n\"\n",
    "    fold0_train = np.concatenate((np.concatenate((np.concatenate((splitted_data[1],splitted_data[2]),axis=0),splitted_data[3]),axis=0),splitted_data[4]),axis=0)\n",
    "    fold1_train = np.concatenate((np.concatenate((np.concatenate((splitted_data[0],splitted_data[2]),axis=0),splitted_data[3]),axis=0),splitted_data[4]),axis=0)\n",
    "    fold2_train = np.concatenate((np.concatenate((np.concatenate((splitted_data[3],splitted_data[4]),axis=0),splitted_data[0]),axis=0),splitted_data[1]),axis=0)\n",
    "    fold3_train = np.concatenate((np.concatenate((np.concatenate((splitted_data[4],splitted_data[0]),axis=0),splitted_data[1]),axis=0),splitted_data[2]),axis=0)\n",
    "    fold4_train = np.concatenate((np.concatenate((np.concatenate((splitted_data[0],splitted_data[1]),axis=0),splitted_data[2]),axis=0),splitted_data[3]),axis=0)\n",
    "    print \"_________________________________FOLD 0______________________________________\"\n",
    "    np.savetxt(\"hwk1_datasets/Datasets/Q3.2_dataset/k0/CandC_valid0.csv\", splitted_data[0], fmt='%15s', delimiter=',') \n",
    "    np.savetxt(\"hwk1_datasets/Datasets/Q3.2_dataset/k0/CandC_train0.csv\", fold0_train, fmt='%15s', delimiter=',') \n",
    "    print \"_________________________________FOLD 1______________________________________\"\n",
    "    np.savetxt(\"hwk1_datasets/Datasets/Q3.2_dataset/k1/CandC_valid1.csv\", splitted_data[1], fmt='%15s', delimiter=',') \n",
    "    np.savetxt(\"hwk1_datasets/Datasets/Q3.2_dataset/k1/CandC_train1.csv\", fold1_train, fmt='%15s', delimiter=',') \n",
    "    print \"_________________________________FOLD 2______________________________________\"\n",
    "    np.savetxt(\"hwk1_datasets/Datasets/Q3.2_dataset/k2/CandC_valid2.csv\", splitted_data[2], fmt='%15s', delimiter=',') \n",
    "    np.savetxt(\"hwk1_datasets/Datasets/Q3.2_dataset/k2/CandC_train2.csv\", fold2_train, fmt='%15s', delimiter=',') \n",
    "    print \"_________________________________FOLD 3______________________________________\"\n",
    "    np.savetxt(\"hwk1_datasets/Datasets/Q3.2_dataset/k3/CandC_valid3.csv\", splitted_data[3], fmt='%15s', delimiter=',') \n",
    "    np.savetxt(\"hwk1_datasets/Datasets/Q3.2_dataset/k3/CandC_train3.csv\", fold3_train, fmt='%15s', delimiter=',') \n",
    "    print \"_________________________________FOLD 4______________________________________\"\n",
    "    np.savetxt(\"hwk1_datasets/Datasets/Q3.2_dataset/k4/CandC_valid4.csv\", splitted_data[4], fmt='%15s', delimiter=',') \n",
    "    np.savetxt(\"hwk1_datasets/Datasets/Q3.2_dataset/k4/CandC_train4.csv\", fold4_train, fmt='%15s', delimiter=',')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def train_closeform(self, x, y ,v_x, v_y):\n",
    "        self.valid_x = v_x\n",
    "        self.valid_y = v_y\n",
    "        self.train_x = x\n",
    "        self.train_y = y\n",
    "        self.train_close()\n",
    "        t_mse, v_mse = self.compute_loss()\n",
    "        return t_mse, v_mse\n",
    "    def gen_features(self,x,degree=1):\n",
    "        ones = np.ones((x.shape[0],1))\n",
    "        features = np.concatenate((ones,x), axis=1)\n",
    "        return features\n",
    "    def train_close(self):\n",
    "        self.feat = self.gen_features(self.train_x)\n",
    "        self.w = np.dot(np.dot(inv(np.dot(self.feat.T, self.feat)), self.feat.T), self.train_y)\n",
    "        print \"\\nLearned Parameters(W):\\n{}\".format(self.w)\n",
    "    def predict(self, X, w):\n",
    "        y = np.dot(X, w)\n",
    "        return y\n",
    "    def compute_loss(self,degree=1):\n",
    "        train_mse = ((np.dot(self.feat, self.w) - self.train_y)**2).mean(axis=0)\n",
    "        valid_mse = ((np.dot(self.gen_features(self.valid_x,degree), self.w) - self.valid_y)**2).mean(axis=0)\n",
    "        print \"\\nTrain MSE:{}, Valid MSE:{}\".format(train_mse, valid_mse)\n",
    "        return train_mse, valid_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RidgeReg(LinearRegression):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(RidgeReg, self).__init__(*args, **kwargs)\n",
    "    def l2_regularization(self,x, y, lambda_):\n",
    "        self.feat = self.gen_features(x)\n",
    "        I = np.identity(self.feat.shape[1])\n",
    "        self.w_ridge = np.dot(np.dot(inv((np.dot(self.feat.T, self.feat)+(lambda_*I))), self.feat.T), y)\n",
    "        print \"\\nParameters W_ridge after L2 regularization corresponding to Lambda {} is: \\n {} \" .format(lambda_, self.w_ridge)\n",
    "        return self.w_ridge;\n",
    "    def compute_reg_loss(self, x, y, valid_x, valid_y, lambda_):\n",
    "        train_mse = ((np.dot(self.gen_features(x), self.w_ridge) - y)**2).mean(axis=0) + lambda_*(np.dot(self.w_ridge.T,self.w_ridge))\n",
    "        valid_mse = ((np.dot(self.gen_features(valid_x), self.w_ridge) - valid_y)**2).mean(axis=0)+ lambda_*(np.dot(self.w_ridge.T,self.w_ridge))\n",
    "        print \"After Regularization with lambda {}:\\n Train MSE:{}, Valid MSE:{}\".format(lambda_,train_mse, valid_mse)\n",
    "        return train_mse, valid_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**********************************PART 1: Fill the missing values****************************\n",
      "\n",
      "\n",
      "_____________________________Features Specification________________________\n",
      "\n",
      "____________________________feature number:0____________________________\n",
      "size of feature 0 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 41.  28.  47. ...,  54.  53.  34.]\n",
      "number of nan for this feature:0\n",
      "Size of feature 0 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 41.  28.  47. ...,  54.  53.  34.]\n",
      "\n",
      "____________________________feature number:1____________________________\n",
      "size of feature 1 before delete NaNs:1994\n",
      "Before Delete NaNs:[ nan  nan  nan ...,  nan  nan   3.]\n",
      "number of nan for this feature:1174\n",
      "Size of feature 1 column after delete NaNs:820\n",
      "After Delete NaNs:[   7.   79.   21.    9.   19.   21.   11.   17.    7.  133.   35.    7.\n",
      "   79.   35.   15.   49.   17.   53.  141.    3.   61.   21.  750.    3.\n",
      "    9.   45.   91.    7.   15.   11.   41.    5.   11.    3.   13.   17.\n",
      "    7.   29.   23.   41.    3.    9.   27.   41.    5.   17.  790.   75.\n",
      "  113.   17.   35.   17.    1.   25.   45.   39.   23.    9.    3.    5.\n",
      "   33.   43.    3.   17.    3.  770.   17.   55.   45.   77.    7.   93.\n",
      "   39.   99.   23.   35.   39.    7.  141.   29.   11.   63.   39.   35.\n",
      "    5.  165.  139.    1.   27.    7.  153.   79.    3.   17.    5.    1.\n",
      "  133.   99.   17.   27.   71.   55.   17.   27.   43.   95.    3.    9.\n",
      "  740.    7.    3.   69.   91.   41.   23.  163.   31.   77.   35.   27.\n",
      "  510.    3.  133.    7.   13.   27.   71.   27.   61.   87.   23.    1.\n",
      "  101.    3.   13.   27.   31.   11.   43.   35.   49.   71.   91.    7.\n",
      "   17.  143.    7.   13.    5.   29.    3.   11.  135.   49.   39.   31.\n",
      "   93.   17.   35.    3.   63.   23.   15.   95.    1.   21.  840.   39.\n",
      "    7.   23.   97.   49.   67.   13.    9.   35.   77.    3.   21.    3.\n",
      "    5.   11.   49.    9.   13.   17.   27.   23.    1.    5.    3.   17.\n",
      "   13.    3.   27.   65.    5.   13.   77.  105.    9.   25.  107.    5.\n",
      "    7.   59.  121.   27.   23.   37.   63.   61.    5.   57.   29.  650.\n",
      "   29.  127.    7.   25.    7.   27.    1.  119.  119.   17.   27.  510.\n",
      "   29.    5.    3.    3.   21.   25.    3.   19.   79.   25.    3.  151.\n",
      "   21.   93.  129.   25.    9.   31.  105.    5.    7.  139.   89.   75.\n",
      "   31.  119.  193.   11.    9.   23.   87.   89.  169.   25.   27.    1.\n",
      "   11.  119.   79.   15.    5.   25.   41.    7.   27.   39.    3.   25.\n",
      "  105.   45.  215.  125.  153.   15.   23.   43.   15.    9.  129.   13.\n",
      "    1.    5.   17.    1.   21.   27.  570.    5.   39.  119.   35.    5.\n",
      "  113.    9.   53.    3.  800.   27.   17.   17.   21.   23.   35.   17.\n",
      "   71.    5.    1.   17.   21.   17.  710.    5.   85.   23.   27.   11.\n",
      "   35.   25.    9.    3.   71.   21.   95.    3.   27.   71.    5.    5.\n",
      "   33.    3.    5.   21.   97.    9.   11.   87.    3.    5.   17.    7.\n",
      "   63.  119.    9.    5.   15.    7.   61.    9.    1.   17.   53.    1.\n",
      "    5.  113.    3.    3.   17.   45.   79.   17.  111.   79.   17.  139.\n",
      "   31.   17.   29.   13.    3.   93.   13.    7.   29.    9.   55.   21.\n",
      "  101.  123.   11.    1.    7.   13.   15.    9.   35.   17.    3.   31.\n",
      "    1.   35.   27.    7.   39.  157.  139.   31.   73.   27.   13.   17.\n",
      "    1.   19.   13.   17.   77.    1.   21.   13.   19.   27.   17.   31.\n",
      "   27.    1.   15.   17.    3.   15.    1.    7.   13.  700.    7.   23.\n",
      "   79.   45.   17.   21.  131.    3.   39.   79.   35.   45.   35.   15.\n",
      "   13.   13.   37.   13.   13.    1.   71.    3.   43.   17.    3.    3.\n",
      "   31.   71.   17.    1.   21.    3.    1.    3.    7.   23.  133.    9.\n",
      "    5.    5.   45.   13.   17.   29.  810.   27.   27.  119.    5.   17.\n",
      "  133.   61.   39.   21.   27.   61.   25.  157.   27.   19.   17.   21.\n",
      "  683.  730.    9.    3.   23.  660.   77.   89.    9.    3.   11.   17.\n",
      "   13.    3.   11.   21.   93.    5.   17.    7.  101.    1.   11.   17.\n",
      "   13.   95.   45.   11.   17.  113.   21.   35.   29.   13.    9.  590.\n",
      "   41.   25.   29.   71.  167.    9.   13.   29.   21.   27.   17.    9.\n",
      "   91.   79.   39.   13.    9.  181.   11.    9.   17.   25.   35.   31.\n",
      "    3.   15.    5.  117.   25.   27.    7.   71.   29.    7.   11.   21.\n",
      "    1.    9.   91.    3.   63.   23.   79.    3.    9.   31.  153.   25.\n",
      "    1.  131.   89.    1.    9.   79.   17.   23.    3.   79.    1.   21.\n",
      "   15.  147.   35.   27.  735.   17.   11.  129.   31.   33.   45.    3.\n",
      "   23.   17.   23.  690.   23.   53.    3.   25.    1.    9.    1.   71.\n",
      "  139.   65.   27.  113.    5.    7.    3.    3.   13.  103.    3.   29.\n",
      "   39.   23.    7.   11.    1.   27.   69.  151.   95.  187.   41.   71.\n",
      "    3.   31.    5.   35.   49.   29.   37.   25.  101.    3.    9.   69.\n",
      "   29.   13.  133.  133.   39.   39.    7.   61.  155.   17.   39.   23.\n",
      "   25.  630.    1.   35.   91.    1.    3.  550.    9.  101.  133.    3.\n",
      "   17.    9.    9.  145.    9.   29.   21.    1.  820.   31.    3.    9.\n",
      "   29.   79.   21.   49.    9.   35.    3.  830.   73.   17.  131.    9.\n",
      "    1.    3.   29.   23.    3.   23.  760.   11.   21.   13.    7.   25.\n",
      "   71.   37.   25.  101.  151.    7.   27.  670.   71.   29.    5.   15.\n",
      "   57.    3.  153.   85.    3.   25.   13.   81.   31.   59.   17.   33.\n",
      "   65.   39.   81.    7.    3.    1.    9.   35.  119.   19.   25.   17.\n",
      "  109.   11.    9.   21.   71.   29.    3.    9.   45.   75.    7.    1.\n",
      "    9.   17.    1.   23.   11.    3.   77.   89.    7.  133.   17.   19.\n",
      "  133.   23.   27.    5.   17.   35.   75.   79.  173.  133.   23.   27.\n",
      "   17.   17.   17.    9.   17.    5.   21.   61.    3.   25.    3.  775.\n",
      "    3.   19.   91.   25.    5.    3.   69.    5.   35.  173.   21.   23.\n",
      "    1.   17.   29.    1.    7.   21.   35.    3.    3.   81.    3.  680.\n",
      "    9.    9.   99.    3.]\n",
      "\n",
      "____________________________feature number:2____________________________\n",
      "size of feature 2 before delete NaNs:1994\n",
      "Before Delete NaNs:[    nan     nan     nan ...,     nan     nan  42750.]\n",
      "number of nan for this feature:1177\n",
      "Size of feature 2 column after delete NaNs:817\n",
      "After Delete NaNs:[  5.76600000e+04   1.79750000e+04   5.59550000e+04   5.43600000e+04\n",
      "   2.79500000e+03   1.13150000e+04   5.23500000e+04   8.02300000e+04\n",
      "   1.80800000e+04   8.42500000e+04   4.16640000e+04   2.87400000e+04\n",
      "   6.11200000e+04   4.31000000e+04   2.53800000e+04   7.90020000e+04\n",
      "   3.70000000e+04   3.01400000e+04   8.82000000e+04   5.57700000e+04\n",
      "   9.10850000e+04   6.09000000e+04   6.53920000e+04   8.41600000e+03\n",
      "   3.74900000e+04   8.69680000e+04   7.71520000e+04   1.06750000e+04\n",
      "   7.20000000e+03   7.58000000e+03   7.87360000e+04   4.99600000e+04\n",
      "   4.51400000e+04   8.30500000e+04   4.38000000e+04   7.96100000e+04\n",
      "   1.20160000e+04   1.08240000e+04   5.25600000e+04   3.22960000e+04\n",
      "   6.95840000e+04   8.75600000e+04   6.49800000e+04   9.39550000e+04\n",
      "   6.05450000e+04   7.61350000e+04   7.52160000e+04   2.78150000e+04\n",
      "   3.66100000e+04   3.60000000e+04   2.94280000e+04   8.47800000e+04\n",
      "   3.87400000e+04   4.40700000e+04   3.33600000e+03   4.03500000e+04\n",
      "   3.16450000e+04   5.38900000e+04   7.05500000e+04   2.82400000e+04\n",
      "   5.10250000e+04   3.28000000e+04   7.23600000e+04   4.01150000e+04\n",
      "   6.30000000e+04   6.80000000e+04   3.58000000e+03   8.39750000e+04\n",
      "   5.37500000e+04   8.45280000e+04   1.00000000e+04   5.69660000e+04\n",
      "   1.56400000e+04   8.80000000e+04   6.94200000e+04   1.60000000e+04\n",
      "   6.15300000e+04   4.75000000e+03   4.96750000e+04   6.93000000e+04\n",
      "   4.75400000e+04   4.30820000e+04   7.90400000e+04   4.36200000e+04\n",
      "   8.95000000e+03   4.23640000e+04   5.08250000e+04   2.08000000e+03\n",
      "   4.25100000e+04   1.42600000e+04   3.82800000e+03   7.51250000e+04\n",
      "   1.86400000e+04   3.87150000e+04   7.75700000e+04   5.97350000e+04\n",
      "   5.92500000e+04   5.90200000e+04   6.25350000e+04   7.50150000e+04\n",
      "   4.85000000e+04   6.30000000e+04   3.98350000e+04   3.49800000e+04\n",
      "   6.17140000e+04   7.70000000e+04   6.53700000e+04   4.83400000e+04\n",
      "   6.40000000e+04   5.46400000e+04   4.62250000e+04   6.90000000e+04\n",
      "   6.52550000e+04   4.48320000e+04   7.41180000e+04   1.34560000e+04\n",
      "   5.65500000e+04   2.35840000e+04   7.69400000e+04   7.16200000e+04\n",
      "   1.00000000e+03   4.35540000e+04   8.34320000e+04   5.90000000e+04\n",
      "   3.44500000e+04   4.83000000e+04   4.68960000e+04   2.64300000e+04\n",
      "   7.41040000e+04   2.37500000e+03   1.83880000e+04   2.94300000e+04\n",
      "   5.33800000e+04   9.06720000e+04   4.75000000e+04   1.43950000e+04\n",
      "   6.57600000e+04   8.07400000e+04   4.50560000e+04   7.14160000e+04\n",
      "   4.64100000e+04   2.95530000e+04   4.50080000e+04   7.62200000e+04\n",
      "   3.22500000e+04   2.88260000e+04   1.91800000e+04   6.92740000e+04\n",
      "   4.54600000e+04   6.17980000e+04   3.70700000e+04   6.36240000e+04\n",
      "   7.30600000e+04   2.40000000e+04   7.44800000e+04   7.86500000e+04\n",
      "   4.48560000e+04   8.05100000e+04   7.89320000e+04   4.47000000e+04\n",
      "   4.07750000e+04   5.43100000e+04   6.66600000e+04   7.60250000e+04\n",
      "   8.98000000e+03   6.09150000e+04   8.67200000e+04   6.60600000e+04\n",
      "   7.76300000e+04   2.02300000e+04   7.72000000e+04   6.27800000e+03\n",
      "   7.30000000e+04   1.30450000e+04   4.16100000e+04   7.18000000e+03\n",
      "   7.26320000e+04   2.13000000e+04   5.02500000e+04   4.20900000e+04\n",
      "   4.37400000e+04   4.80200000e+04   8.33420000e+04   5.91050000e+04\n",
      "   7.78500000e+04   3.49520000e+04   6.40800000e+04   9.00000000e+03\n",
      "   2.55600000e+04   6.24300000e+04   8.49000000e+04   2.19900000e+04\n",
      "   2.18400000e+03   1.27000000e+04   5.18250000e+04   6.34180000e+04\n",
      "   6.87500000e+04   4.43280000e+04   5.48810000e+04   3.78250000e+04\n",
      "   6.00150000e+04   2.52300000e+04   6.24320000e+04   4.51200000e+04\n",
      "   6.60700000e+03   3.92250000e+04   5.64560000e+04   2.89500000e+04\n",
      "   1.90000000e+04   3.29100000e+04   5.36820000e+04   2.77060000e+04\n",
      "   2.21100000e+04   2.06600000e+03   8.27040000e+04   3.50000000e+04\n",
      "   3.46800000e+04   8.69250000e+04   1.22800000e+04   3.26400000e+04\n",
      "   2.68200000e+04   5.96410000e+04   7.17500000e+03   8.16770000e+04\n",
      "   8.80840000e+04   3.52150000e+04   2.38750000e+04   4.00000000e+03\n",
      "   1.47120000e+04   7.08800000e+04   5.91000000e+03   8.70700000e+04\n",
      "   7.86900000e+04   3.10100000e+04   6.52800000e+04   6.19200000e+04\n",
      "   5.30000000e+04   4.80000000e+04   5.04400000e+04   1.42000000e+03\n",
      "   6.45440000e+04   1.79800000e+03   6.63760000e+04   2.59500000e+04\n",
      "   4.13000000e+04   1.36900000e+04   6.50000000e+03   1.29400000e+04\n",
      "   2.87700000e+04   5.93500000e+04   5.44850000e+04   4.21680000e+04\n",
      "   6.00900000e+04   5.69790000e+04   9.39260000e+04   8.02800000e+04\n",
      "   7.70000000e+04   5.82000000e+04   3.88000000e+04   5.40400000e+04\n",
      "   8.65480000e+04   3.06900000e+04   4.11650000e+04   9.28000000e+03\n",
      "   5.22000000e+04   8.40000000e+04   8.53000000e+04   4.32200000e+04\n",
      "   5.75100000e+04   4.29900000e+04   5.83500000e+04   2.24900000e+04\n",
      "   5.64600000e+04   5.18100000e+04   2.22400000e+04   7.30200000e+04\n",
      "   8.62200000e+04   5.31040000e+04   5.96080000e+04   7.61060000e+04\n",
      "   6.29000000e+04   5.89800000e+04   7.56720000e+04   4.82500000e+03\n",
      "   5.20700000e+04   5.51280000e+04   7.60300000e+04   5.05800000e+04\n",
      "   4.50000000e+04   3.96250000e+04   5.00000000e+04   3.04550000e+04\n",
      "   3.50750000e+04   1.84480000e+04   8.04900000e+04   2.62750000e+04\n",
      "   6.43090000e+04   1.60140000e+04   5.78800000e+04   4.00400000e+04\n",
      "   7.31000000e+03   4.30000000e+04   6.16800000e+04   7.64320000e+04\n",
      "   4.76700000e+04   7.26000000e+04   6.65700000e+04   3.31800000e+04\n",
      "   5.26300000e+04   9.24600000e+03   6.82600000e+04   8.11680000e+04\n",
      "   8.43000000e+03   8.63700000e+04   5.07000000e+03   1.97800000e+04\n",
      "   7.34050000e+04   5.70000000e+04   7.65700000e+04   4.90560000e+04\n",
      "   8.20000000e+04   4.68200000e+04   2.70250000e+04   6.84600000e+04\n",
      "   1.96000000e+03   4.06750000e+04   6.29400000e+04   4.12240000e+04\n",
      "   8.02400000e+04   5.87300000e+04   7.43000000e+04   4.02900000e+04\n",
      "   5.00340000e+04   2.38500000e+04   3.93000000e+04   1.00300000e+04\n",
      "   6.08250000e+04   7.19900000e+04   6.07850000e+04   1.22000000e+03\n",
      "   7.60700000e+04   3.00750000e+04   8.70000000e+04   8.60250000e+04\n",
      "   2.51120000e+04   1.18000000e+04   5.10550000e+04   4.91210000e+04\n",
      "   3.42500000e+03   6.61450000e+04   7.88000000e+04   1.53500000e+04\n",
      "   2.15040000e+04   2.94050000e+04   7.37600000e+04   8.10480000e+04\n",
      "   5.48370000e+04   5.14000000e+03   1.74400000e+04   2.93330000e+04\n",
      "   6.83880000e+04   6.10000000e+04   1.10000000e+04   3.31440000e+04\n",
      "   5.88000000e+04   3.02100000e+04   3.97270000e+04   1.03750000e+04\n",
      "   6.99400000e+04   5.57500000e+04   7.94600000e+04   4.38950000e+04\n",
      "   5.98800000e+04   7.85100000e+04   5.07840000e+04   6.55080000e+04\n",
      "   7.98000000e+04   6.61750000e+04   7.73440000e+04   8.00700000e+04\n",
      "   2.66750000e+04   2.50650000e+04   1.82560000e+04   9.32180000e+04\n",
      "   5.02600000e+04   8.25250000e+04   1.41400000e+04   5.36800000e+04\n",
      "   5.99800000e+04   3.10000000e+04   7.29280000e+04   8.76000000e+03\n",
      "   2.25600000e+04   5.70000000e+04   1.67490000e+04   8.36220000e+04\n",
      "   2.48000000e+03   5.24800000e+04   3.19800000e+04   2.24560000e+04\n",
      "   4.71380000e+04   7.31400000e+04   5.33680000e+04   6.18900000e+04\n",
      "   6.26000000e+03   5.61300000e+04   7.81000000e+03   1.69200000e+04\n",
      "   7.82500000e+04   8.10350000e+04   2.00000000e+03   7.30700000e+04\n",
      "   2.48200000e+04   7.58150000e+04   6.22500000e+04   1.80700000e+04\n",
      "   5.46880000e+04   6.46750000e+04   6.18000000e+04   2.02900000e+04\n",
      "   1.77100000e+04   7.79300000e+04   7.13900000e+04   8.21200000e+04\n",
      "   3.36200000e+04   2.00800000e+04   1.42000000e+04   5.60000000e+04\n",
      "   8.07800000e+04   6.93900000e+04   5.25840000e+04   6.32640000e+04\n",
      "   3.30120000e+04   3.95100000e+04   8.53500000e+04   8.51880000e+04\n",
      "   5.32000000e+03   3.11250000e+04   8.09900000e+04   7.87760000e+04\n",
      "   5.56000000e+03   4.34400000e+04   1.96450000e+04   1.12000000e+04\n",
      "   6.96900000e+04   4.69500000e+03   3.71750000e+04   8.35000000e+04\n",
      "   8.13250000e+04   4.30000000e+03   7.03800000e+04   1.60500000e+03\n",
      "   2.06000000e+03   5.16600000e+04   7.78400000e+04   4.70420000e+04\n",
      "   2.57000000e+04   1.85000000e+04   5.57450000e+04   7.64900000e+04\n",
      "   1.00000000e+03   2.66400000e+04   4.73600000e+04   7.41190000e+04\n",
      "   7.31680000e+04   5.76000000e+04   4.99700000e+04   2.30000000e+04\n",
      "   1.32080000e+04   9.45970000e+04   4.61500000e+03   7.03200000e+04\n",
      "   8.20000000e+04   5.81100000e+04   5.90000000e+03   8.40770000e+04\n",
      "   4.89000000e+04   5.61600000e+03   5.52750000e+04   7.18920000e+04\n",
      "   5.91900000e+04   1.64950000e+04   2.95500000e+04   1.50000000e+04\n",
      "   4.59900000e+04   5.52160000e+04   1.76500000e+04   1.04720000e+04\n",
      "   7.22150000e+04   6.12250000e+04   4.89520000e+04   6.18320000e+04\n",
      "   3.45500000e+04   8.35120000e+04   1.84550000e+04   3.56240000e+04\n",
      "   5.73020000e+04   5.11500000e+04   3.84000000e+04   8.02700000e+04\n",
      "   4.66800000e+04   4.55600000e+04   5.10000000e+04   1.84000000e+04\n",
      "   2.10000000e+03   3.82880000e+04   4.05800000e+04   4.90200000e+04\n",
      "   5.24700000e+04   2.29600000e+04   1.19500000e+04   2.06000000e+03\n",
      "   3.79400000e+04   4.98400000e+04   4.63800000e+04   6.09600000e+03\n",
      "   4.14400000e+04   7.37700000e+04   1.97000000e+04   7.94920000e+04\n",
      "   7.24950000e+04   5.32800000e+04   6.01200000e+04   3.63000000e+04\n",
      "   1.62500000e+04   2.13440000e+04   1.12720000e+04   7.00000000e+01\n",
      "   4.01890000e+04   5.50000000e+04   4.76280000e+04   5.59500000e+03\n",
      "   1.93900000e+04   8.41920000e+04   7.66500000e+03   5.10000000e+03\n",
      "   7.72550000e+04   3.56500000e+04   2.16000000e+04   8.46750000e+04\n",
      "   6.46500000e+04   1.00000000e+02   5.48700000e+04   9.20360000e+04\n",
      "   2.98600000e+04   1.97750000e+04   1.88200000e+04   4.93220000e+04\n",
      "   7.50980000e+04   4.86000000e+03   6.53400000e+04   1.79400000e+04\n",
      "   2.56800000e+03   7.29750000e+04   4.13100000e+04   4.80900000e+04\n",
      "   3.77200000e+04   2.38320000e+04   8.30800000e+04   8.17400000e+04\n",
      "   4.50000000e+03   4.41050000e+04   7.41900000e+04   1.18500000e+03\n",
      "   4.50960000e+04   2.86800000e+04   5.99250000e+04   1.07500000e+04\n",
      "   2.94000000e+04   4.11000000e+04   1.95500000e+04   6.31500000e+04\n",
      "   5.72600000e+04   7.64600000e+04   5.96400000e+04   2.88750000e+04\n",
      "   7.48800000e+04   5.60600000e+04   3.49500000e+04   3.34080000e+04\n",
      "   1.97920000e+04   3.02100000e+04   2.57700000e+04   3.97840000e+04\n",
      "   5.29800000e+04   9.17500000e+03   2.63400000e+04   7.67780000e+04\n",
      "   2.23000000e+04   7.38950000e+04   6.37680000e+04   1.31350000e+04\n",
      "   1.77200000e+04   5.88800000e+04   5.78700000e+04   7.90000000e+04\n",
      "   8.45120000e+04   3.88550000e+04   4.89550000e+04   5.12100000e+04\n",
      "   4.97840000e+04   6.87900000e+04   5.48080000e+04   2.44200000e+04\n",
      "   4.98900000e+04   1.67750000e+04   6.74600000e+04   7.72000000e+04\n",
      "   4.12160000e+04   6.05000000e+04   4.96000000e+04   2.54850000e+04\n",
      "   2.10000000e+04   8.21050000e+04   4.04400000e+04   5.38500000e+04\n",
      "   6.06400000e+03   4.08900000e+04   5.29800000e+04   5.39600000e+04\n",
      "   4.31400000e+04   7.00200000e+04   4.56900000e+04   4.15000000e+04\n",
      "   3.07800000e+03   4.86200000e+04   4.69250000e+04   1.21440000e+04\n",
      "   4.82440000e+04   4.83420000e+04   9.13700000e+04   9.07410000e+04\n",
      "   8.31520000e+04   6.01200000e+04   7.98200000e+04   2.01000000e+04\n",
      "   7.16820000e+04   1.80000000e+04   3.85500000e+04   5.88800000e+03\n",
      "   1.98400000e+04   5.48750000e+04   6.89400000e+04   8.28700000e+04\n",
      "   2.86400000e+04   3.05000000e+03   1.78000000e+04   3.24480000e+04\n",
      "   5.10000000e+04   6.46200000e+04   1.31500000e+04   6.62000000e+04\n",
      "   5.73860000e+04   4.33440000e+04   3.15400000e+04   7.14300000e+04\n",
      "   8.08500000e+03   6.24300000e+04   2.97440000e+04   8.07000000e+03\n",
      "   3.18900000e+04   5.06400000e+04   2.90200000e+04   3.12400000e+04\n",
      "   1.60000000e+04   3.23100000e+04   6.60000000e+04   5.63750000e+04\n",
      "   5.59500000e+04   4.91200000e+04   7.87400000e+04   2.61500000e+04\n",
      "   6.43040000e+04   6.86450000e+04   4.05600000e+04   4.93000000e+03\n",
      "   6.81700000e+04   8.36800000e+04   3.05700000e+04   3.23280000e+04\n",
      "   1.46500000e+03   1.81300000e+04   8.67000000e+04   3.97650000e+04\n",
      "   3.77720000e+04   4.75350000e+04   2.94430000e+04   2.59900000e+04\n",
      "   8.61600000e+04   8.44750000e+04   4.49680000e+04   4.69240000e+04\n",
      "   5.24900000e+04   4.72000000e+03   5.17000000e+03   1.10000000e+04\n",
      "   4.59000000e+04   5.21400000e+04   6.57900000e+04   6.70000000e+04\n",
      "   7.60000000e+03   7.88650000e+04   7.97400000e+04   2.67600000e+04\n",
      "   7.00000000e+03   2.13440000e+04   7.57400000e+04   5.15750000e+04\n",
      "   6.00000000e+04   1.20000000e+04   1.83500000e+04   3.47950000e+04\n",
      "   3.84240000e+04   9.34800000e+04   3.73800000e+04   8.14400000e+04\n",
      "   6.41450000e+04   8.67720000e+04   8.25900000e+04   7.49440000e+04\n",
      "   8.50360000e+04   2.26300000e+04   5.42700000e+04   2.11050000e+04\n",
      "   8.53120000e+04   4.06200000e+04   4.33350000e+04   5.28050000e+04\n",
      "   9.28900000e+04   7.65400000e+04   2.10000000e+04   9.31700000e+04\n",
      "   5.14000000e+03   2.76000000e+04   6.84300000e+04   1.41600000e+04\n",
      "   4.92000000e+03   3.03670000e+04   1.29000000e+04   2.52000000e+04\n",
      "   1.74750000e+04   3.80770000e+04   7.28240000e+04   4.58100000e+04\n",
      "   2.21850000e+04   1.53840000e+04   7.42000000e+03   6.33600000e+04\n",
      "   6.76100000e+04   4.76160000e+04   5.55740000e+04   5.17600000e+04\n",
      "   7.56200000e+04   4.65200000e+04   1.45750000e+04   4.01800000e+04\n",
      "   3.14700000e+04   5.62700000e+04   7.24200000e+04   6.08800000e+03\n",
      "   1.96200000e+04   4.79200000e+03   1.00250000e+04   2.49250000e+04\n",
      "   1.04640000e+04   8.70560000e+04   5.92800000e+04   6.67000000e+04\n",
      "   6.07500000e+04   2.13000000e+03   7.72000000e+03   4.93000000e+04\n",
      "   7.37250000e+04   7.90000000e+04   5.32240000e+04   6.03300000e+04\n",
      "   4.90800000e+04   3.07000000e+04   3.78750000e+04   3.65100000e+04\n",
      "   4.99500000e+04   1.50600000e+04   4.78800000e+04   7.40000000e+04\n",
      "   6.57320000e+04   7.58160000e+04   3.64800000e+04   7.45400000e+04\n",
      "   7.00000000e+04   3.04200000e+04   5.55400000e+04   7.90560000e+04\n",
      "   7.86000000e+04   1.64250000e+04   2.24700000e+04   2.03520000e+04\n",
      "   6.91700000e+04   3.20600000e+04   6.21480000e+04   4.16900000e+04\n",
      "   5.47050000e+04   3.63500000e+03   7.46300000e+04   5.30000000e+03\n",
      "   1.88500000e+04   1.48750000e+04   6.79450000e+04   8.36400000e+03\n",
      "   5.23200000e+04   2.14800000e+04   7.46080000e+04   1.35700000e+04\n",
      "   4.76720000e+04   5.47160000e+04   4.63650000e+04   6.39680000e+04\n",
      "   4.27500000e+04]\n",
      "\n",
      "____________________________feature number:3____________________________\n",
      "size of feature 3 before delete NaNs:1994\n",
      "Before Delete NaNs:[ nan  nan  nan ...,  nan  nan  nan]\n",
      "number of nan for this feature:1994\n",
      "Size of feature 3 column after delete NaNs:0\n",
      "After Delete NaNs:[]\n",
      "\n",
      "____________________________feature number:4____________________________\n",
      "size of feature 4 before delete NaNs:1994\n",
      "Before Delete NaNs:[  7.  10.   8. ...,   5.   4.   1.]\n",
      "number of nan for this feature:0\n",
      "Size of feature 4 column after delete NaNs:1994\n",
      "After Delete NaNs:[  7.  10.   8. ...,   5.   4.   1.]\n",
      "\n",
      "____________________________feature number:5____________________________\n",
      "size of feature 5 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.    0.01  0.06 ...,  0.02  0.05  0.01]\n",
      "number of nan for this feature:0\n",
      "Size of feature 5 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.    0.01  0.06 ...,  0.02  0.05  0.01]\n",
      "\n",
      "____________________________feature number:6____________________________\n",
      "size of feature 6 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.3   0.46  0.36 ...,  0.32  0.41  0.43]\n",
      "number of nan for this feature:0\n",
      "Size of feature 6 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.3   0.46  0.36 ...,  0.32  0.41  0.43]\n",
      "\n",
      "____________________________feature number:7____________________________\n",
      "size of feature 7 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.01  1.    0.11 ...,  0.08  0.14  0.07]\n",
      "number of nan for this feature:0\n",
      "Size of feature 7 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.01  1.    0.11 ...,  0.08  0.14  0.07]\n",
      "\n",
      "____________________________feature number:8____________________________\n",
      "size of feature 8 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.94  0.08  0.9  ...,  0.93  0.76  0.86]\n",
      "number of nan for this feature:0\n",
      "Size of feature 8 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.94  0.08  0.9  ...,  0.93  0.76  0.86]\n",
      "\n",
      "____________________________feature number:9____________________________\n",
      "size of feature 9 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.13  0.03  0.04 ...,  0.02  0.32  0.24]\n",
      "number of nan for this feature:0\n",
      "Size of feature 9 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.13  0.03  0.04 ...,  0.02  0.32  0.24]\n",
      "\n",
      "____________________________feature number:10____________________________\n",
      "size of feature 10 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.04  0.    0.01 ...,  0.01  0.09  0.05]\n",
      "number of nan for this feature:0\n",
      "Size of feature 10 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.04  0.    0.01 ...,  0.01  0.09  0.05]\n",
      "\n",
      "____________________________feature number:11____________________________\n",
      "size of feature 11 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.35  0.48  0.52 ...,  0.33  0.5   0.4 ]\n",
      "number of nan for this feature:0\n",
      "Size of feature 11 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.35  0.48  0.52 ...,  0.33  0.5   0.4 ]\n",
      "\n",
      "____________________________feature number:12____________________________\n",
      "size of feature 12 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.41  0.48  0.56 ...,  0.33  0.72  0.58]\n",
      "number of nan for this feature:0\n",
      "Size of feature 12 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.41  0.48  0.56 ...,  0.33  0.72  0.58]\n",
      "\n",
      "____________________________feature number:13____________________________\n",
      "size of feature 13 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.25  0.28  0.48 ...,  0.2   0.59  0.39]\n",
      "number of nan for this feature:0\n",
      "Size of feature 13 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.25  0.28  0.48 ...,  0.2   0.59  0.39]\n",
      "\n",
      "____________________________feature number:14____________________________\n",
      "size of feature 14 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.53  0.53  0.51 ...,  0.68  0.42  0.24]\n",
      "number of nan for this feature:0\n",
      "Size of feature 14 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.53  0.53  0.51 ...,  0.68  0.42  0.24]\n",
      "\n",
      "____________________________feature number:15____________________________\n",
      "size of feature 15 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.    0.    0.08 ...,  0.03  0.06  0.03]\n",
      "number of nan for this feature:0\n",
      "Size of feature 15 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.    0.    0.08 ...,  0.03  0.06  0.03]\n",
      "\n",
      "____________________________feature number:16____________________________\n",
      "size of feature 16 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.    0.    1.   ...,  1.    1.    0.93]\n",
      "number of nan for this feature:0\n",
      "Size of feature 16 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.    0.    1.   ...,  1.    1.    0.93]\n",
      "\n",
      "____________________________feature number:17____________________________\n",
      "size of feature 17 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.21  0.07  0.19 ...,  0.27  0.18  0.74]\n",
      "number of nan for this feature:0\n",
      "Size of feature 17 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.21  0.07  0.19 ...,  0.27  0.18  0.74]\n",
      "\n",
      "____________________________feature number:18____________________________\n",
      "size of feature 18 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.43  0.29  0.45 ...,  0.32  0.42  0.84]\n",
      "number of nan for this feature:0\n",
      "Size of feature 18 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.43  0.29  0.45 ...,  0.32  0.42  0.84]\n",
      "\n",
      "____________________________feature number:19____________________________\n",
      "size of feature 19 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.26  0.43  0.3  ...,  0.14  0.07  0.25]\n",
      "number of nan for this feature:0\n",
      "Size of feature 19 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.26  0.43  0.3  ...,  0.14  0.07  0.25]\n",
      "\n",
      "____________________________feature number:20____________________________\n",
      "size of feature 20 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.47  0.2   0.42 ...,  0.59  0.42  0.75]\n",
      "number of nan for this feature:0\n",
      "Size of feature 20 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.47  0.2   0.42 ...,  0.59  0.42  0.75]\n",
      "\n",
      "____________________________feature number:21____________________________\n",
      "size of feature 21 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.56  0.65  0.55 ...,  0.73  0.41  0.21]\n",
      "number of nan for this feature:0\n",
      "Size of feature 21 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.56  0.65  0.55 ...,  0.73  0.41  0.21]\n",
      "\n",
      "____________________________feature number:22____________________________\n",
      "size of feature 22 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.34  1.    0.41 ...,  0.29  0.54  0.09]\n",
      "number of nan for this feature:0\n",
      "Size of feature 22 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.34  1.    0.41 ...,  0.29  0.54  0.09]\n",
      "\n",
      "____________________________feature number:23____________________________\n",
      "size of feature 23 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.57  0.26  0.45 ...,  0.86  0.78  0.31]\n",
      "number of nan for this feature:0\n",
      "Size of feature 23 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.57  0.26  0.45 ...,  0.86  0.78  0.31]\n",
      "\n",
      "____________________________feature number:24____________________________\n",
      "size of feature 24 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.24  0.11  0.26 ...,  0.3   0.19  0.74]\n",
      "number of nan for this feature:0\n",
      "Size of feature 24 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.24  0.11  0.26 ...,  0.3   0.19  0.74]\n",
      "\n",
      "____________________________feature number:25____________________________\n",
      "size of feature 25 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.24  0.19  0.27 ...,  0.3   0.21  0.78]\n",
      "number of nan for this feature:0\n",
      "Size of feature 25 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.24  0.19  0.27 ...,  0.3   0.21  0.78]\n",
      "\n",
      "____________________________feature number:26____________________________\n",
      "size of feature 26 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.23  0.45  0.27 ...,  0.28  0.22  0.8 ]\n",
      "number of nan for this feature:0\n",
      "Size of feature 26 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.23  0.45  0.27 ...,  0.28  0.22  0.8 ]\n",
      "\n",
      "____________________________feature number:27____________________________\n",
      "size of feature 27 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.23  0.14  0.2  ...,  0.27  0.22  0.45]\n",
      "number of nan for this feature:0\n",
      "Size of feature 27 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.23  0.14  0.2  ...,  0.27  0.22  0.45]\n",
      "\n",
      "____________________________feature number:28____________________________\n",
      "size of feature 28 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.11  0.1   0.15 ...,  0.13  0.12  0.11]\n",
      "number of nan for this feature:0\n",
      "Size of feature 28 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.11  0.1   0.15 ...,  0.13  0.12  0.11]\n",
      "\n",
      "____________________________feature number:29____________________________\n",
      "size of feature 29 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.28  0.2   0.2  ...,  1.    0.18  0.42]\n",
      "number of nan for this feature:0\n",
      "Size of feature 29 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.28  0.2   0.2  ...,  1.    0.18  0.42]\n",
      "\n",
      "____________________________feature number:30____________________________\n",
      "size of feature 30 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.12  0.    0.19 ...,  0.    0.23  0.34]\n",
      "number of nan for this feature:1\n",
      "Size of feature 30 column after delete NaNs:1993\n",
      "After Delete NaNs:[ 0.12  0.    0.19 ...,  0.    0.23  0.34]\n",
      "\n",
      "____________________________feature number:31____________________________\n",
      "size of feature 31 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.14  0.23  0.34 ...,  0.55  0.3   0.51]\n",
      "number of nan for this feature:0\n",
      "Size of feature 31 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.14  0.23  0.34 ...,  0.55  0.3   0.51]\n",
      "\n",
      "____________________________feature number:32____________________________\n",
      "size of feature 32 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.01  0.06  0.07 ...,  0.02  0.06  0.  ]\n",
      "number of nan for this feature:0\n",
      "Size of feature 32 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.01  0.06  0.07 ...,  0.02  0.06  0.  ]\n",
      "\n",
      "____________________________feature number:33____________________________\n",
      "size of feature 33 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.43  1.    0.45 ...,  0.24  0.48  0.08]\n",
      "number of nan for this feature:0\n",
      "Size of feature 33 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.43  1.    0.45 ...,  0.24  0.48  0.08]\n",
      "\n",
      "____________________________feature number:34____________________________\n",
      "size of feature 34 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.22  0.74  0.47 ...,  0.43  0.22  0.15]\n",
      "number of nan for this feature:0\n",
      "Size of feature 34 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.22  0.74  0.47 ...,  0.43  0.22  0.15]\n",
      "\n",
      "____________________________feature number:35____________________________\n",
      "size of feature 35 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.3   0.73  0.5  ...,  0.45  0.3   0.15]\n",
      "number of nan for this feature:0\n",
      "Size of feature 35 column after delete NaNs:1994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Delete NaNs:[ 0.3   0.73  0.5  ...,  0.45  0.3   0.15]\n",
      "\n",
      "____________________________feature number:36____________________________\n",
      "size of feature 36 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.25  0.29  0.41 ...,  0.17  0.19  0.71]\n",
      "number of nan for this feature:0\n",
      "Size of feature 36 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.25  0.29  0.41 ...,  0.17  0.19  0.71]\n",
      "\n",
      "____________________________feature number:37____________________________\n",
      "size of feature 37 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.37  0.77  0.41 ...,  0.42  0.47  0.21]\n",
      "number of nan for this feature:0\n",
      "Size of feature 37 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.37  0.77  0.41 ...,  0.42  0.47  0.21]\n",
      "\n",
      "____________________________feature number:38____________________________\n",
      "size of feature 38 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.43  0.23  0.37 ...,  0.22  0.47  0.73]\n",
      "number of nan for this feature:0\n",
      "Size of feature 38 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.43  0.23  0.37 ...,  0.22  0.47  0.73]\n",
      "\n",
      "____________________________feature number:39____________________________\n",
      "size of feature 39 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.45  0.38  0.44 ...,  0.85  0.42  0.39]\n",
      "number of nan for this feature:0\n",
      "Size of feature 39 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.45  0.38  0.44 ...,  0.85  0.42  0.39]\n",
      "\n",
      "____________________________feature number:40____________________________\n",
      "size of feature 40 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.33  0.53  0.55 ...,  0.35  0.36  0.4 ]\n",
      "number of nan for this feature:0\n",
      "Size of feature 40 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.33  0.53  0.55 ...,  0.35  0.36  0.4 ]\n",
      "\n",
      "____________________________feature number:41____________________________\n",
      "size of feature 41 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.61  0.53  0.39 ...,  0.68  0.33  0.13]\n",
      "number of nan for this feature:0\n",
      "Size of feature 41 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.61  0.53  0.39 ...,  0.68  0.33  0.13]\n",
      "\n",
      "____________________________feature number:42____________________________\n",
      "size of feature 42 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.32  0.45  0.47 ...,  0.25  0.29  0.71]\n",
      "number of nan for this feature:0\n",
      "Size of feature 42 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.32  0.45  0.47 ...,  0.25  0.29  0.71]\n",
      "\n",
      "____________________________feature number:43____________________________\n",
      "size of feature 43 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.75  0.64  0.61 ...,  0.4   0.66  0.28]\n",
      "number of nan for this feature:0\n",
      "Size of feature 43 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.75  0.64  0.61 ...,  0.4   0.66  0.28]\n",
      "\n",
      "____________________________feature number:44____________________________\n",
      "size of feature 44 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.36  0.55  0.46 ...,  0.28  0.61  0.52]\n",
      "number of nan for this feature:0\n",
      "Size of feature 44 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.36  0.55  0.46 ...,  0.28  0.61  0.52]\n",
      "\n",
      "____________________________feature number:45____________________________\n",
      "size of feature 45 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.7   0.63  0.53 ...,  0.37  0.77  0.32]\n",
      "number of nan for this feature:0\n",
      "Size of feature 45 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.7   0.63  0.53 ...,  0.37  0.77  0.32]\n",
      "\n",
      "____________________________feature number:46____________________________\n",
      "size of feature 46 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.75  0.67  0.59 ...,  0.4   0.73  0.31]\n",
      "number of nan for this feature:0\n",
      "Size of feature 46 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.75  0.67  0.59 ...,  0.4   0.73  0.31]\n",
      "\n",
      "____________________________feature number:47____________________________\n",
      "size of feature 47 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.37  0.65  0.34 ...,  0.33  0.4   0.4 ]\n",
      "number of nan for this feature:0\n",
      "Size of feature 47 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.37  0.65  0.34 ...,  0.33  0.4   0.4 ]\n",
      "\n",
      "____________________________feature number:48____________________________\n",
      "size of feature 48 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.5   0.13  0.57 ...,  0.66  0.5   0.81]\n",
      "number of nan for this feature:0\n",
      "Size of feature 48 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.5   0.13  0.57 ...,  0.66  0.5   0.81]\n",
      "\n",
      "____________________________feature number:49____________________________\n",
      "size of feature 49 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.55  0.09  0.58 ...,  0.69  0.55  0.82]\n",
      "number of nan for this feature:0\n",
      "Size of feature 49 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.55  0.09  0.58 ...,  0.69  0.55  0.82]\n",
      "\n",
      "____________________________feature number:50____________________________\n",
      "size of feature 50 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.58  0.06  0.56 ...,  0.68  0.54  0.86]\n",
      "number of nan for this feature:0\n",
      "Size of feature 50 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.58  0.06  0.56 ...,  0.68  0.54  0.86]\n",
      "\n",
      "____________________________feature number:51____________________________\n",
      "size of feature 51 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.6   0.    0.58 ...,  0.8   0.63  0.48]\n",
      "number of nan for this feature:0\n",
      "Size of feature 51 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.6   0.    0.58 ...,  0.8   0.63  0.48]\n",
      "\n",
      "____________________________feature number:52____________________________\n",
      "size of feature 52 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.38  0.8   0.37 ...,  0.33  0.2   0.09]\n",
      "number of nan for this feature:0\n",
      "Size of feature 52 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.38  0.8   0.37 ...,  0.33  0.2   0.09]\n",
      "\n",
      "____________________________feature number:53____________________________\n",
      "size of feature 53 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.44  0.57  0.36 ...,  0.25  0.2   0.24]\n",
      "number of nan for this feature:0\n",
      "Size of feature 53 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.44  0.57  0.36 ...,  0.25  0.2   0.24]\n",
      "\n",
      "____________________________feature number:54____________________________\n",
      "size of feature 54 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.    0.06  0.02 ...,  0.01  0.02  0.  ]\n",
      "number of nan for this feature:0\n",
      "Size of feature 54 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.    0.06  0.02 ...,  0.01  0.02  0.  ]\n",
      "\n",
      "____________________________feature number:55____________________________\n",
      "size of feature 55 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.15  1.    0.16 ...,  0.09  0.23  0.11]\n",
      "number of nan for this feature:0\n",
      "Size of feature 55 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.15  1.    0.16 ...,  0.09  0.23  0.11]\n",
      "\n",
      "____________________________feature number:56____________________________\n",
      "size of feature 56 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.    0.    0.01 ...,  0.    0.01  0.01]\n",
      "number of nan for this feature:0\n",
      "Size of feature 56 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.    0.    0.01 ...,  0.    0.01  0.01]\n",
      "\n",
      "____________________________feature number:57____________________________\n",
      "size of feature 57 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.1   0.96  0.64 ...,  0.03  0.28  0.27]\n",
      "number of nan for this feature:0\n",
      "Size of feature 57 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.1   0.96  0.64 ...,  0.03  0.28  0.27]\n",
      "\n",
      "____________________________feature number:58____________________________\n",
      "size of feature 58 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.13  0.71  0.63 ...,  0.03  0.4   0.42]\n",
      "number of nan for this feature:0\n",
      "Size of feature 58 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.13  0.71  0.63 ...,  0.03  0.4   0.42]\n",
      "\n",
      "____________________________feature number:59____________________________\n",
      "size of feature 59 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.12  0.58  0.56 ...,  0.04  0.5   0.45]\n",
      "number of nan for this feature:0\n",
      "Size of feature 59 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.12  0.58  0.56 ...,  0.04  0.5   0.45]\n",
      "\n",
      "____________________________feature number:60____________________________\n",
      "size of feature 60 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.11  0.49  0.5  ...,  0.03  0.51  0.45]\n",
      "number of nan for this feature:0\n",
      "Size of feature 60 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.11  0.49  0.5  ...,  0.03  0.51  0.45]\n",
      "\n",
      "____________________________feature number:61____________________________\n",
      "size of feature 61 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.03  0.04  0.07 ...,  0.01  0.09  0.21]\n",
      "number of nan for this feature:0\n",
      "Size of feature 61 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.03  0.04  0.07 ...,  0.01  0.09  0.21]\n",
      "\n",
      "____________________________feature number:62____________________________\n",
      "size of feature 62 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.04  0.03  0.06 ...,  0.01  0.11  0.29]\n",
      "number of nan for this feature:0\n",
      "Size of feature 62 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.04  0.03  0.06 ...,  0.01  0.11  0.29]\n",
      "\n",
      "____________________________feature number:63____________________________\n",
      "size of feature 63 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.03  0.02  0.05 ...,  0.01  0.13  0.28]\n",
      "number of nan for this feature:0\n",
      "Size of feature 63 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.03  0.02  0.05 ...,  0.01  0.13  0.28]\n",
      "\n",
      "____________________________feature number:64____________________________\n",
      "size of feature 64 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.03  0.02  0.04 ...,  0.01  0.12  0.26]\n",
      "number of nan for this feature:0\n",
      "Size of feature 64 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.03  0.02  0.04 ...,  0.01  0.12  0.26]\n",
      "\n",
      "____________________________feature number:65____________________________\n",
      "size of feature 65 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.84  0.97  0.97 ...,  0.85  0.87  0.79]\n",
      "number of nan for this feature:0\n",
      "Size of feature 65 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.84  0.97  0.97 ...,  0.85  0.87  0.79]\n",
      "\n",
      "____________________________feature number:66____________________________\n",
      "size of feature 66 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.1   0.03  0.02 ...,  0.06  0.05  0.1 ]\n",
      "number of nan for this feature:0\n",
      "Size of feature 66 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.1   0.03  0.02 ...,  0.06  0.05  0.1 ]\n",
      "\n",
      "____________________________feature number:67____________________________\n",
      "size of feature 67 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.21  0.52  0.12 ...,  0.08  0.23  0.16]\n",
      "number of nan for this feature:0\n",
      "Size of feature 67 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.21  0.52  0.12 ...,  0.08  0.23  0.16]\n",
      "\n",
      "____________________________feature number:68____________________________\n",
      "size of feature 68 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.17  0.43  0.1  ...,  0.09  0.18  0.16]\n",
      "number of nan for this feature:0\n",
      "Size of feature 68 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.17  0.43  0.1  ...,  0.09  0.18  0.16]\n",
      "\n",
      "____________________________feature number:69____________________________\n",
      "size of feature 69 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.32  0.48  0.29 ...,  0.34  0.31  0.4 ]\n",
      "number of nan for this feature:0\n",
      "Size of feature 69 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.32  0.48  0.29 ...,  0.34  0.31  0.4 ]\n",
      "\n",
      "____________________________feature number:70____________________________\n",
      "size of feature 70 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.35  0.42  0.37 ...,  0.38  0.24  0.41]\n",
      "number of nan for this feature:0\n",
      "Size of feature 70 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.35  0.42  0.37 ...,  0.38  0.24  0.41]\n",
      "\n",
      "____________________________feature number:71____________________________\n",
      "size of feature 71 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.33  0.56  0.23 ...,  0.26  0.41  0.31]\n",
      "number of nan for this feature:0\n",
      "Size of feature 71 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.33  0.56  0.23 ...,  0.26  0.41  0.31]\n",
      "\n",
      "____________________________feature number:72____________________________\n",
      "size of feature 72 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.39  0.32  0.51 ...,  0.71  0.18  0.81]\n",
      "number of nan for this feature:0\n",
      "Size of feature 72 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.39  0.32  0.51 ...,  0.71  0.18  0.81]\n",
      "\n",
      "____________________________feature number:73____________________________\n",
      "size of feature 73 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.14  0.45  0.06 ...,  0.04  0.23  0.05]\n",
      "number of nan for this feature:0\n",
      "Size of feature 73 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.14  0.45  0.06 ...,  0.04  0.23  0.05]\n",
      "\n",
      "____________________________feature number:74____________________________\n",
      "size of feature 74 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.61  0.65  0.57 ...,  0.41  0.71  0.47]\n",
      "number of nan for this feature:0\n",
      "Size of feature 74 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.61  0.65  0.57 ...,  0.41  0.71  0.47]\n",
      "\n",
      "____________________________feature number:75____________________________\n",
      "size of feature 75 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.   0.   0.  ...,  0.5  0.   0.5]\n",
      "number of nan for this feature:0\n",
      "Size of feature 75 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.   0.   0.  ...,  0.5  0.   0.5]\n",
      "\n",
      "____________________________feature number:76____________________________\n",
      "size of feature 76 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.02  0.03  0.08 ...,  0.03  0.05  0.02]\n",
      "number of nan for this feature:0\n",
      "Size of feature 76 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.02  0.03  0.08 ...,  0.03  0.05  0.02]\n",
      "\n",
      "____________________________feature number:77____________________________\n",
      "size of feature 77 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.63  0.71  0.71 ...,  0.81  0.76  0.76]\n",
      "number of nan for this feature:0\n",
      "Size of feature 77 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.63  0.71  0.71 ...,  0.81  0.76  0.76]\n",
      "\n",
      "____________________________feature number:78____________________________\n",
      "size of feature 78 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.39  0.36  0.47 ...,  0.68  0.23  0.79]\n",
      "number of nan for this feature:0\n",
      "Size of feature 78 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.39  0.36  0.47 ...,  0.68  0.23  0.79]\n",
      "\n",
      "____________________________feature number:79____________________________\n",
      "size of feature 79 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.35  1.    0.87 ...,  0.18  0.31  0.14]\n",
      "number of nan for this feature:0\n",
      "Size of feature 79 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.35  1.    0.87 ...,  0.18  0.31  0.14]\n",
      "\n",
      "____________________________feature number:80____________________________\n",
      "size of feature 80 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.45  0.59  0.44 ...,  0.69  0.45  0.14]\n",
      "number of nan for this feature:0\n",
      "Size of feature 80 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.45  0.59  0.44 ...,  0.69  0.45  0.14]\n",
      "\n",
      "____________________________feature number:81____________________________\n",
      "size of feature 81 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.    0.44  0.56 ...,  0.33  0.25  0.85]\n",
      "number of nan for this feature:0\n",
      "Size of feature 81 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.    0.44  0.56 ...,  0.33  0.25  0.85]\n",
      "\n",
      "____________________________feature number:82____________________________\n",
      "size of feature 82 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.51  0.89  0.55 ...,  0.19  0.39  0.04]\n",
      "number of nan for this feature:0\n",
      "Size of feature 82 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.51  0.89  0.55 ...,  0.19  0.39  0.04]\n",
      "\n",
      "____________________________feature number:83____________________________\n",
      "size of feature 83 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.62  0.42  0.14 ...,  0.03  0.26  0.06]\n",
      "number of nan for this feature:0\n",
      "Size of feature 83 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.62  0.42  0.14 ...,  0.03  0.26  0.06]\n",
      "\n",
      "____________________________feature number:84____________________________\n",
      "size of feature 84 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.09  0.06  0.09 ...,  0.07  0.13  0.65]\n",
      "number of nan for this feature:0\n",
      "Size of feature 84 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.09  0.06  0.09 ...,  0.07  0.13  0.65]\n",
      "\n",
      "____________________________feature number:85____________________________\n",
      "size of feature 85 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.08  0.06  0.1  ...,  0.07  0.11  0.75]\n",
      "number of nan for this feature:0\n",
      "Size of feature 85 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.08  0.06  0.1  ...,  0.07  0.11  0.75]\n",
      "\n",
      "____________________________feature number:86____________________________\n",
      "size of feature 86 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.09  0.08  0.12 ...,  0.07  0.11  0.83]\n",
      "number of nan for this feature:0\n",
      "Size of feature 86 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.09  0.08  0.12 ...,  0.07  0.11  0.83]\n",
      "\n",
      "____________________________feature number:87____________________________\n",
      "size of feature 87 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.15  0.    0.12 ...,  0.12  0.22  0.91]\n",
      "number of nan for this feature:0\n",
      "Size of feature 87 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.15  0.    0.12 ...,  0.12  0.22  0.91]\n",
      "\n",
      "____________________________feature number:88____________________________\n",
      "size of feature 88 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.17  0.02  0.15 ...,  0.14  0.25  0.94]\n",
      "number of nan for this feature:0\n",
      "Size of feature 88 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.17  0.02  0.15 ...,  0.14  0.25  0.94]\n",
      "\n",
      "____________________________feature number:89____________________________\n",
      "size of feature 89 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.2   0.05  0.18 ...,  0.13  0.26  1.  ]\n",
      "number of nan for this feature:0\n",
      "Size of feature 89 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.2   0.05  0.18 ...,  0.13  0.26  1.  ]\n",
      "\n",
      "____________________________feature number:90____________________________\n",
      "size of feature 90 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.13  0.07  0.15 ...,  0.18  0.24  0.93]\n",
      "number of nan for this feature:0\n",
      "Size of feature 90 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.13  0.07  0.15 ...,  0.18  0.24  0.93]\n",
      "\n",
      "____________________________feature number:91____________________________\n",
      "size of feature 91 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.34  0.85  0.45 ...,  0.2   0.4   0.39]\n",
      "number of nan for this feature:0\n",
      "Size of feature 91 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.34  0.85  0.45 ...,  0.2   0.4   0.39]\n",
      "\n",
      "____________________________feature number:92____________________________\n",
      "size of feature 92 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.2   0.43  0.23 ...,  0.01  0.34  0.81]\n",
      "number of nan for this feature:0\n",
      "Size of feature 92 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.2   0.43  0.23 ...,  0.01  0.34  0.81]\n",
      "\n",
      "____________________________feature number:93____________________________\n",
      "size of feature 93 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.46  0.46  0.26 ...,  0.24  0.33  0.47]\n",
      "number of nan for this feature:0\n",
      "Size of feature 93 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.46  0.46  0.26 ...,  0.24  0.33  0.47]\n",
      "\n",
      "____________________________feature number:94____________________________\n",
      "size of feature 94 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.01  0.    0.03 ...,  0.    0.02  0.  ]\n",
      "number of nan for this feature:0\n",
      "Size of feature 94 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.01  0.    0.03 ...,  0.    0.02  0.  ]\n",
      "\n",
      "____________________________feature number:95____________________________\n",
      "size of feature 95 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.    0.    0.   ...,  0.    0.01  0.  ]\n",
      "number of nan for this feature:0\n",
      "Size of feature 95 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.    0.    0.   ...,  0.    0.01  0.  ]\n",
      "\n",
      "____________________________feature number:96____________________________\n",
      "size of feature 96 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.14  0.01  0.04 ...,  0.1   0.13  0.33]\n",
      "number of nan for this feature:0\n",
      "Size of feature 96 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.14  0.01  0.04 ...,  0.1   0.13  0.33]\n",
      "\n",
      "____________________________feature number:97____________________________\n",
      "size of feature 97 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.44  0.95  0.63 ...,  0.5   0.28  0.44]\n",
      "number of nan for this feature:0\n",
      "Size of feature 97 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.44  0.95  0.63 ...,  0.5   0.28  0.44]\n",
      "\n",
      "____________________________feature number:98____________________________\n",
      "size of feature 98 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.47  0.58  0.49 ...,  0.86  0.23  0.4 ]\n",
      "number of nan for this feature:0\n",
      "Size of feature 98 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.47  0.58  0.49 ...,  0.86  0.23  0.4 ]\n",
      "\n",
      "____________________________feature number:99____________________________\n",
      "size of feature 99 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.49  0.86  0.54 ...,  0.84  0.22  0.57]\n",
      "number of nan for this feature:0\n",
      "Size of feature 99 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.49  0.86  0.54 ...,  0.84  0.22  0.57]\n",
      "\n",
      "____________________________feature number:100____________________________\n",
      "size of feature 100 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.39  0.88  0.59 ...,  0.81  0.    0.53]\n",
      "number of nan for this feature:0\n",
      "Size of feature 100 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.39  0.88  0.59 ...,  0.81  0.    0.53]\n",
      "\n",
      "____________________________feature number:101____________________________\n",
      "size of feature 101 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.01 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 101 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.01  0.03  0.25  0.01  0.01  0.02  0.15  0.03  0.01  0.01  0.01  0.05\n",
      "  0.01  0.03  0.02  0.01  0.01  0.02  0.01  0.03  0.01  0.03  0.02  0.04\n",
      "  0.19  0.02  0.01  0.02  0.03  0.04  0.02  0.02  0.04  0.08  0.05  0.38\n",
      "  0.08  0.02  0.02  0.01  0.02  0.02  0.02  0.01  0.03  0.14  0.02  0.13\n",
      "  0.01  0.02  0.14  0.29  0.03  0.02  0.02  0.02  0.01  0.02  0.07  0.02\n",
      "  0.04  0.08  0.02  0.07  0.01  0.01  0.02  0.01  0.02  0.03  0.02  0.03\n",
      "  0.09  0.02  0.03  0.01  0.03  0.09  0.03  0.02  0.1   0.02  0.03  0.03\n",
      "  0.01  0.01  0.01  0.02  0.02  0.02  0.01  0.02  0.03  0.03  0.01  0.01\n",
      "  0.01  0.03  0.01  0.63  0.11  0.03  0.05  0.04  0.41  0.05  0.02  0.02\n",
      "  0.05  0.01  0.02  0.07  0.01  0.01  0.39  0.1   0.01  0.12  0.02  0.01\n",
      "  0.01  0.    0.07  0.01  0.07  0.07  0.01  0.04  1.    0.34  0.02  0.17\n",
      "  0.01  0.13  0.06  0.02  0.03  0.05  0.14  0.14  0.02  0.07  0.03  0.03\n",
      "  0.02  0.21  0.01  0.01  0.01  0.04  0.02  0.01  0.04  0.05  0.02  0.01\n",
      "  0.06  0.02  0.07  0.01  0.06  0.02  0.02  0.01  0.01  0.03  0.03  0.05\n",
      "  0.01  0.02  0.01  0.02  0.04  0.02  0.04  0.04  0.02  0.01  0.07  0.16\n",
      "  0.13  1.    0.01  0.    0.04  0.05  0.01  0.02  0.06  0.15  0.04  0.01\n",
      "  0.04  0.12  0.01  0.2   0.24  0.01  0.02  0.04  0.02  0.03  0.01  0.02\n",
      "  0.02  0.03  0.04  0.04  0.05  0.3   0.04  0.02  0.02  0.04  0.01  0.04\n",
      "  0.01  0.3   0.58  0.05  0.07  0.01  0.01  0.12  0.03  0.12  0.02  0.02\n",
      "  0.07  0.02  0.02  0.2   0.04  0.02  0.02  0.05  0.25  0.02  0.01  0.03\n",
      "  0.01  0.16  0.1   0.42  0.06  0.28  0.02  0.01  0.02  0.3   0.01  0.02\n",
      "  0.01  0.01  0.01  0.01  0.09  0.07  0.04  0.03  0.08  0.01  0.07  0.03\n",
      "  0.02  0.17  0.03  0.01  0.18  0.02  0.01  0.04  0.02  0.22  0.01  0.13\n",
      "  0.45  1.    0.02  0.03  0.01  0.01  0.03  0.01  0.03  0.05  0.54  0.02\n",
      "  0.01  0.02  0.04  0.06  0.08  0.02  0.01  0.02  0.06  0.01  0.03  0.08\n",
      "  0.01  0.02  0.07  0.24  0.02  0.01  0.02  1.    0.15  0.03  0.03  0.\n",
      "  0.02  0.02  0.05  0.02  0.01  0.04  0.02]\n",
      "\n",
      "____________________________feature number:102____________________________\n",
      "size of feature 102 before delete NaNs:1994\n",
      "Before Delete NaNs:[ nan  nan  0.2 ...,  nan  nan  nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 102 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.2   0.12  0.14  0.07  0.11  0.1   0.1   0.17  0.17  0.2   0.12  0.13\n",
      "  0.03  0.13  0.18  0.22  0.38  0.15  0.26  0.5   0.16  0.02  0.22  0.5\n",
      "  0.41  0.2   0.33  0.28  0.2   0.24  0.19  0.26  0.24  0.15  0.35  0.35\n",
      "  0.09  0.11  0.12  0.17  0.08  0.1   0.18  0.08  0.3   0.17  0.09  0.26\n",
      "  0.2   1.    0.2   0.1   0.18  0.2   0.16  0.14  0.12  0.18  0.11  0.22\n",
      "  0.21  0.29  0.25  0.11  0.25  0.18  0.98  0.39  0.17  0.25  0.1   0.17\n",
      "  0.35  0.19  0.16  0.21  0.18  0.27  0.07  0.13  1.    0.12  0.23  0.13\n",
      "  0.1   0.12  0.2   0.26  0.17  0.2   0.19  0.09  0.12  0.16  0.14  0.3\n",
      "  0.38  0.4   0.25  0.4   0.09  0.1   0.16  0.15  0.29  0.13  0.19  0.14\n",
      "  0.15  0.2   0.18  0.16  0.11  0.28  0.23  0.28  0.2   0.25  0.15  0.09\n",
      "  0.21  0.1   0.18  0.1   0.32  0.23  0.2   0.24  0.85  0.14  0.12  0.21\n",
      "  0.1   0.13  0.34  0.25  0.09  0.68  0.25  0.27  0.16  1.    0.26  1.\n",
      "  0.18  0.26  0.18  0.1   0.17  0.17  0.25  0.15  0.04  0.28  0.21  0.23\n",
      "  0.3   0.18  0.02  0.1   0.24  0.05  0.26  0.11  0.22  0.08  0.16  0.27\n",
      "  0.13  0.12  0.16  0.15  0.1   0.26  0.13  0.43  0.27  0.14  0.14  0.15\n",
      "  1.    0.23  0.18  0.25  0.22  0.2   0.19  0.19  0.3   0.15  0.25  0.15\n",
      "  0.12  0.12  0.27  0.25  0.64  0.09  0.15  0.1   0.19  0.17  0.2   0.03\n",
      "  0.09  0.1   0.1   0.2   0.16  0.28  0.33  0.15  0.83  0.14  0.17  0.21\n",
      "  0.17  0.21  0.24  0.08  0.1   0.18  0.28  0.14  0.17  0.15  0.19  0.09\n",
      "  0.1   0.15  0.24  0.39  0.33  0.14  0.08  0.21  0.16  0.09  0.29  0.21\n",
      "  0.2   0.14  0.27  0.16  0.16  0.25  0.13  0.15  0.11  0.2   0.19  0.31\n",
      "  0.19  0.15  0.08  0.06  0.28  0.28  0.12  0.22  0.2   0.14  0.72  0.22\n",
      "  0.16  0.28  0.16  0.2   0.17  0.26  0.13  0.13  0.13  0.33  0.15  0.31\n",
      "  0.37  0.43  0.18  0.19  0.19  0.21  0.02  0.09  0.08  0.27  0.68  0.17\n",
      "  0.38  0.16  0.09  0.19  0.13  0.15  0.19  0.    0.67  0.14  0.55  0.11\n",
      "  0.21  0.37  0.43  0.25  0.14  0.14  0.11  0.35  0.42  0.11  0.1   0.13\n",
      "  0.24  0.28  0.18  0.19  0.08  0.32  0.19]\n",
      "\n",
      "____________________________feature number:103____________________________\n",
      "size of feature 103 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.98 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 103 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.98  0.96  0.8   0.98  0.98  0.97  0.84  0.97  0.98  0.98  0.98  0.93\n",
      "  0.99  0.96  0.97  0.98  0.98  0.97  0.98  0.97  0.98  0.95  0.98  0.98\n",
      "  0.83  0.97  0.97  0.97  0.96  0.95  0.97  0.97  0.95  0.91  0.95  0.64\n",
      "  0.9   0.97  0.97  0.98  0.96  0.97  0.97  0.98  0.96  0.85  0.97  0.87\n",
      "  0.98  0.98  0.87  0.89  0.96  0.97  0.97  0.97  0.98  0.97  0.92  0.97\n",
      "  0.95  0.91  0.97  0.92  0.98  0.98  0.97  0.98  0.97  0.96  0.97  0.96\n",
      "  0.9   0.97  0.96  0.98  0.98  0.92  0.98  0.97  0.93  0.97  0.96  0.96\n",
      "  0.98  0.98  0.98  0.97  0.97  0.97  0.98  0.97  0.96  0.96  0.98  0.98\n",
      "  0.98  0.96  0.98  0.37  0.87  0.96  0.93  0.96  0.63  0.95  0.97  0.97\n",
      "  0.93  0.98  0.97  0.93  0.98  0.98  0.63  0.89  0.97  0.89  0.98  0.98\n",
      "  0.98  0.99  0.92  0.98  0.92  0.92  0.99  0.95  0.    0.67  0.97  0.88\n",
      "  0.98  0.86  0.93  0.97  0.96  0.97  0.85  0.85  0.96  0.92  0.96  0.97\n",
      "  0.96  0.81  0.98  0.98  0.98  0.95  0.97  0.99  0.95  0.95  0.97  0.98\n",
      "  0.93  0.97  0.95  0.98  0.93  0.98  0.97  0.98  0.98  0.96  0.96  0.94\n",
      "  0.97  0.97  1.    0.97  0.95  0.97  0.95  0.94  0.97  0.98  0.91  0.83\n",
      "  0.85  0.    0.98  0.99  0.95  0.94  0.98  0.98  0.93  0.83  0.95  0.98\n",
      "  0.95  0.87  0.98  0.8   0.83  0.98  0.96  0.95  0.98  0.96  0.98  0.97\n",
      "  0.97  0.96  0.95  0.95  0.94  0.72  0.95  0.97  0.98  0.95  0.98  0.95\n",
      "  0.98  0.81  0.49  0.94  0.91  0.98  0.98  0.87  0.96  0.87  0.97  0.96\n",
      "  0.92  0.97  0.97  0.85  0.94  0.97  0.97  0.94  0.75  0.98  0.98  0.96\n",
      "  0.97  0.86  0.91  0.55  0.94  0.73  0.97  0.98  0.97  0.67  0.98  0.97\n",
      "  0.98  0.98  0.98  0.98  0.9   0.95  0.95  0.96  0.92  0.98  0.94  0.97\n",
      "  0.97  0.82  0.96  0.98  0.82  0.96  0.98  0.95  0.97  0.76  0.98  0.86\n",
      "  0.53  0.    0.97  0.97  0.97  0.98  0.96  0.97  0.96  0.94  0.61  0.97\n",
      "  0.97  0.97  0.95  0.92  0.91  0.97  0.98  0.97  0.93  0.98  0.97  0.91\n",
      "  0.98  0.98  0.93  0.75  0.97  0.98  0.97  0.    0.84  0.96  0.96  0.98\n",
      "  0.97  0.97  0.93  0.97  0.98  0.94  0.97]\n",
      "\n",
      "____________________________feature number:104____________________________\n",
      "size of feature 104 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.23 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 104 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.23  0.16  0.13  0.08  0.14  0.14  0.13  0.19  0.23  0.2   0.12  0.18\n",
      "  0.03  0.16  0.23  0.27  0.46  0.19  0.32  0.44  0.22  0.04  0.21  0.31\n",
      "  0.42  0.25  0.41  0.32  0.23  0.29  0.25  0.32  0.26  0.2   0.37  0.39\n",
      "  0.12  0.14  0.15  0.21  0.12  0.13  0.23  0.1   0.33  0.2   0.12  0.31\n",
      "  0.19  0.86  0.21  0.03  0.2   0.25  0.22  0.16  0.16  0.22  0.14  0.28\n",
      "  0.25  0.32  0.3   0.14  0.32  0.2   0.88  0.45  0.19  0.26  0.12  0.22\n",
      "  0.43  0.22  0.19  0.25  0.1   0.27  0.04  0.15  1.    0.14  0.26  0.17\n",
      "  0.14  0.15  0.27  0.32  0.22  0.24  0.23  0.12  0.15  0.2   0.18  0.36\n",
      "  0.39  0.47  0.32  0.47  0.11  0.13  0.2   0.16  0.32  0.14  0.21  0.16\n",
      "  0.2   0.25  0.21  0.18  0.14  0.31  0.26  0.32  0.25  0.24  0.13  0.1\n",
      "  0.27  0.14  0.21  0.11  0.38  0.27  0.19  0.28  1.    0.16  0.14  0.17\n",
      "  0.13  0.17  0.42  0.3   0.12  0.45  0.3   0.32  0.21  1.    0.31  1.\n",
      "  0.24  0.27  0.19  0.11  0.22  0.2   0.27  0.17  0.06  0.32  0.27  0.29\n",
      "  0.36  0.21  0.01  0.1   0.29  0.03  0.32  0.12  0.26  0.11  0.21  0.29\n",
      "  0.17  0.16  0.    0.19  0.13  0.3   0.16  0.55  0.29  0.17  0.19  0.18\n",
      "  1.    0.28  0.22  0.28  0.26  0.26  0.24  0.2   0.38  0.19  0.29  0.19\n",
      "  0.15  0.15  0.33  0.3   0.51  0.13  0.19  0.13  0.21  0.21  0.21  0.04\n",
      "  0.13  0.12  0.13  0.23  0.21  0.3   0.36  0.19  0.65  0.18  0.21  0.26\n",
      "  0.19  0.15  0.25  0.11  0.13  0.24  0.37  0.18  0.2   0.17  0.24  0.12\n",
      "  0.13  0.17  0.27  0.33  0.42  0.16  0.1   0.25  0.19  0.1   0.36  0.24\n",
      "  0.26  0.14  0.28  0.21  0.14  0.28  0.16  0.18  0.14  0.27  0.23  0.36\n",
      "  0.25  0.17  0.1   0.08  0.32  0.19  0.15  0.26  0.2   0.17  0.63  0.23\n",
      "  0.2   0.33  0.17  0.24  0.2   0.34  0.17  0.16  0.17  0.41  0.18  0.38\n",
      "  0.46  0.57  0.21  0.22  0.25  0.25  0.04  0.13  0.12  0.33  0.56  0.2\n",
      "  0.48  0.18  0.12  0.23  0.16  0.17  0.22  0.01  0.8   0.18  0.57  0.14\n",
      "  0.27  0.39  0.45  0.3   0.18  0.19  0.14  0.4   0.49  0.14  0.12  0.15\n",
      "  0.29  0.32  0.24  0.23  0.12  0.41  0.23]\n",
      "\n",
      "____________________________feature number:105____________________________\n",
      "size of feature 105 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.01 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 105 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.01  0.03  0.33  0.03  0.01  0.03  0.28  0.05  0.02  0.02  0.02  0.07\n",
      "  0.01  0.03  0.03  0.02  0.02  0.02  0.01  0.01  0.02  0.11  0.04  0.\n",
      "  0.28  0.03  0.01  0.03  0.03  0.04  0.04  0.03  0.09  0.06  0.05  0.52\n",
      "  0.15  0.01  0.05  0.03  0.03  0.02  0.02  0.05  0.05  0.17  0.04  0.23\n",
      "  0.    0.02  0.18  0.3   0.07  0.03  0.03  0.05  0.05  0.06  0.42  0.02\n",
      "  0.04  0.2   0.02  0.1   0.01  0.04  0.    0.02  0.    0.08  0.1   0.03\n",
      "  0.17  0.03  0.05  0.01  0.02  0.08  0.01  0.04  0.09  0.07  0.04  0.06\n",
      "  0.07  0.06  0.02  0.01  0.04  0.03  0.01  0.03  0.06  0.08  0.02  0.\n",
      "  0.01  0.02  0.04  0.74  0.17  0.07  0.13  0.05  0.16  0.05  0.02  0.03\n",
      "  0.03  0.02  0.06  0.06  0.03  0.01  0.66  0.05  0.02  0.08  0.01  0.04\n",
      "  0.03  0.01  0.07  0.02  0.07  0.07  0.03  0.06  0.44  0.33  0.03  0.16\n",
      "  0.04  0.31  0.09  0.01  0.13  0.02  0.23  0.16  0.03  0.08  0.03  0.02\n",
      "  0.04  0.5   0.02  0.02  0.02  0.04  0.03  0.02  0.    0.05  0.12  0.01\n",
      "  0.04  0.03  0.11  0.01  0.05  0.01  0.03  0.09  0.02  0.04  0.04  0.04\n",
      "  0.03  0.03  0.02  0.02  0.1   0.02  0.06  0.07  0.05  0.03  0.29  0.16\n",
      "  0.18  1.    0.02  0.01  0.05  0.14  0.02  0.05  0.05  0.26  0.06  0.01\n",
      "  0.05  0.43  0.02  0.11  0.29  0.03  0.04  0.09  0.02  0.03  0.02  0.02\n",
      "  0.06  0.07  0.07  0.13  0.05  0.24  0.09  0.02  0.02  0.08  0.02  0.1\n",
      "  0.02  0.32  0.78  0.05  0.4   0.01  0.01  0.15  0.06  0.58  0.07  0.04\n",
      "  0.18  0.05  0.02  0.03  0.08  0.03  0.08  0.04  0.35  0.03  0.02  0.04\n",
      "  0.02  0.26  0.11  0.74  0.1   0.49  0.01  0.02  0.04  0.63  0.02  0.02\n",
      "  0.02  0.04  0.01  0.02  0.1   0.08  0.06  0.05  0.26  0.02  0.05  0.06\n",
      "  0.02  0.32  0.03  0.01  0.24  0.03  0.02  0.04  0.06  0.28  0.03  0.45\n",
      "  0.26  1.    0.03  0.08  0.04  0.02  0.09  0.04  0.05  0.06  1.    0.05\n",
      "  0.03  0.03  0.11  0.06  0.03  0.07  0.02  0.01  0.1   0.03  0.2   0.18\n",
      "  0.01  0.    0.07  0.32  0.05  0.01  0.02  1.    0.15  0.04  0.03  0.01\n",
      "  0.01  0.03  0.1   0.03  0.03  0.04  0.03]\n",
      "\n",
      "____________________________feature number:106____________________________\n",
      "size of feature 106 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.13 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 106 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.13  0.09  0.17  0.1   0.08  0.12  0.18  0.2   0.14  0.14  0.09  0.13\n",
      "  0.03  0.08  0.15  0.2   0.4   0.11  0.16  0.12  0.12  0.1   0.25  0.06\n",
      "  0.46  0.19  0.15  0.25  0.14  0.18  0.26  0.2   0.34  0.1   0.26  0.4\n",
      "  0.14  0.05  0.18  0.2   0.07  0.08  0.13  0.19  0.31  0.17  0.14  0.38\n",
      "  0.02  0.6   0.21  0.09  0.27  0.21  0.14  0.25  0.29  0.29  0.54  0.15\n",
      "  0.13  0.52  0.17  0.13  0.17  0.26  0.12  0.29  0.02  0.36  0.28  0.13\n",
      "  0.53  0.14  0.17  0.09  0.11  0.2   0.02  0.18  1.    0.24  0.19  0.18\n",
      "  0.29  0.26  0.17  0.12  0.17  0.18  0.08  0.08  0.17  0.32  0.11  0.08\n",
      "  0.17  0.18  0.38  0.39  0.12  0.19  0.28  0.13  0.1   0.1   0.11  0.15\n",
      "  0.07  0.2   0.3   0.12  0.14  0.08  0.34  0.11  0.14  0.14  0.05  0.14\n",
      "  0.26  0.08  0.14  0.1   0.23  0.18  0.24  0.26  0.3   0.12  0.12  0.17\n",
      "  0.17  0.27  0.38  0.11  0.27  0.21  0.32  0.25  0.14  1.    0.19  0.84\n",
      "  0.19  0.51  0.16  0.1   0.18  0.13  0.22  0.17  0.    0.21  0.72  0.14\n",
      "  0.17  0.14  0.05  0.06  0.15  0.02  0.22  0.36  0.2   0.09  0.17  0.15\n",
      "  0.13  0.12  0.1   0.1   0.19  0.12  0.13  0.43  0.34  0.19  0.45  0.13\n",
      "  1.    0.3   0.14  0.2   0.17  0.4   0.15  0.32  0.21  0.22  0.28  0.1\n",
      "  0.13  0.37  0.23  0.11  0.61  0.17  0.17  0.19  0.15  0.12  0.13  0.02\n",
      "  0.21  0.19  0.13  0.46  0.12  0.18  0.48  0.08  0.41  0.21  0.14  0.4\n",
      "  0.13  0.19  0.28  0.08  0.47  0.13  0.19  0.15  0.2   0.58  0.36  0.1\n",
      "  0.23  0.22  0.15  0.05  0.42  0.11  0.22  0.13  0.19  0.09  0.23  0.18\n",
      "  0.13  0.19  0.23  0.25  0.19  0.37  0.07  0.11  0.15  0.37  0.15  0.18\n",
      "  0.14  0.22  0.06  0.05  0.23  0.22  0.13  0.21  0.49  0.14  0.34  0.28\n",
      "  0.12  0.42  0.12  0.13  0.19  0.24  0.14  0.09  0.23  0.34  0.25  0.84\n",
      "  0.18  0.69  0.14  0.35  0.26  0.15  0.08  0.14  0.11  0.22  1.    0.25\n",
      "  0.43  0.12  0.18  0.14  0.04  0.31  0.15  0.    0.77  0.17  1.    0.21\n",
      "  0.08  0.07  0.31  0.27  0.18  0.09  0.08  0.22  0.32  0.11  0.07  0.07\n",
      "  0.12  0.21  0.24  0.17  0.1   0.23  0.17]\n",
      "\n",
      "____________________________feature number:107____________________________\n",
      "size of feature 107 before delete NaNs:1994\n",
      "Before Delete NaNs:[ nan  nan  0.2 ...,  nan  nan  nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 107 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.2   0.21  0.37  0.39  0.2   0.34  0.51  0.38  0.25  0.21  0.23  0.3\n",
      "  0.17  0.18  0.27  0.29  0.36  0.21  0.2   0.06  0.23  0.74  0.38  0.02\n",
      "  0.4   0.31  0.14  0.31  0.21  0.24  0.45  0.25  0.47  0.2   0.25  0.39\n",
      "  0.46  0.13  0.45  0.38  0.22  0.23  0.22  0.66  0.35  0.32  0.47  0.49\n",
      "  0.02  0.2   0.34  0.28  0.49  0.34  0.28  0.55  0.75  0.54  1.    0.22\n",
      "  0.19  0.63  0.22  0.36  0.23  0.48  0.02  0.25  0.02  0.49  0.86  0.24\n",
      "  0.52  0.24  0.33  0.13  0.2   0.24  0.09  0.45  0.24  0.62  0.28  0.44\n",
      "  0.9   0.7   0.27  0.15  0.32  0.29  0.13  0.26  0.42  0.65  0.23  0.07\n",
      "  0.15  0.14  0.51  0.33  0.4   0.56  0.58  0.28  0.1   0.23  0.17  0.31\n",
      "  0.14  0.31  0.56  0.24  0.37  0.08  0.5   0.12  0.23  0.18  0.1   0.47\n",
      "  0.41  0.22  0.25  0.29  0.24  0.26  0.41  0.37  0.11  0.26  0.3   0.26\n",
      "  0.48  0.65  0.39  0.14  0.87  0.09  0.43  0.3   0.26  0.3   0.23  0.18\n",
      "  0.33  0.68  0.28  0.31  0.33  0.25  0.3   0.35  0.    0.24  1.    0.2\n",
      "  0.19  0.25  0.39  0.18  0.19  0.1   0.29  1.    0.3   0.29  0.34  0.18\n",
      "  0.29  0.3   0.19  0.2   0.56  0.15  0.3   0.35  0.43  0.42  1.    0.27\n",
      "  0.37  0.45  0.25  0.26  0.25  0.67  0.25  0.54  0.23  0.47  0.37  0.2\n",
      "  0.31  0.99  0.28  0.14  0.34  0.51  0.35  0.54  0.24  0.22  0.21  0.17\n",
      "  0.67  0.55  0.38  0.78  0.23  0.21  0.51  0.17  0.16  0.46  0.26  0.64\n",
      "  0.23  0.3   0.39  0.26  1.    0.22  0.21  0.34  0.37  1.    0.61  0.31\n",
      "  0.68  0.46  0.21  0.02  0.45  0.25  0.81  0.2   0.38  0.28  0.26  0.28\n",
      "  0.19  0.43  0.27  0.5   0.37  0.5   0.15  0.22  0.41  0.61  0.26  0.19\n",
      "  0.23  0.45  0.18  0.23  0.27  0.26  0.31  0.32  0.84  0.32  0.16  0.44\n",
      "  0.23  0.52  0.22  0.2   0.37  0.3   0.33  0.22  0.55  0.35  0.52  0.95\n",
      "  0.15  0.58  0.25  0.61  0.46  0.24  0.57  0.43  0.35  0.27  0.54  0.47\n",
      "  0.4   0.24  0.61  0.23  0.09  0.66  0.25  0.08  0.41  0.38  1.    0.57\n",
      "  0.11  0.05  0.24  0.37  0.4   0.19  0.21  0.2   0.26  0.3   0.22  0.17\n",
      "  0.15  0.25  0.44  0.27  0.34  0.24  0.29]\n",
      "\n",
      "____________________________feature number:108____________________________\n",
      "size of feature 108 before delete NaNs:1994\n",
      "Before Delete NaNs:[ nan  nan  0.2 ...,  nan  nan  nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 108 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.2   0.12  0.14  0.07  0.11  0.1   0.1   0.17  0.17  0.2   0.12  0.13\n",
      "  0.03  0.13  0.18  0.22  0.38  0.15  0.26  0.5   0.16  0.02  0.22  0.5\n",
      "  0.41  0.2   0.33  0.28  0.2   0.24  0.19  0.26  0.24  0.15  0.35  0.35\n",
      "  0.09  0.11  0.12  0.17  0.08  0.1   0.18  0.08  0.3   0.17  0.09  0.26\n",
      "  0.2   1.    0.2   0.1   0.18  0.2   0.16  0.14  0.12  0.18  0.11  0.22\n",
      "  0.21  0.29  0.25  0.11  0.25  0.18  0.98  0.39  0.17  0.25  0.1   0.17\n",
      "  0.35  0.19  0.16  0.21  0.18  0.27  0.07  0.13  1.    0.12  0.23  0.13\n",
      "  0.1   0.12  0.2   0.26  0.17  0.2   0.19  0.09  0.12  0.16  0.14  0.3\n",
      "  0.38  0.4   0.25  0.4   0.09  0.1   0.16  0.15  0.29  0.13  0.19  0.14\n",
      "  0.15  0.2   0.18  0.16  0.11  0.28  0.23  0.28  0.2   0.25  0.15  0.09\n",
      "  0.21  0.1   0.18  0.1   0.32  0.23  0.2   0.24  0.85  0.14  0.12  0.21\n",
      "  0.1   0.13  0.34  0.25  0.09  0.68  0.25  0.27  0.16  1.    0.26  1.\n",
      "  0.18  0.26  0.18  0.1   0.17  0.17  0.25  0.15  0.04  0.28  0.21  0.23\n",
      "  0.3   0.18  0.02  0.1   0.24  0.05  0.26  0.11  0.22  0.08  0.16  0.27\n",
      "  0.13  0.12  0.16  0.15  0.1   0.26  0.13  0.43  0.27  0.14  0.14  0.15\n",
      "  1.    0.23  0.18  0.25  0.22  0.2   0.19  0.19  0.3   0.15  0.25  0.15\n",
      "  0.12  0.12  0.27  0.25  0.64  0.09  0.15  0.1   0.19  0.17  0.2   0.03\n",
      "  0.09  0.1   0.1   0.2   0.16  0.28  0.33  0.15  0.83  0.14  0.17  0.21\n",
      "  0.17  0.21  0.24  0.08  0.1   0.18  0.28  0.14  0.17  0.15  0.19  0.09\n",
      "  0.1   0.15  0.24  0.39  0.33  0.14  0.08  0.21  0.16  0.09  0.29  0.21\n",
      "  0.2   0.14  0.27  0.16  0.16  0.25  0.13  0.15  0.11  0.2   0.19  0.31\n",
      "  0.19  0.15  0.08  0.06  0.28  0.29  0.12  0.22  0.2   0.14  0.72  0.22\n",
      "  0.16  0.28  0.16  0.2   0.17  0.26  0.13  0.13  0.13  0.33  0.15  0.31\n",
      "  0.37  0.43  0.18  0.19  0.19  0.21  0.02  0.09  0.08  0.27  0.68  0.17\n",
      "  0.38  0.16  0.09  0.19  0.13  0.15  0.19  0.    0.67  0.14  0.55  0.11\n",
      "  0.21  0.37  0.43  0.25  0.14  0.14  0.11  0.35  0.42  0.11  0.1   0.13\n",
      "  0.24  0.28  0.18  0.19  0.08  0.32  0.19]\n",
      "\n",
      "____________________________feature number:109____________________________\n",
      "size of feature 109 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.89 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 109 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.89  0.62  0.8   0.56  0.32  1.    0.99  0.74  0.57  0.24  0.78  0.78\n",
      "  0.63  0.76  0.86  0.93  0.92  0.95  0.82  0.78  0.29  0.    0.94  0.33\n",
      "  0.17  0.68  0.53  0.41  0.7   0.53  0.62  0.88  0.    0.63  0.22  0.51\n",
      "  0.75  0.83  0.95  0.94  0.85  0.95  0.84  0.36  0.34  0.77  0.43  0.62\n",
      "  0.98  0.95  0.95  0.84  0.28  0.78  0.95  0.82  0.7   0.92  0.94  0.55\n",
      "  0.65  0.46  0.    1.    0.67  0.62  0.66  1.    0.91  0.7   0.37  0.77\n",
      "  0.35  0.62  0.82  0.77  0.57  0.6   0.57  0.86  0.9   0.58  0.76  0.94\n",
      "  0.93  0.86  0.36  0.83  0.85  0.76  0.4   0.53  0.46  0.91  0.83  1.\n",
      "  0.27  0.7   0.28  0.28  0.69  0.84  0.73  0.8   0.56  0.93  0.74  0.89\n",
      "  0.96  0.9   0.87  0.55  0.84  0.36  0.63  0.65  0.98  0.48  0.36  0.24\n",
      "  0.77  0.51  0.72  0.75  0.45  0.62  0.63  0.72  0.94  1.    0.85  0.66\n",
      "  0.7   0.46  1.    0.91  0.84  0.12  0.39  0.32  0.76  0.51  0.69  0.96\n",
      "  0.97  0.84  0.6   0.96  0.86  0.86  0.7   1.    0.56  0.77  0.78  0.73\n",
      "  0.97  0.94  0.7   0.88  0.31  0.84  0.36  0.57  0.85  0.89  0.98  0.73\n",
      "  0.89  0.66  0.58  0.71  0.67  0.74  0.56  0.5   0.9   0.93  0.75  0.81\n",
      "  0.58  0.78  0.79  0.09  0.51  0.74  0.91  0.45  0.61  0.98  0.96  0.77\n",
      "  0.69  0.81  0.36  0.52  0.34  0.    0.58  0.66  0.36  0.    0.79  0.    0.9\n",
      "  0.9   0.93  0.83  0.53  0.46  0.22  0.78  0.78  0.83  0.8   0.94  0.22\n",
      "  0.48  0.5   0.77  0.94  0.62  0.45  0.63  0.29  0.49  0.97  0.95  0.9\n",
      "  0.86  0.8   0.87  0.71  0.9   0.61  0.98  0.82  0.81  0.45  0.86  0.53\n",
      "  0.9   0.48  0.93  0.58  0.9   0.83  0.94  0.76  0.76  0.82  1.    0.78\n",
      "  0.65  0.69  0.52  0.64  0.75  0.62  0.71  0.68  0.74  0.67  0.67  0.88\n",
      "  0.87  0.77  0.88  0.65  0.78  0.79  0.78  0.62  0.76  0.57  0.36  0.76\n",
      "  0.56  0.95  0.55  0.95  0.83  0.94  0.62  0.8   0.55  0.99  0.82  0.95\n",
      "  0.79  0.31  0.77  0.72  0.65  0.98  0.54  0.58  0.53  0.51  0.78  0.81\n",
      "  0.81  0.11  0.61  0.99  0.85  0.71  0.48  0.54  0.45  0.74  0.42  0.53\n",
      "  0.96  0.84  0.61  0.63  0.77  0.61]\n",
      "\n",
      "____________________________feature number:110____________________________\n",
      "size of feature 110 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.97 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 110 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.97  0.84  0.79  0.88  0.83  0.    0.    0.89  0.72  0.66  0.95  0.8\n",
      "  0.71  0.92  0.83  1.    0.88  0.98  0.76  0.97  0.72  0.7   0.99  0.69\n",
      "  0.79  0.68  0.97  0.52  0.65  0.79  0.69  0.93  0.06  0.77  0.29  0.55\n",
      "  0.52  0.85  0.98  0.79  0.92  0.98  0.94  0.4   0.37  0.78  0.7   0.64\n",
      "  0.91  0.99  0.66  0.57  0.75  0.99  0.99  0.76  0.91  0.9   0.53  0.46\n",
      "  0.75  0.92  0.68  0.27  0.69  0.88  0.69  0.65  0.99  0.53  0.78  0.65\n",
      "  0.52  0.94  0.91  0.94  0.99  0.89  0.61  0.92  0.85  0.69  0.86  0.93\n",
      "  0.93  0.79  0.47  1.    0.82  0.77  0.51  0.49  0.73  0.37  0.96  0.\n",
      "  0.46  0.8   0.79  0.56  0.82  0.68  0.93  0.23  0.72  0.86  0.9   0.97\n",
      "  0.66  0.87  0.98  0.68  0.96  0.84  0.52  0.88  0.98  0.65  0.93  0.41\n",
      "  0.77  0.16  0.56  0.92  0.67  0.72  0.73  0.7   0.    0.3   0.88  0.87\n",
      "  0.72  0.72  0.71  0.91  0.66  0.74  0.43  0.81  0.96  0.85  0.86  0.92\n",
      "  0.99  0.74  0.85  0.88  0.98  0.93  0.94  0.89  0.88  0.72  0.97  1.    0.7\n",
      "  0.97  0.61  0.96  0.57  0.21  0.91  0.    0.94  0.84  0.76  0.81  0.83\n",
      "  0.63  0.62  0.71  0.8   0.76  0.73  0.69  0.82  0.85  0.89  0.88  0.67\n",
      "  0.4   0.97  0.32  0.5   0.55  0.98  0.5   0.89  0.61  0.97  0.96  0.89\n",
      "  0.82  0.3   0.72  0.32  0.82  0.84  0.87  0.92  0.44  0.81  0.78  0.93\n",
      "  0.    0.92  0.75  0.9   0.36  0.1   0.    0.92  0.92  0.97  0.87  0.71\n",
      "  0.49  0.64  0.83  0.87  0.63  0.87  0.46  0.8   0.29  0.68  0.97  0.79\n",
      "  0.76  0.94  0.61  0.59  0.75  0.59  0.79  0.7   1.    0.73  0.85  0.89\n",
      "  0.6   0.72  0.75  0.41  0.62  0.83  0.87  0.91  0.77  0.81  0.69  0.91\n",
      "  0.89  0.76  0.51  0.78  0.52  0.82  0.83  0.51  0.84  0.94  0.33  0.84\n",
      "  0.62  0.    0.9   0.82  0.54  0.95  0.89  0.78  0.59  0.85  0.56  0.59\n",
      "  0.58  0.72  0.28  0.92  0.9   0.36  0.48  0.82  0.78  0.35  0.54  0.79\n",
      "  0.76  0.84  0.96  0.72  0.79  0.85  0.78  0.93  0.58  0.6   0.88  0.97\n",
      "  0.99  0.73  0.75  0.68  0.69  0.8   0.62  0.84  0.74  0.67  0.53  0.7\n",
      "  0.62  0.87  0.69  0.48  0.46  0.85]\n",
      "\n",
      "____________________________feature number:111____________________________\n",
      "size of feature 111 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.04 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 111 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.04  0.04  0.19  0.09  0.04  0.    0.06  0.01  0.15  0.55  0.04  0.03\n",
      "  0.46  0.03  0.18  0.    0.17  0.02  0.    0.02  0.38  0.45  0.02  0.49\n",
      "  0.18  0.51  0.04  0.63  0.29  0.08  0.44  0.05  0.02  0.33  0.86  0.62\n",
      "  0.2   0.03  0.02  0.12  0.06  0.04  0.08  0.09  0.18  0.23  0.06  0.34\n",
      "  0.11  0.    0.43  0.19  0.36  0.02  0.02  0.06  0.1   0.07  0.06  0.48\n",
      "  0.31  0.13  0.43  0.1   0.49  0.16  0.36  0.12  0.02  0.68  0.09  0.28\n",
      "  0.41  0.05  0.13  0.07  0.01  0.17  0.41  0.07  0.05  0.06  0.18  0.03\n",
      "  0.    0.15  0.53  0.    0.03  0.02  0.04  0.04  0.42  0.54  0.07  0.8\n",
      "  0.02  0.22  0.34  0.67  0.12  0.07  0.12  0.44  0.3   0.15  0.09  0.03\n",
      "  0.45  0.13  0.    0.5   0.    0.04  0.22  0.13  0.    0.54  0.11  0.71\n",
      "  0.36  0.07  0.7   0.02  0.44  0.42  0.28  0.43  1.    0.15  0.06  0.08\n",
      "  0.08  0.13  0.04  0.11  0.1   0.41  0.91  0.27  0.    0.19  0.13  0.08\n",
      "  0.02  0.4   0.02  0.02  0.03  0.1   0.08  0.06  0.19  0.43  0.03  0.\n",
      "  0.15  0.02  0.09  0.    0.38  1.    0.13  1.    0.07  0.03  0.15  0.1\n",
      "  0.1   0.05  0.59  0.09  0.12  0.1   0.41  0.33  0.29  0.14  0.06  0.08\n",
      "  0.52  0.34  0.04  0.    0.75  0.46  0.    0.1   0.15  0.08  0.05  0.07\n",
      "  0.06  0.2   0.87  0.42  0.07  0.06  0.25  0.04  0.04  0.51  0.24  0.29\n",
      "  0.08  0.    0.06  0.38  0.14  1.    1.    0.    0.06  0.03  0.05  0.16\n",
      "  0.46  0.81  0.39  0.11  0.03  0.06  0.14  0.14  0.27  0.64  0.33  0.\n",
      "  0.17  0.07  0.09  0.2   0.28  0.07  0.12  0.15  0.45  0.    0.27  0.07\n",
      "  0.12  0.25  0.43  0.09  0.94  0.17  0.03  0.08  0.02  0.36  0.16  0.06\n",
      "  0.12  0.18  0.    0.78  0.22  0.37  0.27  0.2   0.77  0.26  0.05  0.99\n",
      "  0.05  0.3   1.    0.    0.19  0.28  0.02  0.04  0.03  0.49  0.05  0.66\n",
      "  0.52  0.59  0.42  0.71  0.08  0.07  0.11  0.39  0.04  0.13  0.38  0.24\n",
      "  0.06  0.09  0.23  0.01  0.45  0.16  0.11  0.3   0.08  0.09  0.12  0.12\n",
      "  0.04  0.    0.34  0.33  0.    0.16  0.    0.28  0.16  0.34  0.08  0.02\n",
      "  0.49  0.05  0.06  0.47  0.04  0.78  0.24]\n",
      "\n",
      "____________________________feature number:112____________________________\n",
      "size of feature 112 before delete NaNs:1994\n",
      "Before Delete NaNs:[ nan  nan   0. ...,  nan  nan  nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 112 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.    0.15  0.1   0.07  0.16  1.    1.    0.16  0.31  0.    0.02  0.3   0.\n",
      "  0.11  0.09  0.    0.03  0.    0.4   0.01  0.07  0.03  0.    0.    0.16\n",
      "  0.    0.    0.12  0.01  0.03  0.06  0.06  0.01  0.01  0.28  0.09  0.53\n",
      "  0.19  0.    0.17  0.06  0.    0.02  0.19  0.05  0.01  0.3   0.19  0.04\n",
      "  0.    0.11  0.39  0.01  0.    0.    0.25  0.05  0.09  0.65  0.4   0.04\n",
      "  0.    0.08  1.    0.    0.    0.14  0.46  0.    0.    0.23  0.28  0.36\n",
      "  0.03  0.01  0.    0.    0.    0.    0.04  0.17  0.33  0.04  0.07  0.07\n",
      "  0.13  0.    0.    0.06  0.08  0.04  0.17  0.    0.16  0.    0.83  0.\n",
      "  0.11  0.    0.01  0.04  0.34  0.    0.54  0.12  0.08  0.07  0.02  0.09\n",
      "  0.03  0.04  0.    0.    0.23  0.27  0.06  0.    0.02  0.    0.    0.\n",
      "  0.83  0.01  0.11  0.09  0.01  0.03  0.04  0.07  1.    0.08  0.06  0.11\n",
      "  0.27  0.44  0.04  0.31  0.    0.    0.02  0.06  0.04  0.04  0.04  0.\n",
      "  0.02  0.2   0.13  0.    0.    0.02  0.12  0.    0.02  0.    0.    0.34\n",
      "  0.03  0.46  0.    0.31  0.    0.02  0.33  0.03  0.23  0.24  0.19  0.    0.1\n",
      "  0.    0.04  0.17  0.02  0.02  0.    0.    0.05  0.09  0.05  0.01  0.55\n",
      "  0.    0.02  0.04  0.04  0.03  0.06  0.01  0.53  0.    0.    0.1   0.04\n",
      "  0.23  0.01  0.21  0.21  0.    0.    0.1   0.29  0.02  0.07  0.    1.\n",
      "  0.03  0.02  0.01  0.02  0.05  1.    0.03  0.09  0.    0.04  0.    0.\n",
      "  0.16  0.05  0.16  0.09  0.07  0.28  0.05  0.28  0.16  0.03  0.16  0.32\n",
      "  0.    0.38  0.15  0.34  0.42  0.14  0.03  0.    0.13  0.16  0.06  0.39\n",
      "  0.01  0.29  0.    0.43  0.16  0.11  0.07  0.    0.14  0.46  0.02  0.\n",
      "  0.33  0.    0.12  0.4   0.01  0.07  0.    0.    0.    0.03  0.04  0.3\n",
      "  0.18  0.02  0.03  0.42  0.02  0.15  0.14  0.16  0.03  0.04  0.13  0.07\n",
      "  0.02  0.27  0.04  0.07  0.9   0.28  0.19  0.22  0.67  0.43  0.19  0.3\n",
      "  0.03  0.05  0.    0.16  0.14  0.05  0.02  0.02  0.07  0.07  0.    0.02\n",
      "  0.09  0.06  0.52  0.23  0.26  0.31  0.1   0.07  0.29  0.04  0.    0.57\n",
      "  0.09  0.03  0.47  0.07  0.  ]\n",
      "\n",
      "____________________________feature number:113____________________________\n",
      "size of feature 113 before delete NaNs:1994\n",
      "Before Delete NaNs:[ nan  nan   0. ...,  nan  nan  nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 113 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.    0.38  0.21  0.32  0.49  0.    0.1   0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.09  0.    0.    0.    0.    0.    0.\n",
      "  0.    0.12  0.18  0.    0.    0.    0.    0.    0.06  0.01  0.26  0.12\n",
      "  0.12  0.3   0.    0.    0.    0.    0.    0.05  0.98  0.15  0.    0.12\n",
      "  0.    0.6   0.18  0.    0.    0.67  0.    0.    0.39  0.    0.22  0.    0.\n",
      "  0.15  0.    0.15  0.    0.    0.    0.51  0.32  0.    0.    0.12  0.    0.\n",
      "  0.    0.    0.    0.    0.04  0.69  0.    0.1   0.    0.15  0.    0.\n",
      "  0.11  0.    0.    0.52  0.    1.    0.    0.    0.    0.    0.    0.06\n",
      "  0.81  0.4   0.    0.23  0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    1.    0.04  0.14  0.    0.    0.    0.    1.    0.    0.    0.\n",
      "  0.05  0.75  0.    0.14  0.04  0.    0.09  0.    0.26  0.    0.    1.    0.\n",
      "  0.05  0.08  0.    0.    0.19  0.    0.    0.    0.    0.35  0.    0.    0.\n",
      "  0.    0.    0.    0.    0.    0.    0.    0.48  0.    0.05  0.    0.\n",
      "  0.59  0.    0.    0.    0.    1.    0.56  0.    0.    0.16  0.    0.07\n",
      "  0.    0.    0.    0.09  0.31  0.    0.52  0.    0.    0.    0.    0.    0.\n",
      "  0.06  0.12  0.    0.    0.08  0.22  0.15  0.06  0.16  0.21  0.    0.8   0.\n",
      "  0.72  0.16  0.    0.14  0.    0.08  0.    0.    0.03  0.    0.    0.12\n",
      "  0.08  0.    0.08  0.    0.    0.    0.49  0.05  0.    0.    1.    0.    1.\n",
      "  0.11  0.11  0.    0.    0.    0.27  0.    0.    0.51  0.13  0.05  0.    0.3\n",
      "  0.    0.    0.07  0.04  0.1   0.    0.03  0.38  0.15  0.26  0.    0.    0.\n",
      "  0.    0.    0.18  0.    0.04  0.    0.07  0.    0.13  0.    0.14  0.1\n",
      "  0.13  0.04  0.    0.    0.09  0.23  0.17  0.    0.37  0.    0.    0.\n",
      "  0.11  0.    0.    0.93  0.    0.    0.    1.    0.19  0.    0.01  0.35\n",
      "  0.74  0.    0.    0.05  0.    0.13  0.    0.    0.06  0.    0.17  0.    0.\n",
      "  0.    0.    0.03  0.    0.8   0.37  0.13  0.    0.    0.59  0.    0.    0.\n",
      "  0.12  0.    0.29  0.07  0.  ]\n",
      "\n",
      "____________________________feature number:114____________________________\n",
      "size of feature 114 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.03 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 114 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.03  0.16  0.22  0.14  0.19  1.    1.    0.07  0.31  0.38  0.04  0.22\n",
      "  0.32  0.09  0.19  0.    0.14  0.01  0.26  0.03  0.31  0.33  0.01  0.34\n",
      "  0.22  0.35  0.03  0.53  0.23  0.07  0.34  0.05  0.02  0.24  0.79  0.49\n",
      "  0.52  0.15  0.02  0.21  0.08  0.03  0.07  0.19  0.16  0.17  0.33  0.36\n",
      "  0.1   0.01  0.37  0.43  0.27  0.01  0.01  0.27  0.1   0.11  0.5   0.6\n",
      "  0.26  0.09  0.35  0.81  0.34  0.13  0.34  0.39  0.01  0.52  0.23  0.38\n",
      "  0.53  0.07  0.1   0.05  0.01  0.12  0.29  0.07  0.15  0.33  0.15  0.07\n",
      "  0.05  0.2   0.37  0.    0.07  0.06  0.06  0.19  0.29  0.67  0.05  1.\n",
      "  0.01  0.22  0.23  0.48  0.18  0.3   0.08  0.67  0.28  0.16  0.11  0.03\n",
      "  0.37  0.11  0.02  0.35  0.    0.18  0.52  0.13  0.01  0.39  0.08  0.49\n",
      "  0.25  0.93  0.49  0.09  0.37  0.31  0.28  0.33  1.    0.78  0.1   0.1\n",
      "  0.13  0.29  0.32  0.09  0.37  0.28  0.63  0.21  0.04  0.16  0.13  0.08\n",
      "  0.01  0.29  0.15  0.1   0.02  0.07  0.07  0.12  0.13  0.31  0.02  0.\n",
      "  0.34  0.03  0.33  0.    0.47  0.88  0.1   1.    0.07  0.17  0.27  0.2\n",
      "  0.17  0.13  0.41  0.08  0.21  0.08  0.3   0.23  0.2   0.13  0.11  0.12\n",
      "  0.36  0.66  0.03  0.02  0.55  0.35  0.02  0.1   0.12  0.42  0.04  0.05\n",
      "  0.12  0.19  0.69  0.3   0.17  0.2   0.18  0.11  0.08  0.61  0.19  0.25\n",
      "  0.07  1.    0.07  0.28  0.1   0.7   1.    1.    0.08  0.09  0.04  0.13\n",
      "  0.32  0.56  0.29  0.15  0.13  0.07  0.15  0.4   0.22  0.78  0.34  0.03\n",
      "  0.23  0.26  0.06  0.41  0.3   0.28  0.41  0.21  0.33  0.    0.3   0.15\n",
      "  0.13  0.44  0.3   0.27  0.65  0.38  0.17  0.14  0.09  0.25  0.19  0.34\n",
      "  0.1   0.12  0.23  0.54  0.23  0.53  0.2   0.18  0.54  0.18  0.05  0.72\n",
      "  0.08  0.41  1.    0.02  0.16  0.5   0.05  0.13  0.15  0.43  0.05  0.48\n",
      "  0.46  0.46  0.3   0.76  0.09  0.1   0.65  0.55  0.18  0.24  0.71  0.48\n",
      "  0.22  0.26  0.18  0.04  0.31  0.18  0.17  0.24  0.07  0.06  0.14  0.13\n",
      "  0.03  0.01  0.3   0.27  0.35  0.34  0.21  0.41  0.17  0.28  0.3   0.04\n",
      "  0.34  0.39  0.09  0.35  0.37  0.6   0.17]\n",
      "\n",
      "____________________________feature number:115____________________________\n",
      "size of feature 115 before delete NaNs:1994\n",
      "Before Delete NaNs:[ nan  nan   0. ...,  nan  nan  nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 115 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.    0.03  0.11  0.01  0.08  0.03  0.13  0.07  0.01  0.02  0.04  0.05\n",
      "  0.01  0.05  0.02  0.01  0.03  0.03  0.02  0.04  0.02  0.04  0.03  0.04\n",
      "  0.18  0.02  0.04  0.02  0.06  0.05  0.01  0.03  0.18  0.11  0.03  0.22\n",
      "  0.15  0.03  0.02  0.03  0.01  0.01  0.02  0.04  0.05  0.12  0.04  0.12\n",
      "  0.02  0.03  0.11  0.18  0.05  0.06  0.03  0.02  0.02  0.04  0.08  0.03\n",
      "  0.1   0.12  0.01  0.08  0.01  0.04  0.06  0.05  0.    0.01  0.03  0.05\n",
      "  0.19  0.04  0.03  0.02  0.03  0.12  0.02  0.02  0.11  0.04  0.03  0.02\n",
      "  0.03  0.03  0.01  0.02  0.03  0.02  0.01  0.02  0.07  0.09  0.01  0.\n",
      "  0.01  0.01  0.11  0.37  0.07  0.04  0.11  0.15  0.16  0.08  0.04  0.04\n",
      "  0.06  0.03  0.05  0.07  0.02  0.04  0.4   0.11  0.05  0.07  0.02  0.03\n",
      "  0.04  0.01  0.11  0.01  0.04  0.1   0.01  0.05  0.73  0.11  0.03  0.06\n",
      "  0.02  0.1   0.07  0.01  0.04  0.1   0.1   0.19  0.02  0.18  0.07  0.05\n",
      "  0.02  0.16  0.03  0.03  0.04  0.03  0.04  0.01  0.07  0.04  0.03  0.01\n",
      "  0.1   0.03  0.02  0.04  0.13  0.02  0.05  0.04  0.    0.07  0.05  0.05\n",
      "  0.03  0.03  0.03  0.01  0.09  0.01  0.08  0.45  0.06  0.07  0.08  0.14\n",
      "  0.16  1.    0.03  0.01  0.07  0.07  0.02  0.1   0.1   0.11  0.05  0.02\n",
      "  0.04  0.17  0.03  0.17  0.16  0.01  0.07  0.04  0.04  0.04  0.02  0.\n",
      "  0.05  0.03  0.06  0.12  0.05  0.26  0.07  0.    0.03  0.04  0.02  0.03\n",
      "  0.04  0.44  0.33  0.06  0.04  0.02  0.05  0.19  0.03  0.08  0.07  0.03\n",
      "  0.1   0.07  0.01  0.07  0.12  0.01  0.02  0.15  0.13  0.05  0.02  0.01\n",
      "  0.    0.08  0.04  0.18  0.05  0.19  0.03  0.03  0.03  0.15  0.02  0.03\n",
      "  0.02  0.03  0.02  0.12  0.18  0.09  0.04  0.04  0.17  0.02  0.1   0.08\n",
      "  0.04  0.15  0.06  0.02  0.11  0.03  0.    0.15  0.02  0.12  0.03  0.11\n",
      "  0.26  1.    0.05  0.11  0.02  0.03  0.06  0.05  0.05  0.08  0.49  0.02\n",
      "  0.02  0.02  0.07  0.09  0.01  0.04  0.02  0.04  0.16  0.02  0.07  0.05\n",
      "  0.01  0.02  0.07  0.16  0.02  0.    0.04  1.    0.26  0.07  0.03  0.01\n",
      "  0.    0.03  0.05  0.07  0.04  0.02  0.03]\n",
      "\n",
      "____________________________feature number:116____________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of feature 116 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.43 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 116 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.43  0.5   0.71  0.71  0.36  0.14  0.36  0.43  0.36  0.64  0.71  0.57\n",
      "  0.29  0.64  0.71  0.64  0.36  0.43  0.5   0.36  0.57  0.43  0.71  0.5\n",
      "  0.71  0.5   0.57  0.29  0.43  0.93  0.64  0.57  0.14  0.86  0.79  0.57\n",
      "  0.93  0.57  0.5   0.43  0.64  0.43  0.29  0.29  0.64  0.93  0.86  0.43\n",
      "  0.64  0.43  0.79  0.93  0.29  0.5   0.43  0.57  0.57  0.64  0.43  0.29\n",
      "  0.57  0.07  0.57  0.57  0.21  0.64  0.36  0.71  0.64  0.29  0.86  0.79\n",
      "  0.43  0.57  0.57  0.71  0.14  0.21  0.64  0.64  0.29  0.5   0.64  0.57\n",
      "  0.64  0.64  0.64  0.71  0.36  0.5   0.64  0.43  0.5   0.64  0.43  0.29\n",
      "  0.36  0.57  0.57  0.64  0.5   0.43  0.5   0.93  0.79  0.57  0.5   0.57\n",
      "  0.71  0.29  0.43  0.86  0.86  0.79  0.93  0.64  0.93  0.5   0.43  0.57\n",
      "  0.71  0.5   0.57  0.36  0.64  0.5   0.64  0.14  0.86  0.79  0.5   0.71\n",
      "  0.57  0.5   0.5   0.57  0.71  0.57  0.57  0.43  0.64  0.71  0.57  0.64\n",
      "  0.79  0.93  0.29  0.71  0.29  0.5   0.5   0.64  0.64  0.57  0.57  0.5\n",
      "  0.36  0.36  0.86  0.43  0.71  0.21  0.5   0.5   0.5   1.    0.36  0.5\n",
      "  0.43  0.57  0.79  0.5   0.57  0.21  0.57  0.5   0.29  0.36  0.79  0.57\n",
      "  0.79  0.86  0.43  0.29  0.5   0.43  0.79  0.64  0.64  0.57  0.21  0.5\n",
      "  0.71  0.57  0.36  0.57  0.43  0.57  0.43  0.93  0.29  0.36  0.43  0.29\n",
      "  0.93  0.14  0.57  0.64  0.5   0.64  0.57  0.29  0.43  0.79  0.43  0.29\n",
      "  0.43  0.79  0.57  0.71  0.5   0.5   0.64  0.79  0.5   0.64  0.29  0.5\n",
      "  0.64  0.71  0.43  0.57  0.93  0.5   0.57  0.79  0.57  0.43  0.71  0.71\n",
      "  0.57  0.57  0.79  0.93  0.57  0.57  0.64  0.86  0.64  1.    0.36  0.14\n",
      "  0.21  0.43  0.5   0.21  0.14  0.93  0.36  0.43  0.64  0.5   0.64  0.79\n",
      "  0.57  0.93  0.36  0.36  0.86  0.5   0.    0.79  0.93  0.79  0.5   0.71\n",
      "  0.57  0.93  0.5   0.5   0.57  0.57  0.64  0.36  0.57  0.93  0.86  0.57\n",
      "  0.    0.36  0.57  0.79  0.86  0.43  0.21  0.57  0.79  0.36  0.86  0.36\n",
      "  0.07  0.64  0.93  0.43  0.07  0.64  0.93  0.64  0.93  0.36  0.93  0.36\n",
      "  0.36  0.36  0.57  0.43  0.79  0.5   0.5 ]\n",
      "\n",
      "____________________________feature number:117____________________________\n",
      "size of feature 117 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.09 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 117 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.09  0.76  0.12  0.9   0.26  0.67  0.23  0.39  0.65  0.41  0.45  0.25\n",
      "  0.    0.06  0.99  0.39  0.44  0.5   0.48  0.11  0.3   0.    0.09  0.\n",
      "  0.17  0.19  0.32  0.25  0.19  0.22  0.02  0.12  0.17  0.15  0.3   0.23\n",
      "  0.86  0.38  0.19  0.09  0.07  0.25  0.19  0.75  0.66  0.23  0.32  0.6\n",
      "  0.17  0.66  0.23  0.97  0.26  0.18  0.31  0.28  0.14  0.43  0.27  1.\n",
      "  0.57  0.4   0.11  0.36  0.37  0.15  0.41  0.    0.67  0.05  0.27  0.16\n",
      "  1.    0.59  0.06  0.12  0.11  0.24  0.05  0.29  0.14  1.    0.08  0.29\n",
      "  0.18  0.35  0.1   0.15  0.14  0.74  0.    0.58  0.09  0.38  0.2   0.65\n",
      "  0.44  0.28  0.39  0.18  0.18  0.54  0.11  0.22  0.57  0.44  0.38  0.15\n",
      "  0.5   0.16  0.05  1.    0.29  0.31  0.16  0.49  0.1   0.27  0.42  0.74\n",
      "  0.39  0.37  0.    0.09  0.43  0.16  0.58  0.55  0.51  0.48  0.02  0.2\n",
      "  0.38  0.72  0.3   0.19  0.04  0.12  0.3   0.17  0.02  0.36  0.33  0.15\n",
      "  0.15  0.2   0.39  0.19  0.3   0.78  0.39  0.16  0.74  0.35  0.5   0.16\n",
      "  0.25  0.41  1.    0.55  0.18  0.07  0.21  0.25  0.14  0.79  0.2   0.17\n",
      "  0.24  0.04  0.38  0.19  0.29  0.24  0.14  0.01  0.46  0.32  0.33  0.34\n",
      "  0.27  0.21  0.09  0.62  0.11  0.28  0.06  0.42  0.22  0.3   0.38  0.45\n",
      "  0.13  0.28  0.21  0.17  0.24  0.47  0.42  0.32  0.31  0.31  0.1   0.66\n",
      "  0.24  0.42  0.35  0.35  0.22  0.54  0.28  0.47  0.27  0.35  0.25  0.65\n",
      "  0.19  0.48  0.26  0.    0.33  0.38  0.06  0.24  0.14  0.42  0.02  0.38\n",
      "  0.11  0.39  0.04  0.38  0.11  0.15  0.52  0.14  1.    0.34  0.2   0.1\n",
      "  0.14  0.16  0.66  0.35  0.13  0.16  1.    0.04  0.34  0.5   0.17  0.3\n",
      "  0.07  0.27  0.92  0.06  0.22  0.01  0.03  0.26  0.34  0.42  0.06  0.1\n",
      "  0.12  0.17  0.21  0.34  0.09  0.32  0.48  0.    0.36  0.07  0.08  0.19\n",
      "  0.84  0.1   0.14  0.32  1.    0.16  0.32  0.76  0.36  0.37  0.43  0.17\n",
      "  0.19  0.1   0.66  0.09  0.23  0.09  0.31  0.06  0.28  0.25  0.18  0.24\n",
      "  0.33  0.37  0.27  0.19  0.1   0.5   0.45  0.63  0.1   0.11  0.36  0.14\n",
      "  0.19  0.12  0.44  0.11  0.33  0.03  0.1 ]\n",
      "\n",
      "____________________________feature number:118____________________________\n",
      "size of feature 118 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.01  0.02  0.09 ...,  0.05  0.06  0.07]\n",
      "number of nan for this feature:0\n",
      "Size of feature 118 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.01  0.02  0.09 ...,  0.05  0.06  0.07]\n",
      "\n",
      "____________________________feature number:119____________________________\n",
      "size of feature 119 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.15  0.17  0.13 ...,  0.1   0.16  0.06]\n",
      "number of nan for this feature:0\n",
      "Size of feature 119 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.15  0.17  0.13 ...,  0.1   0.16  0.06]\n",
      "\n",
      "____________________________feature number:120____________________________\n",
      "size of feature 120 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.05  0.02  0.05 ...,  0.05  0.44  0.31]\n",
      "number of nan for this feature:0\n",
      "Size of feature 120 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.05  0.02  0.05 ...,  0.05  0.44  0.31]\n",
      "\n",
      "____________________________feature number:121____________________________\n",
      "size of feature 121 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.09 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 121 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.09  0.09  0.41  0.04  0.05  0.05  0.58  0.    0.05  0.06  0.03  0.32\n",
      "  0.09  0.15  0.04  0.01  0.    0.01  0.    0.06  0.04  0.17  0.02  0.07\n",
      "  0.29  0.04  0.02  0.03  0.22  0.08  0.08  0.04  0.26  0.26  0.06  0.61\n",
      "  0.13  0.07  0.02  0.09  0.06  0.05  0.    0.04  0.06  0.72  0.04  0.3   0.\n",
      "  0.11  0.23  0.69  0.18  0.06  0.04  0.03  0.03  0.15  0.23  0.06  0.13\n",
      "  0.19  0.04  0.21  0.    0.05  0.07  0.02  0.07  0.16  0.07  0.09  0.15\n",
      "  0.04  0.06  0.03  0.01  0.14  0.1   0.05  0.44  0.04  0.03  0.06  0.05\n",
      "  0.04  0.04  0.02  0.06  0.03  0.04  0.05  0.17  0.05  0.06  0.02  0.07\n",
      "  0.04  0.03  0.67  0.62  0.12  0.35  0.08  0.37  0.1   0.03  0.14  0.12\n",
      "  0.03  0.05  0.24  0.04  0.    0.53  0.21  0.02  0.55  0.04  0.06  0.1\n",
      "  0.01  0.37  0.01  0.09  0.51  0.06  0.06  0.65  0.73  0.12  0.31  0.04\n",
      "  0.2   0.23  0.02  0.08  0.14  0.36  0.18  0.02  0.47  0.1   0.24  0.05\n",
      "  0.45  0.04  0.03  0.03  0.09  0.04  0.    0.11  0.03  0.02  0.01  0.08\n",
      "  0.02  0.09  0.01  0.1   0.04  0.01  0.05  0.01  0.08  0.04  0.11  0.06\n",
      "  0.04  0.07  0.02  0.19  0.02  0.2   0.23  0.09  0.02  0.18  0.37  0.43\n",
      "  1.    0.05  0.01  0.24  0.23  0.02  0.04  0.06  0.55  0.06  0.02  0.08\n",
      "  0.4   0.03  0.38  0.58  0.05  0.11  0.25  0.    0.08  0.12  0.02  0.06\n",
      "  0.08  0.07  0.11  0.25  0.89  0.05  0.08  0.11  0.16  0.02  0.16  0.03\n",
      "  0.64  1.    0.32  0.32  0.01  0.02  0.27  0.13  0.26  0.05  0.1   0.09\n",
      "  0.1   0.02  0.41  0.06  0.16  0.06  0.22  1.    0.05  0.03  0.05  0.02\n",
      "  0.3   0.29  1.    0.22  0.68  0.05  0.11  0.07  0.71  0.02  0.06  0.02\n",
      "  0.05  0.03  0.11  0.19  0.1   0.24  0.03  0.25  0.11  0.32  0.08  0.09\n",
      "  0.71  0.11  0.02  0.68  0.07  0.1   0.11  0.08  0.25  0.05  0.28  0.82\n",
      "  0.92  0.07  0.05  0.01  0.02  0.21  0.06  0.09  0.08  1.    0.05  0.05\n",
      "  0.07  0.16  0.29  0.53  0.09  0.02  0.15  0.19  0.01  0.23  0.17  0.\n",
      "  0.02  0.15  0.49  0.04  0.04  0.05  1.    0.98  0.08  0.09  0.03  0.07\n",
      "  0.03  0.14  0.06  0.01  0.17  0.05]\n",
      "\n",
      "____________________________feature number:122____________________________\n",
      "size of feature 122 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.01 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 122 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.01  0.07  0.36  0.03  0.01  0.02  0.12  0.02  0.02  0.    0.01  0.05\n",
      "  0.02  0.02  0.02  0.01  0.    0.02  0.01  0.04  0.01  0.03  0.01  0.01\n",
      "  0.19  0.01  0.01  0.02  0.03  0.02  0.01  0.03  0.04  0.07  0.05  0.34\n",
      "  0.15  0.05  0.02  0.03  0.03  0.01  0.02  0.03  0.03  0.18  0.03  0.18\n",
      "  0.01  0.04  0.14  0.62  0.03  0.01  0.01  0.03  0.01  0.04  0.13  0.02\n",
      "  0.07  0.05  0.03  0.07  0.01  0.01  0.03  0.02  0.01  0.03  0.06  0.02\n",
      "  0.08  0.02  0.01  0.03  0.03  0.1   0.04  0.02  0.08  0.05  0.03  0.04\n",
      "  0.02  0.02  0.01  0.02  0.02  0.03  0.01  0.05  0.04  0.04  0.01  0.\n",
      "  0.08  0.04  0.01  0.58  0.13  0.05  0.05  0.07  0.38  0.08  0.02  0.05\n",
      "  0.03  0.01  0.02  0.03  0.01  0.02  0.62  0.14  0.02  0.07  0.01  0.04\n",
      "  0.02  0.02  0.04  0.02  0.05  0.07  0.01  0.03  0.72  0.3   0.02  0.15\n",
      "  0.03  0.28  0.06  0.02  0.06  0.02  0.13  0.08  0.02  0.12  0.02  0.02\n",
      "  0.03  0.19  0.01  0.02  0.    0.02  0.02  0.01  0.03  0.06  0.02  0.02\n",
      "  0.06  0.03  0.1   0.02  0.04  0.01  0.02  0.05  0.01  0.04  0.02  0.04\n",
      "  0.03  0.03  0.01  0.02  0.06  0.03  0.04  0.02  0.01  0.02  0.09  0.2\n",
      "  0.15  1.    0.    0.    0.03  0.06  0.01  0.04  0.06  0.15  0.03  0.01\n",
      "  0.04  0.12  0.02  0.15  0.34  0.03  0.02  0.09  0.02  0.07  0.01  0.03\n",
      "  0.04  0.01  0.05  0.03  0.04  0.26  0.03  0.02  0.03  0.05  0.01  0.04\n",
      "  0.    0.24  0.52  0.09  0.09  0.01  0.01  0.19  0.03  0.28  0.02  0.04\n",
      "  0.06  0.02  0.01  0.28  0.04  0.02  0.04  0.08  0.35  0.03  0.02  0.02\n",
      "  0.01  0.17  0.1   0.47  0.05  0.27  0.06  0.02  0.03  0.28  0.02  0.03\n",
      "  0.01  0.02  0.03  0.    0.12  0.06  0.02  0.03  0.04  0.    0.06  0.03\n",
      "  0.02  0.16  0.02  0.02  0.15  0.03  0.01  0.03  0.04  0.32  0.01  0.1\n",
      "  0.41  1.    0.02  0.04  0.03  0.02  0.01  0.03  0.05  0.05  0.64  0.05\n",
      "  0.05  0.02  0.09  0.09  0.07  0.01  0.01  0.02  0.05  0.01  0.05  0.09\n",
      "  0.01  0.02  0.04  0.34  0.02  0.03  0.04  1.    0.17  0.04  0.07  0.02\n",
      "  0.02  0.04  0.08  0.02  0.04  0.03  0.01]\n",
      "\n",
      "____________________________feature number:123____________________________\n",
      "size of feature 123 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.63 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 123 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.63  0.92  0.28  0.59  0.71  0.89  0.73  0.68  0.91  0.44  0.42  0.88\n",
      "  0.39  0.79  0.86  0.8   0.8   0.79  0.77  0.22  0.91  0.89  0.35  0.\n",
      "  0.49  0.84  0.85  0.71  0.63  0.82  0.92  0.79  0.56  0.91  0.54  0.62\n",
      "  0.9   0.85  0.8   0.83  0.99  0.83  0.86  0.7   0.63  0.74  0.91  0.73\n",
      "  0.34  0.22  0.55  0.    0.55  0.84  0.98  0.63  0.84  0.8   0.82  0.86\n",
      "  0.71  0.63  0.74  0.78  0.91  0.57  0.28  0.7   0.62  0.51  0.73  0.87\n",
      "  0.79  0.67  0.76  0.75  0.    0.45  0.    0.69  0.13  0.64  0.67  0.9\n",
      "  0.98  0.83  0.96  0.83  0.88  0.77  0.77  0.78  0.74  0.76  0.78  0.77\n",
      "  0.49  0.72  0.84  0.74  0.79  0.81  0.84  0.57  0.57  0.46  0.62  0.54\n",
      "  0.94  0.84  0.75  0.64  0.81  0.64  0.64  0.69  0.89  0.42  0.24  0.6\n",
      "  0.92  0.97  0.74  0.57  0.76  0.7   0.41  0.75  0.77  0.65  0.68  0.09\n",
      "  0.75  0.84  0.85  0.79  0.83  0.    0.74  0.75  0.84  0.86  0.73  0.44\n",
      "  0.97  0.5   0.5   0.6   0.84  0.74  0.62  0.56  0.96  0.65  0.86  0.85\n",
      "  0.78  0.69  0.    0.36  0.75  0.    0.85  0.61  0.76  0.84  0.86  0.55\n",
      "  0.86  0.8   0.    0.81  0.76  0.73  0.76  0.93  0.56  0.67  0.92  0.74\n",
      "  0.9   0.83  0.84  0.6   0.71  0.9   0.86  0.48  0.91  0.81  0.7   0.74\n",
      "  0.75  0.8   0.8   0.73  0.1   0.89  0.85  0.84  0.54  0.8   0.46  0.8\n",
      "  0.93  0.69  0.79  0.7   0.94  0.57  0.61  0.83  0.07  0.81  0.72  0.82\n",
      "  0.54  0.01  0.51  0.88  0.92  0.93  0.96  0.83  0.67  0.66  0.79  0.91\n",
      "  0.83  0.62  0.67  0.19  0.93  0.66  0.87  0.74  0.73  0.49  0.81  0.67\n",
      "  0.87  0.38  0.49  0.87  0.25  0.62  0.78  0.81  0.79  0.98  0.78  0.7\n",
      "  0.93  0.58  0.76  0.8   0.69  0.    0.71  0.69  0.45  0.74  0.25  0.57\n",
      "  0.88  0.75  0.5   0.74  0.7   0.92  0.8   0.76  0.81  0.86  0.69  0.79\n",
      "  0.85  1.    0.66  0.66  0.94  0.79  0.75  0.86  0.88  0.84  0.15  0.68\n",
      "  0.91  0.62  0.87  0.81  0.78  0.58  0.7   0.81  0.79  0.83  0.54  0.77\n",
      "  0.88  0.57  0.54  0.79  0.78  0.85  0.78  0.71  0.73  0.73  0.72  0.7\n",
      "  0.78  0.72  0.92  0.74  0.9   0.92  0.75]\n",
      "\n",
      "____________________________feature number:124____________________________\n",
      "size of feature 124 before delete NaNs:1994\n",
      "Before Delete NaNs:[ nan  nan   0. ...,  nan  nan  nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 124 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.   1.   0.5  1.   0.   0.   0.5  0.   1.   0.5  0.   1.   0.   1.   1.\n",
      "  0.   0.   0.   0.   0.5  0.5  1.   1.   1.   0.   0.   0.   1.   1.   0.\n",
      "  1.   0.   0.5  0.5  0.5  0.5  0.5  0.5  1.   1.   0.   0.   0.   0.5  0.\n",
      "  0.5  0.5  1.   0.   1.   0.5  0.5  0.   0.5  0.   0.   0.   0.5  0.5  0.\n",
      "  0.   1.   0.   1.   0.   0.   0.   0.   0.   1.   0.5  1.   1.   1.   0.\n",
      "  0.   0.   0.   1.   1.   1.   1.   0.   0.5  1.   0.   1.   0.   0.5  1.\n",
      "  0.   0.5  0.5  0.5  0.   0.   0.   0.   0.   0.   0.5  0.5  0.   0.5  0.5\n",
      "  1.   0.   0.5  0.5  0.5  0.   1.   1.   0.   0.5  0.5  0.   0.5  0.5  1.\n",
      "  0.   0.5  0.5  0.5  0.5  0.   0.5  0.   0.   0.   0.   0.5  1.   0.5  0.5\n",
      "  0.   0.5  0.   0.5  0.5  0.   0.5  1.   0.   0.   0.5  0.   0.   0.   0.\n",
      "  0.   0.   0.   0.5  0.   1.   0.   1.   1.   1.   0.   0.   0.5  0.5  0.\n",
      "  0.5  0.   0.   1.   0.5  0.   0.   0.   0.   1.   0.   0.   0.   0.5  0.5\n",
      "  0.   0.5  0.   1.   1.   0.5  0.   0.   0.5  0.5  0.   1.   1.   1.   0.\n",
      "  1.   0.5  0.5  0.   1.   0.   0.5  0.5  0.   1.   0.   1.   0.5  0.5  0.5\n",
      "  0.5  1.   1.   1.   0.   1.   1.   0.5  0.5  1.   0.5  0.   0.5  0.5  1.\n",
      "  1.   0.   1.   1.   0.   0.   0.5  1.   0.   1.   0.5  0.   0.   0.   1.\n",
      "  0.   0.5  0.   0.5  0.5  0.5  0.5  0.5  1.   0.5  1.   1.   0.   1.   0.5\n",
      "  0.   0.5  0.   1.   0.5  0.5  0.   0.   1.   0.5  1.   0.5  0.   0.5  1.\n",
      "  1.   1.   1.   0.5  1.   0.5  0.5  0.   0.   1.   1.   0.   0.5  0.5  0.5\n",
      "  1.   0.5  0.5  0.   1.   0.5  0.5  1.   0.   1.   0.   0.   0.   1.   0.5\n",
      "  0.   0.   0.   1.   1.   0.   1.   0.5  0.5  0.5  0.5  0.   1.   0.   0.5\n",
      "  1.   1.   0.   0. ]\n",
      "\n",
      "____________________________feature number:125____________________________\n",
      "size of feature 125 before delete NaNs:1994\n",
      "Before Delete NaNs:[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "number of nan for this feature:0\n",
      "Size of feature 125 column after delete NaNs:1994\n",
      "After Delete NaNs:[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "\n",
      "____________________________feature number:126____________________________\n",
      "size of feature 126 before delete NaNs:1994\n",
      "Before Delete NaNs:[  nan   nan  0.12 ...,   nan   nan   nan]\n",
      "number of nan for this feature:1675\n",
      "Size of feature 126 column after delete NaNs:319\n",
      "After Delete NaNs:[ 0.12  0.24  0.2   0.13  0.08  0.07  0.07  0.12  0.19  0.07  0.06  0.1\n",
      "  0.05  0.07  0.15  0.22  0.21  0.1   0.18  0.48  0.12  0.02  0.11  0.13\n",
      "  0.36  0.11  0.19  0.18  0.18  0.12  0.1   0.23  0.19  0.12  0.27  0.29\n",
      "  0.16  0.19  0.09  0.21  0.08  0.05  0.13  0.14  0.21  0.2   0.11  0.34\n",
      "  0.15  1.    0.17  0.22  0.13  0.1   0.07  0.19  0.1   0.25  0.19  0.18\n",
      "  0.27  0.15  0.21  0.09  0.15  0.12  1.    0.32  0.1   0.15  0.17  0.11\n",
      "  0.29  0.14  0.06  0.31  0.14  0.27  0.09  0.11  1.    0.19  0.15  0.14\n",
      "  0.09  0.13  0.12  0.18  0.14  0.19  0.14  0.15  0.14  0.2   0.1   0.15\n",
      "  1.    0.36  0.16  0.34  0.09  0.16  0.12  0.23  0.25  0.18  0.18  0.24\n",
      "  0.06  0.11  0.12  0.06  0.09  0.23  0.35  0.34  0.18  0.13  0.09  0.16\n",
      "  0.17  0.16  0.09  0.08  0.21  0.2   0.11  0.14  0.56  0.11  0.09  0.17\n",
      "  0.15  0.26  0.29  0.18  0.16  0.27  0.2   0.14  0.1   1.    0.15  0.99\n",
      "  0.18  0.21  0.12  0.12  0.04  0.07  0.16  0.12  0.02  0.28  0.19  0.25\n",
      "  0.28  0.16  0.03  0.1   0.13  0.01  0.19  0.22  0.16  0.09  0.08  0.19\n",
      "  0.16  0.14  0.07  0.15  0.12  0.23  0.1   0.18  0.12  0.15  0.15  0.17\n",
      "  1.    0.19  0.05  0.21  0.12  0.18  0.13  0.3   0.26  0.14  0.14  0.1\n",
      "  0.09  0.1   0.25  0.17  0.82  0.18  0.08  0.21  0.14  0.27  0.08  0.05\n",
      "  0.14  0.04  0.11  0.12  0.12  0.22  0.23  0.1   0.76  0.14  0.15  0.2\n",
      "  0.08  0.15  0.2   0.16  0.11  0.13  0.19  0.21  0.13  0.31  0.16  0.12\n",
      "  0.08  0.11  0.14  0.51  0.26  0.1   0.13  0.27  0.21  0.11  0.23  0.1\n",
      "  0.14  0.13  0.25  0.17  0.1   0.22  0.27  0.13  0.13  0.18  0.23  0.35\n",
      "  0.15  0.12  0.15  0.02  0.32  0.21  0.05  0.19  0.09  0.04  0.54  0.16\n",
      "  0.16  0.24  0.08  0.25  0.12  0.23  0.1   0.08  0.22  0.44  0.1   0.22\n",
      "  0.31  0.27  0.12  0.22  0.24  0.2   0.    0.14  0.13  0.22  0.73  0.28\n",
      "  0.69  0.11  0.16  0.22  0.11  0.09  0.11  0.    0.52  0.09  0.71  0.11\n",
      "  0.09  0.29  0.23  0.32  0.1   0.29  0.17  0.32  0.4   0.13  0.16  0.16\n",
      "  0.17  0.33  0.22  0.12  0.19  0.19  0.1 ]\n",
      "\n",
      "The number of all no-missing features: 101\n",
      "\n",
      "__________________Data after replacing all nan elements:__________________\n",
      "\n",
      "[[  4.10000000e+01   5.88268293e+01   4.61883366e+04 ...,   4.40438871e-01\n",
      "    0.00000000e+00   1.95078370e-01]\n",
      " [  2.80000000e+01   5.88268293e+01   4.61883366e+04 ...,   4.40438871e-01\n",
      "    0.00000000e+00   1.95078370e-01]\n",
      " [  4.70000000e+01   5.88268293e+01   4.61883366e+04 ...,   0.00000000e+00\n",
      "    0.00000000e+00   1.20000000e-01]\n",
      " ..., \n",
      " [  5.40000000e+01   5.88268293e+01   4.61883366e+04 ...,   4.40438871e-01\n",
      "    0.00000000e+00   1.95078370e-01]\n",
      " [  5.30000000e+01   5.88268293e+01   4.61883366e+04 ...,   4.40438871e-01\n",
      "    0.00000000e+00   1.95078370e-01]\n",
      " [  3.40000000e+01   3.00000000e+00   4.27500000e+04 ...,   4.40438871e-01\n",
      "    0.00000000e+00   1.95078370e-01]]\n",
      "\n",
      "Data shape:(1994, 127)\n",
      "\n",
      "To check if there is any other Nan(exisiting Nan indices):[]\n",
      "\n",
      "^^^^^^^^^^^^^Congratulation!^^^^^^^^^^^^\n",
      "\n",
      "-------There is no other NaN element in the dataset!--------\n",
      "\n",
      "\n",
      "*******************PART 2 - Split Data and Save each split as a dataset for train and validation*****************\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________DONE! Data is splitted_____________________________________\n",
      "\n",
      "_________________________________FOLD 0______________________________________\n",
      "_________________________________FOLD 1______________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________FOLD 2______________________________________\n",
      "_________________________________FOLD 3______________________________________\n",
      "_________________________________FOLD 4______________________________________\n",
      "\n",
      "_________________________Model Training for run 0_____________________\n",
      "\n",
      "\n",
      "Learned Parameters(W):\n",
      "[  2.59732623e-01   1.08188666e-05  -5.52567142e-02  -4.48610385e-03\n",
      "   7.97505996e-03   2.04620308e-02  -1.48142531e-02  -1.33152555e-03\n",
      "   8.35788956e-04  -7.39256535e-02   4.13051399e-02  -1.43733474e-04\n",
      "   6.68045735e-02  -3.18884238e-03  -8.82422332e-02   8.78651153e-03\n",
      "   1.89991685e-03   3.79845121e-03  -4.00651667e-02   1.10968011e-02\n",
      "   9.24783711e-04   6.19639338e-02   2.10208863e-03   9.03627738e-03\n",
      "   2.73033239e-04   1.23699437e-03   1.78056308e-03   6.50180757e-05\n",
      "  -1.31985904e-04  -6.43255480e-02  -2.32888044e-02  -9.26040899e-03\n",
      "   5.77550743e-03  -2.52515364e-02  -1.63077471e-02  -1.88835292e-02\n",
      "   7.21657346e-03   9.58284495e-03  -1.55741744e-03   1.06218077e-02\n",
      "  -5.42670389e-02   4.12927862e-05  -1.25810586e-02   4.31940237e-02\n",
      "   3.86427821e-02  -2.80618358e-02   6.57334115e-03   1.82666233e-02\n",
      "   1.64159296e-03   8.37605574e-03  -8.19355174e-03   1.48704341e-01\n",
      "   3.23089489e-04  -9.10201115e-02  -1.03397666e-03   2.34890276e-03\n",
      "  -5.75799083e-03   1.59287699e-04  -4.31106106e-02   1.76154547e-02\n",
      "   4.39563499e-02   4.90314608e-04  -6.98308160e-03  -3.41432635e-02\n",
      "  -1.18875710e-02   5.45467519e-03  -4.69461982e-02   1.77976944e-02\n",
      "  -5.33100548e-03  -7.81973061e-02   3.54594338e-02  -9.72210456e-03\n",
      "  -7.27044978e-03   1.55198922e-03   4.16455001e-03   7.39039847e-02\n",
      "   6.27643854e-03   9.96607717e-05  -1.31911379e-03   1.48835589e-02\n",
      "  -7.87781258e-03   5.21372419e-02  -6.31543945e-02   1.89436279e-02\n",
      "   1.64473388e-02   8.36894681e-03  -1.38033895e-02  -1.39432986e-02\n",
      "   6.10428510e-04   1.04215686e-02   4.49849604e-03  -3.84492115e-02\n",
      "   9.70615389e-03   1.78300982e-02  -2.05269453e-03  -9.86899580e-04\n",
      "  -1.36026962e-03  -2.41442666e-04  -1.14611170e+00   1.48352998e+00\n",
      "  -2.65651930e-02   1.89281140e-01  -2.87887504e-01   1.51861730e-01\n",
      "   1.19131940e-02  -8.44358314e-01   3.37697996e-02  -2.28709343e-01\n",
      "  -1.90595455e-02   5.52346587e-02   7.14356819e-02  -2.50068828e-01\n",
      "  -2.08863912e-01  -2.07470438e-03   2.20110960e-02   1.56062073e-02\n",
      "   1.04767852e-04  -3.97098465e-03   4.88064692e-02   1.52024217e+00\n",
      "  -3.53212161e-02   9.96142925e-03]\n",
      "\n",
      "Train MSE:0.000419789189954, Valid MSE:0.0010956602283\n",
      "\n",
      "\n",
      "______________________________________________________________________________________\n",
      "\n",
      "_________________________Model Training for run 1_____________________\n",
      "\n",
      "\n",
      "Learned Parameters(W):\n",
      "[  6.23557586e-01  -9.30812379e-05   2.70583421e-02   7.85974969e-03\n",
      "   6.19132186e-03   1.51803364e-02  -1.41102980e-02  -7.39805839e-03\n",
      "  -1.06533843e-03  -4.19784085e-02   2.17380785e-02  -1.08739512e-02\n",
      "   2.38027406e-02  -3.05141299e-03  -5.84604826e-02  -2.11975167e-04\n",
      "   1.20916708e-03  -1.96691558e-03   3.87730376e-03   2.42097437e-03\n",
      "  -2.97961562e-03   7.79649447e-02   1.74314466e-02  -1.17015022e-02\n",
      "   1.04072106e-03   2.72183836e-03  -1.61461776e-03  -4.89384421e-04\n",
      "  -2.74072613e-03  -8.62702972e-02   1.24510482e-02  -5.11677704e-03\n",
      "  -1.17899533e-02  -2.84290427e-02  -4.65690105e-03  -4.43834153e-03\n",
      "   3.75683782e-03   3.27627205e-04   1.25606038e-02   1.52206585e-02\n",
      "  -5.42717876e-02   7.73081030e-04  -3.11586389e-02   8.70936385e-02\n",
      "   1.19818852e-02  -2.53395613e-02   3.24944776e-02   1.31535089e-02\n",
      "   4.07415179e-03   4.16799582e-03  -5.18898398e-03   1.86033503e-01\n",
      "   1.45440279e-03  -7.72695539e-02  -7.99871916e-04   1.36164923e-03\n",
      "  -1.01932445e-03  -4.99745866e-03  -1.76061654e-02   1.48979036e-02\n",
      "  -2.21253025e-02   7.24432917e-02  -1.18255893e-02  -3.40829980e-02\n",
      "  -9.38361197e-03   1.39583248e-02  -5.14299778e-02   1.45380921e-02\n",
      "  -5.74319452e-03  -4.00400859e-02   3.61339987e-02  -1.20525558e-02\n",
      "  -4.19535951e-03  -2.22885854e-02   1.78536794e-03   2.94561857e-02\n",
      "   3.58753982e-03   3.90257935e-03   8.74906206e-03  -1.24515016e-03\n",
      "   2.50213592e-03   6.35016140e-02  -6.68905421e-02   3.65976378e-03\n",
      "   1.72501374e-02  -1.41344819e-02  -1.55800122e-02   1.27534865e-02\n",
      "   9.01003911e-04   3.69764529e-03  -1.05208684e-03  -5.11700221e-02\n",
      "  -2.06923562e-03  -1.84432942e-03  -6.37718849e-03   1.30454697e-02\n",
      "  -7.60816339e-03   3.83939389e-03  -1.60246306e+00   2.22356109e+00\n",
      "  -6.34089820e-01  -3.31610930e-01  -1.47632240e-01   1.09163135e-01\n",
      "   4.30924776e-03  -1.01022644e+00   3.64359688e-02  -8.82967370e-02\n",
      "   7.16747347e-02   1.68094314e-01   9.29085871e-02  -2.35262568e-01\n",
      "  -1.49483859e-01   1.43965906e-03   1.57520165e-02  -4.41447095e-03\n",
      "  -8.60115743e-03  -2.41390389e-03   4.86511832e-02   1.18029326e+00\n",
      "   5.27894813e-02   3.93818543e-03]\n",
      "\n",
      "Train MSE:0.000335587567593, Valid MSE:0.0013322577744\n",
      "\n",
      "\n",
      "______________________________________________________________________________________\n",
      "\n",
      "_________________________Model Training for run 2_____________________\n",
      "\n",
      "\n",
      "Learned Parameters(W):\n",
      "[  9.03064839e-01  -3.85465766e-05   7.44650879e-03  -4.29465662e-03\n",
      "   1.21436429e-02   2.62930116e-02  -1.77357333e-02  -1.23906267e-02\n",
      "   1.43193619e-03  -1.00563215e-01   4.93465075e-02   5.78643527e-04\n",
      "   4.24291703e-02  -3.08523486e-03  -8.90989393e-02   1.26766397e-02\n",
      "  -5.47656614e-04   3.90632735e-03  -2.59812737e-02   1.47260082e-02\n",
      "  -1.46893362e-04   7.81483715e-02  -1.72598823e-02   1.82104650e-02\n",
      "   1.74936107e-03   1.39809310e-03   1.95156321e-03  -2.58967459e-04\n",
      "  -1.88988305e-03  -8.25347826e-02  -1.31167302e-02  -1.71064758e-03\n",
      "  -6.10715453e-05  -2.30567398e-02  -7.98996367e-03  -8.12821241e-03\n",
      "   7.93203054e-03   7.31540335e-03  -3.41217865e-03   3.50288841e-03\n",
      "  -2.99758002e-02   9.07668181e-03   8.48490874e-03   1.53735765e-02\n",
      "  -7.09521322e-04  -5.92323520e-02   5.86691087e-02   2.69148250e-02\n",
      "   8.15187104e-03   7.53605393e-03  -4.05221764e-03   2.09178901e-01\n",
      "   4.88528253e-03  -7.47276866e-02  -1.74223187e-03   2.47841976e-03\n",
      "  -7.92292534e-03   6.65559794e-03  -1.67589965e-02  -2.57349094e-02\n",
      "   7.95637374e-02  -8.71499637e-03  -4.33308995e-03  -2.83648523e-02\n",
      "  -1.77943380e-02   2.82121208e-02  -2.45639099e-02   2.08127833e-02\n",
      "  -1.09826388e-02  -8.49619458e-02   4.14722478e-02  -1.40951527e-02\n",
      "  -6.17212063e-03  -2.79558959e-02  -2.62068953e-03   7.54368499e-02\n",
      "   6.93684461e-03   3.48790583e-03   3.53133832e-03   1.71971563e-02\n",
      "  -5.32966098e-03   3.87078848e-02  -3.17655550e-02   2.19782999e-03\n",
      "   2.17140318e-02   8.39136907e-03  -1.50718986e-02  -9.91503170e-03\n",
      "   3.15880888e-03  -3.49210556e-03   5.07546144e-03  -5.34879067e-02\n",
      "  -3.47167207e-03   8.73482571e-03  -1.01667229e-02  -8.93483469e-03\n",
      "  -3.10105975e-03   1.36334071e-02  -1.92508856e+00   2.70816547e-01\n",
      "  -7.60543966e-01   3.87233838e-01  -7.66256187e-02  -7.57565784e-03\n",
      "   3.73179082e-02   3.01014548e-01   4.85113985e-02  -1.77978467e-01\n",
      "  -1.05739785e-02   9.17347371e-02   8.64922848e-02  -2.43763061e-01\n",
      "  -4.08151471e-02  -1.08718292e-02   2.30062672e-02  -4.18428919e-03\n",
      "  -1.80501431e-03  -3.74507854e-03   6.13637328e-02   1.20326165e+00\n",
      "  -7.05240964e-02   1.17448543e-02]\n",
      "\n",
      "Train MSE:0.00052609856696, Valid MSE:0.000435530602273\n",
      "\n",
      "\n",
      "______________________________________________________________________________________\n",
      "\n",
      "_________________________Model Training for run 3_____________________\n",
      "\n",
      "\n",
      "Learned Parameters(W):\n",
      "[  1.82511132e+00  -5.77332169e-05   8.09258775e-02  -1.10708543e-03\n",
      "   1.94150390e-02   3.63049768e-02  -1.08405785e-02  -4.94907527e-03\n",
      "  -1.00177804e-03  -7.35005343e-02   3.53601087e-02  -3.39780568e-03\n",
      "  -3.84683634e-02  -1.01772398e-03  -8.50625858e-02   8.13717583e-03\n",
      "   6.74931109e-04  -9.02861183e-04  -3.36269471e-02   1.40846394e-02\n",
      "   7.43181617e-03   7.32760688e-02  -3.83572435e-02   4.13619780e-02\n",
      "   9.51322297e-04   4.56122029e-03   1.94266325e-03  -3.31083873e-04\n",
      "  -4.30365763e-03  -2.24613033e-02  -1.82880200e-02  -2.81460032e-03\n",
      "  -1.23876897e-03  -3.72926477e-02  -1.03278394e-02  -6.55277684e-03\n",
      "   9.93296387e-03   5.63527630e-03  -2.55226107e-04   1.60867420e-02\n",
      "  -7.45901613e-02   1.18129928e-04  -3.82691667e-02   8.20272309e-02\n",
      "   1.09827830e-02  -6.22663564e-02   5.02887523e-02   2.08827511e-02\n",
      "   9.54747659e-03   6.04328707e-03  -2.02265375e-03   1.51499693e-01\n",
      "   4.88519242e-03  -1.18439377e-01  -2.83743620e-03   2.93836108e-03\n",
      "  -1.42144712e-02   1.00963846e-02  -3.96630178e-02  -2.90170492e-03\n",
      "   1.18473879e-01  -3.62241167e-02  -8.86521377e-03  -3.81621141e-02\n",
      "  -6.18620888e-02   6.42042089e-02  -7.47066935e-02   5.26676121e-02\n",
      "  -2.04718120e-02  -1.52827719e-01   5.68527989e-02  -7.05828592e-03\n",
      "  -4.68580403e-03  -5.79220409e-02  -9.39512213e-03   1.53342796e-01\n",
      "   1.04423127e-02   1.20450470e-03   1.36882953e-03   1.32581101e-02\n",
      "  -6.02484827e-03   4.07879233e-02  -5.96746541e-02   2.73195945e-02\n",
      "   2.03053913e-02   2.91419583e-03  -1.45709239e-02  -3.41254865e-03\n",
      "   6.97177239e-04   3.48521486e-03   2.43962227e-03  -2.45759394e-02\n",
      "  -3.17182579e-03  -1.86270843e-03  -9.09781776e-03  -1.82046091e-03\n",
      "  -8.36245172e-03   1.25692616e-02  -2.92479851e+00   9.82573938e-01\n",
      "  -1.67954025e+00   1.27486434e-01  -9.53503861e-02   9.98370937e-02\n",
      "   5.90395517e-03  -1.92583535e-01   4.79157590e-02  -1.86129965e-01\n",
      "  -7.27800318e-02   2.77654899e-02   6.08032586e-02  -1.38059583e-01\n",
      "  -2.99818966e-01   1.72680518e-02   2.04115706e-02  -2.93909550e-03\n",
      "   2.23146549e-03  -3.43811608e-03   6.04602456e-02   1.56034645e+00\n",
      "  -5.96174416e-02   6.21450522e-03]\n",
      "\n",
      "Train MSE:0.000505915462904, Valid MSE:0.000625730531184\n",
      "\n",
      "\n",
      "______________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 4_____________________\n",
      "\n",
      "\n",
      "Learned Parameters(W):\n",
      "[  1.38610687e+00  -1.13809838e-04   2.65956886e-03  -1.92968011e-02\n",
      "   1.06873673e-02   1.31832561e-02  -8.54446748e-03  -6.07437748e-03\n",
      "  -2.85189896e-02  -4.73163055e-02   4.18037160e-02  -1.34609013e-02\n",
      "   8.26640262e-02  -6.47344784e-03  -3.66573979e-02   1.02402586e-03\n",
      "   1.68065733e-03   1.18957084e-02  -2.54193195e-02   4.20557324e-04\n",
      "   2.29974828e-03   2.43051527e-02   1.94546042e-02  -2.07562907e-02\n",
      "  -1.61935697e-03   3.26329127e-03  -3.86035943e-04  -8.85476395e-05\n",
      "  -6.21703991e-03  -1.05268204e-01  -1.39146263e-02   2.36536919e-04\n",
      "  -1.06934320e-02  -2.75316654e-02  -6.38926359e-03  -2.20619768e-02\n",
      "   6.85579242e-03   7.34439036e-03  -2.90349225e-03   7.17104987e-03\n",
      "  -6.48425912e-02   7.76136562e-03  -2.89218714e-02   6.83663031e-02\n",
      "   9.08855458e-03  -2.86838776e-02   1.14148114e-02   1.48162055e-02\n",
      "   2.76081819e-03   5.02933631e-03  -3.05901186e-04   1.92539879e-01\n",
      "  -6.57197714e-03  -1.12577134e-01  -6.27814741e-03   3.10964329e-03\n",
      "  -7.68748083e-03   1.13388987e-02  -1.19074033e-02  -5.77963592e-03\n",
      "   6.05907491e-02  -3.20927008e-02   3.30820317e-03  -9.47199252e-03\n",
      "  -2.47119878e-02   2.49272890e-02  -3.96135035e-02   5.55383151e-02\n",
      "  -1.84340631e-02  -1.54159199e-01   3.78061424e-02  -2.58393244e-03\n",
      "  -4.31465510e-03  -1.70549430e-02  -1.07548823e-03   1.64523719e-01\n",
      "   9.16979301e-03  -1.32734783e-03  -8.26844277e-05   8.77263649e-03\n",
      "  -5.01082457e-03   1.10153491e-02  -5.61016743e-02   5.51097076e-02\n",
      "   1.58038977e-02  -3.51047509e-03  -7.61858911e-03  -5.66852344e-03\n",
      "   1.56567863e-03  -6.02075245e-04   6.05848935e-03  -6.30716786e-02\n",
      "   1.49184356e-03   7.20416454e-03  -4.77681167e-03  -1.51007922e-02\n",
      "   4.30395878e-03   6.84348981e-03  -2.37591703e+00  -8.81037745e+01\n",
      "  -1.22457560e+00   2.33418047e-01  -2.25709412e-01   4.96199245e-03\n",
      "   6.59618088e-02   8.88341485e+01   2.53147215e-02  -1.61575434e-01\n",
      "  -3.28276042e-02   6.69169427e-02   4.95914759e-02  -1.83791083e-01\n",
      "  -1.72384095e-01  -2.09397557e-03   1.64552717e-02  -5.54904066e-03\n",
      "   2.86539734e-03  -5.35877006e-04   7.59785879e-02   1.46800978e+00\n",
      "  -6.17981423e-02   6.64003689e-03]\n",
      "\n",
      "Train MSE:0.000487209246564, Valid MSE:0.00262844616924\n",
      "\n",
      "\n",
      "______________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAngAAAHwCAYAAAA1nBISAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xl4VdXd9vHvL2GSSWZFqoKIFRmF\nIFBFQRBwACSgzIq22mqttXbQ2tc6Pw6PWts+DlVbywnIIARFZVARxFmDIkXAAgoyySCzIJBkvX+s\nA0YMyQFyss5wf66Li2SffXbuUAt39tprLXPOISIiIiKpIyN0ABEREREpWyp4IiIiIilGBU9EREQk\nxajgiYiIiKQYFTwRERGRFKOCJyIiIpJiVPBERGJgZtPM7PLQOUREYqGCJyIJzcyWm1mP0Dmcc+c7\n50bF49pmVtPMHjGzL81sh5ktjX5eLx5fT0RSnwqeiKQ9M6sQ8GtXAmYCLYDeQE3gJ8DXwBmHcb1g\n34uIJA4VPBFJWmZ2kZnNM7MtZvaOmbUu8trNZrbMzLab2UIz61/ktZFm9raZ/cXMNgG3R4+9ZWYP\nmtlmM/vCzM4v8p7ZZvazIu8v6dwmZjYn+rVfM7NHzWz0Qb6Ny4ATgP7OuYXOuULn3Hrn3F3OuanR\n6zkzO7nI9f9tZndHP+5qZqvM7CYz+wp4xswWmdlFRc6vYGYbzaxd9PNO0T+vLWb2iZl1PZL/HUQk\n8ajgiUhSipaVfwE/B+oC/wCmmFnl6CnLgC7A0cAdwGgza1jkEh2Bz4EGwD1Fjn0G1AMeAP5pZnaQ\nCCWd+yzwQTTX7cCIEr6VHsB059yO0r/rgzoWqAOcCFwNjAWGFHm9F7DROfeRmTUCXgbujr7nd8Ak\nM6t/BF9fRBKMCp6IJKurgH845953zhVEn4/bDXQCcM4955xbE70jNh5YwveHPNc45/7unMt3zu2K\nHlvhnHvKOVcAjAIaAscc5OsXe66ZnQB0AP7snNvjnHsLmFLC91EXWHtYfwLfKQRuc87tjn4vzwJ9\nzaxq9PWh0WMAw4Gpzrmp0T+bV4E84IIjzCAiCUQFT0SS1YnAb6PDjFvMbAtwPHAcgJldVmT4dgvQ\nEn+3bZ+VxVzzq30fOOd2Rj+sfpCvf7BzjwM2FTl2sK+1z9f4cngkNjjnvi2SZymwCOgTLXl9+a7g\nnQhccsCf21llkEFEEogexhWRZLUSuMc5d8+BL5jZicBTQHfgXedcgZnNA4oOt7o45VoL1DGzqkVK\n3vElnP8acLeZVXPOfXOQc3YCVYt8fiywqsjnxX0v+4ZpM4CF0dIH/s8txzl3VSnfh4gkMd3BE5Fk\nUNHMqhT5VQFf4H5hZh3Nq2ZmF5pZDaAavvRsADCzK/B38OLOObcCP+R5u5lVMrPOQJ8S3pKDL12T\nzOxUM8sws7pmdouZ7Rs2nQcMNbNMM+sNnBNDlHFAT+Aavrt7BzAaf2evV/R6VaITNX50iN+qiCQw\nFTwRSQZTgV1Fft3unMvDP4f3f8BmYCkwEsA5txB4CHgXWAe0At4ux7zDgM744de7gfH45wN/wDm3\nGz/RYjHwKrANP0GjHvB+9LRf40vilui1ny8tgHNuLf77/0n06+87vhLoB9yCL8Argd+jfw9EUoo5\nF69RChERATCz8cBi59xtobOISHrQT2wiImXMzDqYWdPocGtv/B2zUu+6iYiUFU2yEBEpe8cCufgl\nUFYB1zjnPg4bSUTSiYZoRURERFKMhmhFREREUowKnoiIiEiKSetn8OrVq+caN24cOoaIiIhIqebO\nnbvRORfTvtFpXfAaN25MXl5e6BgiIiIipTKzFbGeqyFaERERkRSjgiciIiKSYlTwRERERFJMWj+D\nV5w9e/awbNkydu7cGTqKxKhq1ao0bdqUSpUqhY4iIiKSEFTwDrBs2TJq1arFj3/8YzIydIMz0RUW\nFvLVV1/x6aefcsopp1CtWrXQkURERIJTgznAzp07OeaYY1TukkRGRgbHHnssBQUFTJo0SXdeRURE\nUMErlspdcsnIyMDM2LRpE2vWrAkdR0REJDg1mRRQvXp1ANasWcPAgQOLPadr166lrvn3yCOPfO8O\n2AUXXMCWLVvKLmg52Lt3b+gIIiIiwangpZDjjjuOiRMnHvb7Dyx4U6dOpVatWmURTURERMqRCl6C\nuemmm3jsscf2f3777bfz0EMPsWPHDrp37067du1o1aoVL7zwwg/eu3z5clq2bAnArl27GDx4MK1b\nt2bQoEHs2rVr/3nXXHMNWVlZtGjRgttuuw2Av/3tb6xZs4Zu3brRrVs3wO/0sXHjRgAefvhhWrZs\nScuWLXnkkUf2f73mzZtz1VVX0aJFC3r27Pm9r7PPyJEjueaaa+jWrRsnnXQSb7zxBldeeSXNmzdn\n5MiRABQUFDBy5EhatmxJq1at+Mtf/gL4SS+9e/emffv2dOnShcWLFx/pH7GIiEjK0yzaEtxwA8yb\nV7bXbNsWov2oWIMHD+aGG27g2muvBWDChAlMnz6dKlWqMHnyZGrWrMnGjRvp1KkTffv2xcyKvc7j\njz9O1apVmT9/PvPnz6ddu3b7X7vnnnuoU6cOBQUFdO/enfnz53P99dfz8MMPM2vWLOrVq/e9a82d\nO5dnnnmG999/H+ccHTt25JxzzqF27dosWbKEsWPH8tRTT3HppZcyadIkhg8f/oM8mzdv5vXXX2fK\nlCn06dOHt99+m6effpoOHTowb948CgoKWL16NQsWLADYPzR89dVX88QTT9CsWTPef/99rr32Wl5/\n/fVD+jMXERFJNyp4Ceb0009n/fr1rFmzhg0bNlC7dm1OOOEE9u7dyy233MKcOXPIyMhg9erVrFu3\njmOPPbbY68yZM4frr78egNatW9O6dev9r02YMIEnn3yS/Px81q5dy8KFC7/3+oHeeust+vfvv38J\nkuzsbN5880369u1LkyZNaNu2LQDt27dn+fLlxV6jT58+mBmtWrXimGOOoVWrVgC0aNGC5cuXc845\n5/D555/zq1/9igsvvJCePXuyY8cO3nnnHS655JL919m9e3fsf5giIiJpSgWvBCXdaYungQMHMnHi\nRL766isGDx4MwJgxY9iwYQNz586lYsWKNG7cmG+//bbE6xR3d++LL77gwQcf5MMPP6R27dqMHDmy\n1Os45w76WuXKlfd/nJmZWewQbdHzMjIyvveejIwM8vPzqV27Np988gkzZszg0UcfZcKECTzyyCPU\nqlWLeWV9G1VERCTF6Rm8BDR48GDGjRvHxIkT98+K3bp1Kw0aNKBixYrMmjWLFStWlHiNs88+mzFj\nxgCwYMEC5s+fD8C2bduoVq0aRx99NOvWrWPatGn731OjRg22b99e7LWef/55du7cyTfffMPkyZPp\n0qVLWX27AGzcuJHCwkIGDBjAXXfdxUcffUTNmjVp0qQJzz33HOCL5ieffFKmX1dERCQV6Q5eAmrR\nogXbt2+nUaNGNGzYEIBhw4bRp08fsrKyaNu2LaeeemqJ17jmmmu44ooraN26NW3btuWMM84AoE2b\nNpx++um0aNGCk046iTPPPHP/e66++mrOP/98GjZsyKxZs/Yfb9euHSNHjtx/jZ/97GecfvrpBx2O\nPRyrV6/miiuuoLCwEIB7770X8Hcur7nmGu6++2727t3L4MGDadOmTZl9XRERkVRkJQ2/pbqsrCx3\n4Npwc+fOpX379oESyeGaO3cub775Jr169aJ58+ah44iIiJQ5M5vrnMuK5VwN0YqIiIgcgXXroJTH\n2cudCp6IiIjIEbjxRmjRAqJPGSUEFTwRERGRw7RtG0yeDL16QSJtZZ9AUURERESSy6RJsGsXXHZZ\n6CTfp4InIiIicpgiEWjWDDp2DJ3k+1TwRERERA7DihUwe7a/e3eQnUODUcFLMFu2bOGxxx47rPde\ncMEF+/dwPZg///nPvPbaa4d1fREREfnO6NH+92K2YA9OBS/BlFTwCgoKSnzv1KlTqVWrVonn3Hnn\nnfTo0eOw84mIiAg454dnzzkHGjcOneaHVPASzM0338yyZcto27Ytv//975k9ezbdunVj6NChtGrV\nCoCLL76Y9u3b06JFC5588sn9723cuDEbN25k+fLlNG/enKuuuooWLVrQs2fP/XvEjhw5kokTJ+4/\n/7bbbqNdu3a0atWKxYsXA7BhwwbOO+882rVrx89//nNOPPFENm7c+IOs1atX56abbqJ9+/b06NGD\nDz74gK5du3LSSScxZcoUAD799FPOOOMM2rZtS+vWrVmyZAkAo0eP3n/85z//eanlVUREJJF88AH8\n97+JN7liH21VVoIblixh3o4dZXrNttWr80izZgd9/b777mPBggXMmzcPgNmzZ/PBBx+wYMECmjRp\nAsC//vUv6tSpw65du+jQoQMDBgygbt2637vOkiVLGDt2LE899RSXXnopkyZNYngx95Dr1avHRx99\nxGOPPcaDDz7I008/zR133MG5557LH//4R6ZPn/69ElnUN998Q9euXbn//vvp378//+///T9effVV\nFi5cyOWXX07fvn154okn+PWvf82wYcPYs2cPBQUFLFq0iPHjx/P2229TsWJFrr32WsaMGcNlifr/\nEhERkQNEIlClCkS3jE84KnhJ4Iwzzthf7gD+9re/MXnyZABWrlzJkiVLflDwmjRpQtu2bQFo3779\nQfeNzc7O3n9Obm4uAG+99db+6/fu3ZvatWsX+95KlSrRu3dvAFq1akXlypWpWLEirVq12v/1Onfu\nzD333MOqVavIzs6mWbNmzJw5k7lz59KhQwcAdu3aRYMGDQ71j0VERCSI3bth3Djo3x9q1gydpngq\neCUo6U5beapWrdr+j2fPns1rr73Gu+++S9WqVenatSvfFrM/SuXKlfd/nJmZuX+I9mDnZWZmkp+f\nD0Cs+xNXrFgRi04bysjI2H+tjIyM/dcaOnQoHTt25OWXX6ZXr148/fTTOOe4/PLLuffee2P6OiIi\nIolk6lTYtClxh2dBz+AlnBo1arB9+/aDvr5161Zq165N1apVWbx4Me+9916ZZzjrrLOYMGECAK+8\n8gqbN28+7Gt9/vnnnHTSSVx//fX07duX+fPn0717dyZOnMj69esB2LRpEytWrCiT7CIiIvEWicCx\nx0Iiz1lUwUswdevW5cwzz6Rly5b8/ve//8HrvXv3Jj8/n9atW3PrrbfSqVOnMs9w22238corr9Cu\nXTumTZtGw4YNqVGjxmFda/z48bRs2ZK2bduyePFiLrvsMk477TTuvvtuevbsSevWrTnvvPNYu3Zt\nGX8XIiIiZW/jRnj5ZRg2DCok8DioxTocl4qysrJcXl7e947NnTuX9u3bB0qUGHbv3k1mZiYVKlTg\n3Xff5Zprrtk/6SNRzZ07lzfffJNevXrRvHnz0HFERCRFPfooXHcdfPIJtG5dvl/bzOY657JiOTeB\nu6eE8uWXX3LppZdSWFhIpUqVeOqpp0JHEhERSQiRCLRpU/7l7lCp4MkPNGvWjI8//jh0DBERkYSy\neLFf/+6hh0InKZ2ewRMRERGJQU4OZGTA0KGhk5ROBa8YhYWFoSPIIdD/XiIiEm+Fhb7g9erlZ9Am\nOhW8A1StWpV169apNCSJwsJCvvrqK/bu3bt/TT4REZGyNns2rFyZ2GvfFRXXZ/DMrDfwVyATeNo5\nd98Br1cGIkB74GtgkHNuefS1PwI/BQqA651zM8zs+Oj5xwKFwJPOub9Gz78duArYEL38Lc65qYea\nuWnTpixbtozVq1erMCSJvXv3smLFCgoLC7+3wLOIiEhZiUT8rhX9+oVOEpu4FTwzywQeBc4DVgEf\nmtkU59zCIqf9FNjsnDvZzAYD9wODzOw0YDDQAjgOeM3MTgHygd865z4ysxrAXDN7tcg1/+Kce/BI\ncleqVInmzZuzaNEiZsyYoZKXJAoLC2ncuDHHH3986CgiIpJivvkGJk6EwYPhqKNCp4lNPO/gnQEs\ndc59DmBm44B+QNGC1w+4PfrxROD/zDeqfsA459xu4AszWwqc4Zx7F1gL4JzbbmaLgEYHXLNMNG/e\nnHr16rFly5aYt+6ScCpXrsxxxx1HxYoVQ0cREZEUM3myL3nJMjwL8S14jYCVRT5fBXQ82DnOuXwz\n2wrUjR5/74D3Nir6RjNrDJwOvF/k8HVmdhmQh7/Td/h7bAH169enfv36R3IJERERSXKRCDRuDGed\nFTpJ7OI5yaK4sc0Db4Ud7JwS32tm1YFJwA3OuW3Rw48DTYG2+Lt8xa5SY2ZXm1memeVt2LChuFNE\nREREAFi9Gl57DUaM8EukJIt4Rl0FFH0g6kfAmoOdY2YVgKOBTSW918wq4svdGOdc7r4TnHPrnHMF\nzrlC4Cn8EPEPOOeedM5lOeeydHdORERESjJmDDjnC14yiWfB+xBoZmZNzKwSftLElAPOmQJcHv14\nIPC68w+8TQEGm1llM2sCNAM+iD6f909gkXPu4aIXMrOGRT7tDywo8+9IRERE0oZzMGoUdO4MzZqF\nTnNo4vYMXvSZuuuAGfhlUv7lnPvUzO4E8pxzU/BlLSc6iWITvgQSPW8CfvJEPvBL51yBmZ0FjAD+\nY2bzol9q33IoD5hZW/xQ7nLg5/H63kRERCT1ffwxLFwIjz8eOsmhs3SeIZqVleXy8vJCxxAREZEE\ndMMNvtytXQt16oROA2Y21zmXFcu5SfS4oIiIiEj52LsXnn0W+vRJjHJ3qFTwRERERA4wYwZs2JBc\na98VpYInIiIicoBIBOrVg969Qyc5PCp4IiIiIkVs3gxTpsCQIVCpUug0h0cFT0RERKSI556D3buT\nd3gWVPBEREREvicSgebNoX370EkOnwqeiIiISNSyZfD22/7unRW3cWqSUMETERERicrJ8cVu2LDQ\nSY6MCp6IiIgIfmuySATOPReOPz50miOjgiciIiKCH5r94ovknlyxjwqeiIiICP7uXdWqkJ0dOsmR\nU8ETERGRtLdrF0yYAAMGQPXqodMcORU8ERERSXsvvghbt6bG8Cyo4ImIiIgQiUCjRtCtW+gkZUMF\nT0RERNLaunUwfToMHw6ZmaHTlA0VPBEREUlrY8dCQUHqDM+CCp6IiIikuUgEsrLgtNNCJyk7Kngi\nIiKStv7zH/j449S6ewcqeCIiIpLGcnKgQgUYPDh0krKlgiciIiJpqaAARo+GCy6A+vVDpylbKngi\nIiKSlmbOhLVrU294FlTwREREJE1FIlCrFlx0UegkZU8FT0RERNLO9u2Qm+ufvatcOXSasqeCJyIi\nImln0iS//2wqDs+CCp6IiIikoUgETj4ZOnUKnSQ+VPBEREQkraxYAbNm+bt3ZqHTxIcKnoiIiKSV\nMWP878OHh80RTyp4IiIikjac88OzZ58NTZqEThM/KngiIiKSNj78ED77LHUnV+yjgiciIiJpIxKB\nKlVg4MDQSeJLBU9ERETSwp49MHYsXHwxHH106DTxpYInIiIiaWHqVNi0KfWHZ0EFT0RERNJEJALH\nHAPnnRc6Sfyp4ImIiEjK+/preOklGDYMKlQInSb+VPBEREQk5Y0fD3v3psfwLKjgiYiISBqIRKB1\na2jTJnSS8qGCJyIiIints8/g/ffT5+4dqOCJiIhIisvJgYwMGDo0dJLyo4InIiIiKauw0Be8nj2h\nYcPQacqPCp6IiIikrDlz4Msv02t4FlTwREREJIVFIlCjBvTrFzpJ+VLBExERkZS0cyc89xxccglU\nrRo6TflSwRMREZGU9PzzsGNH+g3PggqeiIiIpKhIBE48Ebp0CZ2k/KngiYiISMpZswZefRVGjPBL\npKSbNPyWRUREJNU9+6xfImXEiNBJwlDBExERkZTiHIwaBZ06wSmnhE4ThgqeiIiIpJRPPoEFC9Jz\ncsU+KngiIiKSUiIRqFgRBg0KnSQcFTwRERFJGfn5MGYM9OkDdeqEThOOCp6IiIikjBkzYP369B6e\nBRU8ERERSSGRCNStC+efHzpJWCp4IiIikhK2bIEXXoAhQ6BSpdBpwlLBExERkZTw3HOwe7eGZ0EF\nT0RERFJEJAKnngpZWaGThKeCJyIiIklv2TJ46y1/984sdJrwVPBEREQk6Y0e7YvdsGGhkyQGFTwR\nERFJas754dlu3eCEE0KnSQwqeCIiIpLU3nkHPv9ckyuKUsETERGRpBaJQNWqkJ0dOkniUMETERGR\npPXttzB+vC93NWqETpM4VPBEREQkab34ImzdquHZA6ngiYiISNKKROC44+Dcc0MnSSwqeCIiIpKU\n1q+HadNg+HDIzAydJrGo4ImIiEhSGjsWCgpgxIjQSRKPCp6IiIgkpUgE2rWDli1DJ0k8KngiIiKS\ndBYsgI8+0uSKg1HBExERkaSTk+OfuxsyJHSSxKSCJyIiIkmloMDvPXv++dCgQeg0iUkFT0RERJLK\n66/DmjUani2JCp6IiIgklUgEjj4a+vQJnSRxqeCJiIhI0ti+HXJzYdAgqFIldJrEpYInIiIiSSM3\nF3buhMsvD50ksangiYiISNKIRKBpU+jcOXSSxKaCJyIiIknhyy9h1iw/ucIsdJrEpoInIiIiSWHM\nGHDO7z0rJVPBExERkYTnnB+e7dIFTjopdJrEF9eCZ2a9zewzM1tqZjcX83plMxsfff19M2tc5LU/\nRo9/Zma9oseON7NZZrbIzD41s18XOb+Omb1qZkuiv9eO5/cmIiIi5ScvDxYv1tp3sYpbwTOzTOBR\n4HzgNGCImZ12wGk/BTY7504G/gLcH33vacBgoAXQG3gser184LfOueZAJ+CXRa55MzDTOdcMmBn9\nXERERFJAJAKVK8Mll4ROkhzieQfvDGCpc+5z59weYBzQ74Bz+gGjoh9PBLqbmUWPj3PO7XbOfQEs\nBc5wzq11zn0E4JzbDiwCGhVzrVHAxXH6vkRERKQc7dkDY8fCxRf7BY6ldPEseI2AlUU+X8V3ZewH\n5zjn8oGtQN1Y3hsdzj0deD966Bjn3NrotdYC2p1OREQkBUybBl9/reHZQxHPglfcBGYX4zklvtfM\nqgOTgBucc9sOKZTZ1WaWZ2Z5GzZsOJS3ioiISACRCDRoAD17hk6SPOJZ8FYBxxf5/EfAmoOdY2YV\ngKOBTSW918wq4svdGOdcbpFz1plZw+g5DYH1xYVyzj3pnMtyzmXVr1//ML81ERERKQ+bNsGLL8Kw\nYVChQug0ySOeBe9DoJmZNTGzSvhJE1MOOGcKsG+zkYHA6845Fz0+ODrLtgnQDPgg+nzeP4FFzrmH\nS7jW5cALZf4diYiISLkaPx727tXw7KGKWxd2zuWb2XXADCAT+Jdz7lMzuxPIc85NwZe1HDNbir9z\nNzj63k/NbAKwED9z9pfOuQIzOwsYAfzHzOZFv9QtzrmpwH3ABDP7KfAloHk2IiIiSS4SgVatoE2b\n0EmSi/kbZukpKyvL5eXlhY4hIiIixfjvf+HHP4b//V/43e9CpwnPzOY657JiOVc7WYiIiEhCysmB\njAwYOjR0kuSjgiciIiIJp7DQF7zzzoPjjgudJvmo4ImIiEjCefNNWLFCkysOlwqeiIiIJJxIBKpX\n97tXyKFTwRMREZGEsnMnPPec33e2atXQaZKTCp6IiIgklBdegO3bNTx7JFTwREREJKFEInDCCXD2\n2aGTJC8VPBEREUkYa9fCK6/AiBF+iRQ5PPqjExERkYTx7LN+iZQRI0InSW4qeCIiIpIwIhHo2NHv\nYCGHTwVPREREEsInn8D8+ZpcURZU8ERERCQhRCJQsSIMGhQ6SfJTwRMREZHg8vNhzBi46CKoWzd0\nmuSngiciIiLBvfoqrFun4dmyooInIiIiwUUiUKcOXHBB6CSpQQVPREREgtq6FZ5/HoYMgUqVQqdJ\nDSp4IiIiEtTEifDttxqeLUsqeCIiIhJUJOLXvevQIXSS1KGCJyIiIsF88QXMmePv3pmFTpM6VPBE\nREQkmNGj/e/Dh4fNkWpU8ERERCQI5/zwbLducMIJodOkFhU8ERERCeK992DpUk2uiAcVPBEREQki\nEoGjjoIBA0InST0qeCIiIlLudu+GceMgOxtq1AidJvWo4ImIiEi5e+kl2LJFw7PxooInIiIi5W7U\nKGjYELp3D50kNangiYiISLlavx6mTfNLo2Rmhk6TmlTwREREpFyNGwf5+RqejacSC56ZDS/y8ZkH\nvHZdvEKJiIhI6opE4PTToWXL0ElSV2l38G4s8vHfD3jtyjLOIiIiIinu009h7lzdvYu30gqeHeTj\n4j4XERERKVFOjn/ubsiQ0ElSW2kFzx3k4+I+FxERETmoggK/92zv3nDMMaHTpLYKpbx+qpnNx9+t\naxr9mOjnJ8U1mYiIiKSUWbNg9Wp4+OHQSVJfaQWvebmkEBERkZQXicDRR0OfPqGTpL4SC55zbkXR\nz82sLnA28KVzbm48g4mIiEjq2LEDJk2CYcP8/rMSX6Utk/KSmbWMftwQWICfPZtjZjeUQz4RERFJ\nAbm5sHOnZs+Wl9ImWTRxzi2IfnwF8Kpzrg/QES2TIiIiIjGKRKBJEzjzzNLPlSNXWsHbW+Tj7sBU\nAOfcdqAwXqFEREQkdaxcCa+/7u/emRZZKxelTbJYaWa/AlYB7YDpAGZ2FFAxztlEREQkBYwZA87B\niBGhk6SP0u7g/RRoAYwEBjnntkSPdwKeiWMuERERSQHO+eHZM8+Epk1Dp0kfpc2iXQ/8opjjs4BZ\n8QolIiIiqWHuXFi0CP7xj9BJ0kuJBc/MppT0unOub9nGERERkVQSiUDlynDppaGTpJfSnsHrDKwE\nxgLvo/1nRUREJEZ79sDYsdCvH9SqFTpNeimt4B0LnAcMAYYCLwNjnXOfxjuYiIiIJLfp02HjRq19\nF0KJkyyccwXOuenOucvxEyuWArOjM2tFREREDioSgQYNoGfP0EnST2l38DCzysCF+Lt4jYG/Abnx\njSUiIiLJbNMmePFFuPZaqKiF1cpdaZMsRgEtgWnAHUV2tRARERE5qAkT/DN4Gp4No7Q7eCOAb4BT\ngOvtu+WnDXDOuZpxzCYiIiJJKhKBli2hbdvQSdJTaevglbYQsoiIiMj3LFkC774LDzygrclCUYET\nERGRMpWTAxkZMGxY6CTpSwV4LUJbAAAgAElEQVRPREREykxhoS94PXrAcceFTpO+VPBERESkzLz1\nFixfrskVoangiYiISJmJRKB6dbj44tBJ0ltMBc/Mss1siZltNbNtZrbdzLbFO5yIiIgkj127/PIo\nAwdCtWqh06S3Uhc6jnoA6OOcWxTPMCIiIpK8XngBtm/X8GwiiHWIdp3KnYiIiJQkEoHjj4dzzgmd\nRGK9g5dnZuOB54Hd+w4657RlmYiIiPDVVzBjBtx8s18iRcKKteDVBHYCRbcLdmhPWhEREQGefdYv\nkTJiROgkAjEWPOfcFfEOIiIiIskrEoEzzoBTTw2dRCD2WbQ/MrPJZrbezNaZ2SQz+1G8w4mIiEji\n++QT/0uTKxJHrKPkzwBTgOOARsCL0WMiIiKS5nJyoGJFGDQodBLZJ9aCV98594xzLj/6699A/Tjm\nEhERkSSQnw9jxsCFF0K9eqHTyD6xFryNZjbczDKjv4YDX8czmIiIiCS+117zM2g1PJtYYi14VwKX\nAl8Ba4GB0WMiIiKSxiIRqFMHLrggdBIpKtZZtF8CfeOcRURERJLItm0weTJceSVUrhw6jRRVYsEz\nsz845x4ws7/j1737Hufc9XFLJiIiIglt4kT49lsNzyai0u7g7dueLC/eQURERCS5RCJwyil+/TtJ\nLCUWPOfci9EPdzrnniv6mpldErdUIiIiktCWL4c33oC77waz0GnkQLFOsvhjjMdEREQkDYwe7X8f\nPjxsDileac/gnQ9cADQys78VeakmkB/PYCIiIpKYnPPDs127woknhk4jxSntGbw1+Ofv+gJzixzf\nDvwmXqFEREQkcb3/PixZAn/UWF7CKu0ZvE+AT8zsWefc3nLKJCIiIgksEoGjjoIBA0InkYOJaR08\noLGZ3QucBlTZd9A5d1JcUomIiEhC2r0bxo2D/v2hZs3QaeRgYp1k8QzwOP65u25ABMiJVygRERFJ\nTC+/DJs3a+27RBdrwTvKOTcTMOfcCufc7cC58YslIiIiiSgSgYYNoXv30EmkJLEO0X5rZhnAEjO7\nDlgNNIhfLBEREUk0Gzf6O3g33AAVYm0QEkSsd/BuAKoC1wPtgRHA5fEKJSIiIoln3DjIz9fwbDKI\nqX875z6MfrgDuCJ+cURERCRRRSLQti20ahU6iZSmxDt4ZvaimU052K/SLm5mvc3sMzNbamY3F/N6\nZTMbH339fTNrXOS1P0aPf2ZmvYoc/5eZrTezBQdc63YzW21m86K/LojlD0BERERKt2gRfPih7t4l\ni9KGaB8EHgK+AHYBT0V/7QAWlPA+zCwTeBQ4H7+8yhAzO+2A034KbHbOnQz8Bbg/+t7TgMFAC6A3\n8Fj0egD/jh4rzl+cc22jv6aW8r2JiIhIjHJyIDMThgwJnURiUWLBc8694Zx7AzjdOTfIOfdi9NdQ\n4KxSrn0GsNQ597lzbg8wDuh3wDn9gFHRjycC3c3MosfHOed2O+e+AJZGr4dzbg6w6RC+RxERETkC\nhYW+4PXqBcceGzqNxCLWSRb1zWz/osZm1gSoX8p7GgEri3y+Knqs2HOcc/nAVqBujO8tznVmNj86\njFu7uBPM7GozyzOzvA0bNsRwSRERkfQ2ezasWqXh2WQSa8H7DTDbzGab2WxgFn5mbUmsmGMuxnNi\nee+BHgeaAm2Btfih5R9exLknnXNZzrms+vVL66giIiISifhdK/r2DZ1EYhXrLNrpZtYMODV6aLFz\nbncpb1sFHF/k8x8Baw5yziozqwAcjR9+jeW9B2Zct+9jM3sKeKmUfCIiIlKKb76BiRP9s3dHHRU6\njcSqtFm050Z/zwYuxN8hawpcGD1Wkg+BZmbWxMwq4SdNHDjzdgrfrac3EHjdOeeixwdHZ9k2AZoB\nH5SStWGRT/tTyiQQERERKd3kyb7kaXg2uZR2B+8c4HWgTzGvOSD3YG90zuVHd72YAWQC/3LOfWpm\ndwJ5zrkpwD+BHDNbir9zNzj63k/NbAKwEL//7S+dcwUAZjYW6ArUM7NVwG3OuX8CD5hZ22iu5cDP\nY/j+RUREpASRCDRpAmeeGTqJHArzN8zSU1ZWlsvLywsdQ0REJCGtWgUnnAC33gp33BE6jZjZXOdc\nViznlngHz8xuLOl159zDhxJMREREkseYMeAcjBgROokcqtKGaGuUSwoRERFJKM754dmf/AROPjl0\nGjlUJRY855xuyIqIiKShjz6ChQvhiSdCJ5HDEdMyKWZWBb+tWAugyr7jzrkr45RLREREAopEoFIl\nuPTS0EnkcMS60HEOcCzQC3gDvy7d9niFEhERkXD27oVnn/ULG9cudl8oSXSxFryTnXO3At8450bh\n18RrFb9YIiIiEsr06bBxo9a+S2axFry90d+3mFlL/I4TjeOSSERERIKKRKBePejdO3QSOVyxFrwn\nzaw2cCt+l4mFwP1xSyUiIiJBbN4MU6bA0KFQsWLoNHK4YppkATwT3UniDeCkOOYRERGRgCZMgD17\nNDyb7GK9g/eFmT1pZt3NzOKaSERERIKJROC006Bdu9BJ5EjEWvB+DLwG/BJYbmb/Z2ZnxS+WiIiI\nlLelS+Gdd/zdO93OSW4xFTzn3C7n3ATnXDbQFqiJH64VERGRFJGT44vd8OGhk8iRivUOHmZ2jpk9\nBnyEX+xYSx+KiIikiMJCPzzbowc0ahQ6jRypWHey+AKYB0wAfu+c+yauqURERKRcvf02LF8Od90V\nOomUhVhn0bZxzm2LaxIREREJJhKBatWgf//QSaQsxPoMnsqdiIhIitq1yy+PMnCgL3mS/GJ+Bk9E\nRERS05QpsG2b1r5LJSp4IiIiaS4SgeOPh65dQyeRslLiM3hmdkKM19miYVwREZHk89VXMGMG/OEP\nkKHbPimjtEkWowAHlLTcoQP+DUTKKJOIiIiUk7FjoaAARowInUTKUokFzznXrbyCiIiISPmLRKBD\nB2jePHQSKUu6GSsiIpKm5s+HefM0uSIVqeCJlLGCApgzB267DVauDJ1GROTgcnKgQgUYPDh0Eilr\nsS50LCIl2LMHZs2CSZPg+edhwwZ//Pnn/erw1auHzScicqD8fBg9Gi68EOrVC51GypoKnshh2rkT\nXnnFl7oXX4StW32Ru+giyM6GihVhwAA/9DFxomaniUhimTnTz6DV8GxqinUv2u342bJFbQXygN86\n5z4v62AiiWjbNnj5ZV/qpk3zJa9OHV/osrP9Jt1Vqnx3/kMPwW9+A3feCbffHiy2iMgPRCJQu7a/\ngyepJ9Y7eA8Da4Bn8UumDAaOBT4D/gV0jUc4kUSwcaNf5X3SJHjtNT8c27AhjBzpS9055/hnWIrz\n61/7h5jvuANatvTbAImIhLZtG0ye7P8eq1w5dBqJh1gLXm/nXMcinz9pZu855+40s1viEUwkpNWr\n/fNzkybBG29AYSE0bgy/+pUvdZ06xTbkagaPPw6LF8Pll8PJJ0PbtnGPLyJSokmT/P6zGp5NXbEW\nvEIzuxSYGP286H2IA4duRZLS559Dbq7/9e67/ljz5nDLLb7UtW3rC9uhqlzZX7NDB+jXDz78EBo0\nKNvsIiKHIhKBZs2gY8fSz5XkFGvBGwb8FXgMX+jeA4ab2VHAdXHKJhJXzsHChd+Vunnz/PF27eCe\ne6B//7Jb+PPYY+GFF+Css/zEi5kzoVKlsrm2iMihWLECZs+Gu+46vB9aJTnEVPCikyj6HOTlt8ou\njkh8OQdz535X6j77zP8F95OfwMMP+1LXuHF8vna7dvDMM369qV/+Ep58Un+5ikj5Gz3a/z58eNgc\nEl+xzqKtD1wFNC76HufclfGJJVJ2CgrgnXe+K3VffgmZmdCtm58EcfHFftJEeRg0CP7zH3+HsE0b\nuE73v0WkHDnnh2fPOSd+P8xKYoh1iPYF4E3gNaAgfnFEysbevX7h4dxcP1li3Tr/LFzPnn5Ga9++\nfnmTEO68ExYsgBtu8EPA3buHySEi6eeDD+C//4WbbgqdROIt1oJX1Tmn/xwkoe3a5Rcezs31y5ps\n2QLVqvk1nrKz4YILoEaN0Cn97NucHOjcGS65xE+6aNo0dCoRSQeRiF+rU0s2pb5YC95LZnaBc25q\nXNOIHKLt2/3Cw7m5MHUqfPONX7izXz8/maFHDzjqqNApf6hGDV9CO3TwdxPffRdq1gydSkRS2e7d\nMG6cf9ZYf9+kvlgL3q+BW8xsN7AXv9ixc87pPxEpd19/7ctRbi68+qr/S+uYY2DECH+nrmtXv01Y\nojvpJHjuOT9sPGyYH0rOzAydSkRS1dSpsGmT1r5LF7HOok2AgS1JZ2vX+lXXc3P99P6CAjjxRLj2\nWl/qOndOznJ07rnw17/6yRa33gr/8z+hE4lIqopE/JJNPXqETiLlocSCZ2anOucWm1m74l53zn0U\nn1gi8MUX31942Dn48Y/9w8HZ2X7ZkVRYZuTaa/12ZvfeC61awZAhoROJSKrZuNE/znL99QffWlFS\nS2n/M98IXA08VMxrDji3zBNJWlu0yG+hk5sLH3/sj51+up95mp0Np50WNl88mMHf/+6/9yuv9KvL\nZ2WFTiUiqWT8eL+6gIZn04c5l747jWVlZbm8vLzQMdKac77I7St1ixf74507+0kS/fv7Z9XSwYYN\nftJFfr6fWVtea/OJSOrr2NE/r7xvxx5JTmY21zkX0y2AmG/UmtlP+OFCx5FDTidpr7DQD7nuG35d\nvtw/P3fOOfCrX/mFh487LnTK8le/vt/O7Cc/8cV29my/nIGIyJFYvNivf/dQcWNxkrJi3ckiB2gK\nzOO7hY4doIInMdm7F954wxe6yZPhq6/8XqznnQd//jP06QP16oVOGV6bNv5B6IED4Re/8FubpcJz\nhiISTk6OX39z6NDQSaQ8xXoHLws4zaXzeK4csm+/9cuY7Ft4eNMmqFrVLzg8YID/XWsx/dCAAXD7\n7f5X69Zw442hE4lIsios9AWvVy8/g1bSR6wFbwFwLLA2jlkkBWzfDtOm+VL38suwYwccfbRfzDc7\n2/8lk4gLDyeaW2/1e9b+/vd+Yknv3qETiUgyeuMNWLkSHnggdBIpb7EWvHrAQjP7ANi976Bzrm9c\nUklS2bQJXnzRl7oZM/yDvA0a+OGA7Gzo1s0Px0rsMjJg1ChYuhQGD4b33/dLxIiIHIpIxI+U9OsX\nOomUt1gL3u3xDCHJ56uv/M4Lubkwa5af+Xn88f65sexsOPPM5Fx4OJFUq+YnXXTo4P9yfu89qFUr\ndCoRSRbffAMTJ8KgQRo5SUex7mTxRryDSOJbseK7ma9vv+2XOGnWDH73O1/qsrI0IaCsnXiiX0Lm\n3HP9AsgvvaTiLCKxef55/5iM1r5LT6XtZPGWc+4sM9uOnzW7/yW0F21a+Oyz79aomzvXH2vTxk8A\nyM6GFi1U6uKtSxd47DG4+mq4+Wb43/8NnUhEkkEkAo0bw1lnhU4iIZRY8JxzZ0V/1160acI5+OST\n70rdwoX+eMeO/iHd/v3h5JPDZkxHV13ltzN78EG/nZl+IheRkqxeDa+9Bn/6k3+mV9JPqUO0ZpYB\nzHfOtSyHPBJAYaF/iH9fqfviC/8XwtlnwzXX+IWHf/Sj0Cnl4Yd94b7qKjjlFOjUKXQiEUlUzz7r\n/24fMSJ0Egml1ILnnCs0s0/M7ATn3JflEUriLz8f5szxpW7yZFi7FipWhB49/E98ffv6nRUkcVSs\nCBMmwBln+DupeXnQqFHoVCKSaJzzs/A7d/bPSUt6inUWbUPg0+gyKd/sO6hlUpLL7t3+lv2kSX7h\n4a+/9jOrzj/fL6574YV+zTpJXHXr+v/tOnXyd1bnzNHsOBH5vnnz4NNP4fHHQyeRkGIteHfENYXE\nzY4dMH26L3Uvv+wXIq5Z028Nlp3tF9CtWjV0SjkULVr44Zd+/eBnP4PRozXRRUS+E4n4tUcvvTR0\nEgkp1oJ3gXPupqIHzOx+QMunJKDNm/1yGrm5vtx9+63f53XQIF/qunfXwsPJrk8fuOceuOUWv53Z\nTTeV/h4RSX179/ofAPv0gTp1QqeRkGIteOcBB/4Tcn4xxySQdev8ori5uTBzpn/GrlEj/0B+draf\nJl8h1v+1JSncfLOfWfvHP/q7ehddFDqRiIT2yiuwfr1m2kvp6+BdA1wLnGRm84u8VAN4O57BpHRf\nfuknSOTmwptv+gdrmzb1m9NnZ/sdEDQ9PnWZwT//CUuW+G3h3nvP71srIukrEvEjNtq/Wkq7p/Ms\nMA24F7i5yPHtzrlNcUslB/Xf/363m8SHH/pjrVrBn//sS12rVnoeK51UrepXq8/K8jOfP/hAwzIi\n6WrLFj+Sc/XVegxHSl/oeCuwFRhSPnHkQM75Ybh9pW7BAn+8Qwe47z6/XMYpp4TNKGH96Ef+Tm7X\nrv6h6unTNRwvko6ee86vlqDhWYHYn8GTclRY6O/O7Vt4eNkyf1euSxf461/98hgnnBA6pSSSzp3h\nySdh5Ej47W/9fycikl4iEWjeHNq3D51EEoEKXoLIz4e33vpu4eHVq/1dmO7d/QzJvn3hmGNCp5RE\ndvnl/m7vww/7ofqf/Sx0IhEpL8uW+X9D7r1Xj+mIp4IX0O7d8PrrvtS98AJs3AhVqviHY++7z8+K\nrFUrdEpJJvff7xc4vfZaOPVUbTIuki5ycnyxGzYsdBJJFCp45eybb2DGDF/qXnoJtm2DGjV8mRsw\nwJe7atVCp5RkVaECjBsHHTv6STd5eRrOF0l1zvnh2XPPheOPD51GEoUKXjnYssXvIjFpkn8Aftcu\nv+XUwIH+H+EePaBy5dApJVXUquW3M+vY0e928dZb+qFBJJW9/TZ88QXcfnvoJJJIVPDi6Ntv/SzX\nmTP96uLHHQdXXulL3dlna6ajxM+Pfwxjx/o7wyNHwoQJei5HJFVFIn7JpOzs0EkkkWgZ3DiqUsX/\n+vWv4Z13YOVK+L//87fRVe4k3s4/Hx54ACZOhLvvDp1GROJh1y7/A9yAAVC9eug0kkhUM+Js8uTQ\nCSSd3Xijn1n75z9Dy5b+jrKIpI4XX4StW7X2nfyQ7uCJpDAz+Mc//PN4I0b4siciqSMS8fuOd+sW\nOokkGhU8kRRXpYq/k3z00X49xQ0bQicSkbKwbp2fuDdiBGRmhk4jiUYFTyQNNGzo96xdt87P3t6z\nJ3QiETlSY8dCQYEveCIHUsETSRMdOsA//wlz5viJPyKS3CIRyMqC004LnUQSkQqeSBoZOtRvfffE\nE/D446HTiMjh+s9/4OOPNblCDk4FTyTN3HMPXHghXH89zJoVOo2IHI6cHL/c1uDBoZNIolLBE0kz\nmZnw7LPQrBlccgl8/nnoRCJyKAoKYPRouOACqF8/dBpJVCp4ImmoZk2/nVlhod/ObPv20IlEJFYz\nZ8LatRqelZKp4ImkqZNP9ivgL1rkZ+EVFoZOJCKxiET8ntMXXRQ6iSQyFTyRNNajBzz8MLzwAtx2\nW+g0IlKa7dshN9c/e1e5cug0ksi0VZlImvvVr/wOF3ff7bczGzQodCIROZhJk/z+sxqeldLE9Q6e\nmfU2s8/MbKmZ3VzM65XNbHz09ffNrHGR1/4YPf6ZmfUqcvxfZrbezBYccK06ZvaqmS2J/l47nt+b\nSKowg0cfhTPPhCuugI8+Cp1IRA4mEvGPV3TqFDqJJLq4FTwzywQeBc4HTgOGmNmByzH+FNjsnDsZ\n+Atwf/S9pwGDgRZAb+Cx6PUA/h09dqCbgZnOuWbAzOjnIhKDypX9nYF69fyki3XrQicSkQOtWOGX\nNrrsMv+DmUhJ4nkH7wxgqXPuc+fcHmAc0O+Ac/oBo6IfTwS6m5lFj49zzu12zn0BLI1eD+fcHGBT\nMV+v6LVGAReX5TcjkuqOOcY/i/f115CdDbt3h04kIkWNGeN/Hz48bA5JDvEseI2AlUU+XxU9Vuw5\nzrl8YCtQN8b3HugY59za6LXWAg2KO8nMrjazPDPL26Bd10W+5/TTYdQoeOcduOYacC50IhEB///F\nSATOPhuaNAmdRpJBPAtecTeQD/zn4mDnxPLew+Kce9I5l+Wcy6qvFSJFfuCSS+DWW+GZZ+Bvfwud\nRkQAPvwQPvtMkyskdvEseKuA44t8/iNgzcHOMbMKwNH44ddY3nugdWbWMHqthsD6w04ukuZuvx0u\nvhhuvBFeeSV0GhGJRKBKFRg4MHQSSRbxLHgfAs3MrImZVcJPmphywDlTgMujHw8EXnfOuejxwdFZ\ntk2AZsAHpXy9ote6HHihDL4HkbSUkeH3umzRwi+bsmRJ6EQi6WvPHhg71v/QdfTRodNIsohbwYs+\nU3cdMANYBExwzn1qZneaWd/oaf8E6prZUuBGojNfnXOfAhOAhcB04JfOuQIAMxsLvAv82MxWmdlP\no9e6DzjPzJYA50U/F5HDVL26n3SRmQl9+8LWraETiaSnqVNh0yYNz8qhMZfGT1FnZWW5vLy80DFE\nEtrs2XDeedCzp9+/NjOz1LeISBnKzvYTn1atggraniCtmdlc51xWLOdqqzIRKVHXrvD3v/u7CLfc\nEjqNSHr5+mt46SUYNkzlTg6N/nMRkVL94hd+O7MHHoDWrf0/NiISf+PHw969Gp6VQ6c7eCISk7/+\n1d/N++lP/ZINIhJ/kYj/oapNm9BJJNmo4IlITCpWhOeeg4YN/Wy+NaUtXCQiR+Szz+D993X3Tg6P\nCp6IxKxePT/RYutW6N8fvv02dCKR1JWT45csGjo0dBJJRip4InJIWrWC0aPhgw/g6qu1nZlIPBQW\n+oLXs6e/ay5yqFTwROSQXXwx3HWX/wfooYdCpxFJPXPmwJdfanhWDp8Knogclj/9ye9b+4c/+CVU\nRKTsRCJQowb06xc6iSQrFTwROSxm8Mwz0LYtDBkCixeHTiSSGnbu9BOaLrkEqlYNnUaSlQqeiBy2\natXg+ef9Juh9+8LmzaETiSS/55+HHTs0PCtHRgVPRI7ICSdAbi4sXw6DBkF+fuhEIsktEoETT4Qu\nXUInkWSmgiciR+zMM+GJJ+DVV/0zeSJyeNas8f8/GjHCL5Eicri0VZmIlIkrr/Tbmf3lL34plSuu\nCJ1IJPk8+6xfImXEiNBJJNnp5wMRKTMPPgg9evi9a995J3QakeTiHIwaBZ06wSmnhE4jyU4FT0TK\nTIUKfnP0E06A7GxYuTJ0IpHk8cknsGCBJldI2VDBE5EyVaeO385s506/IPLOnaETiSSHSMTv+Txo\nUOgkkgpU8ESkzDVvDmPHwscf+2fztJ2ZSMny82HMGOjTx/+QJHKkVPBEJC4uvBDuvdcP2d57b+g0\nIontlVdg/XoNz0rZ0SxaEYmbP/zBz6z905+gRQttuyRyMJEI1K0L558fOomkCt3BE5G4MYOnn4as\nLBg+3D9ALiLft2WL371iyBCoVCl0GkkVKngiEldHHeX/8ape3W9n9vXXoROJJJaJE2H3bg3PStlS\nwRORuGvUyJe8NWv8Bup794ZOJJI4IhE49VR/p1ukrKjgiUi56NgRnnwSZs2C3/wmdBqRxPD55/Dm\nm/7unVnoNJJKNMlCRMrNZZfBf/7jd7xo1Qp+/vPQiUTCGj3aF7thw0InkVSjO3giUq7uuw9694br\nroM5c0KnEQnHOT88262b3/1FpCyp4IlIucrM9IsgN20KAwbA8uWhE4mE8e67sGyZJldIfKjgiUi5\nq1XLb2e2d69fG2/HjtCJRMpfJAJVq/p9m0XKmgqeiARxyil+l4t9m6sXFoZOJFJ+vv3W//efnQ01\naoROI6lIBU9EgunVy0+4mDwZ7rwzdBqR8vPSS36BYw3PSrxoFq2IBHXDDX47szvugJYtYeDA0IlE\n4i8SgeOOg3PPDZ1EUpXu4IlIUGbwxBPQuTNcfjnMmxc6kUh8rV8P06b57fsyM0OnkVSlgiciwVWu\nDLm5UKeOn3Sxfn3oRCLxM24c5OfDiBGhk0gqU8ETkYRw7LF+O7P16/3yKXv2hE4kEh+RCLRr5x9J\nEIkXFTwRSRjt28Mzz8Bbb8Evf+kXghVJJZ9+CnPnanKFxJ8mWYhIQhk82G9n9j//A23a+B0vRFJF\nTo5/7m7IkNBJJNXpDp6IJJy77oI+ffwM25kzQ6cRKRsFBb7gnX8+NGgQOo2kOhU8EUk4GRl+E/ZT\nT4VLLvHbOYkku9dfhzVrNDwr5UMFT0QSUs2a8MILfhmVvn1h27bQiUSOTCTit+nr0yd0EkkHKngi\nkrCaNoXnnoPPPoNhw/wQl0gy2r7dLwU0aBBUqRI6jaQDFTwRSWjnngt//avf2unWW0OnETk8ubmw\nc6eGZ6X8aBatiCS8a6/125ndey+0aqUZiJJ8IhF/R7pz59BJJF3oDp6IJDwz+PvfoUsXuPJKyMsL\nnUgkdl9+CbNm+bt3ZqHTSLpQwRORpFCpEkyaBMccAxdfDGvXhk4kEpsxY/yi3cOHh04i6UQFT0SS\nRv36fmbt5s2QnQ27d4dOJFIy5/zwbJcucNJJodNIOlHBE5Gk0qaN/wfzvffgF7/QdmaS2PLyYPFi\nTa6Q8qeCJyJJZ8AAuP12+Pe/4ZFHQqcRObhIBCpX9gt2i5QnFTwRSUq33uqL3u9+BzNmhE4j8kN7\n9sDYsf6Z0aOPDp1G0o0KnogkpYwMGDXKL5syaBD897+hE4l837Rp8PXXGp6VMFTwRCRpVavmJ11U\nquS3M9uyJXQike9EItCgAfTsGTqJpCMVPBFJaiee6JdPWbbML4Cs7cwkEWzaBC++6LfYq6AtBSQA\nFTwRSXpdusCjj8L06XDzzaHTiMD48bB3r4ZnJRz9XCEiKeHq/9/evQfHVd5pHv/+Wn2RZEvqxjcs\nWzYmmOCAMcbGYOwYkJYt2LB4yAJhyQ1yKyBTZCuQmUCGLJsKVSE1NRkys5kUMzAJTFiSkB3isJlQ\niYUx5mJswFydEAIEm5hgbLVu1sXqfveP90hq2i25MWp16/TzqVL59Dlvt96XI44enfNevuCXM/vb\nv/X98vSLVcrprrv8z8rLof0AABjvSURBVOGyZeWuiVQr3cETkdD4znfgnHPg85/38+SJlMPLL/uf\nPy1NJuWkgCcioRGLwU9/CvPmwUUXwZtvlrtGUo3uvtuP8r788nLXRKqZAp6IhMqMGbBhA/T0+PnH\n+vrKXSOpJtmsD3jnngvNzeWujVQzBTwRCZ2TTvILvD/1FHzuc1rOTCbPI4/AH/+oPqBSfgp4IhJK\nF14I3/wm3HMPfPvb5a6NVIu77oLp0/3dY5FyUsATkdC64Qa/ysUNN8ADD5S7NhJ2Bw74PqCXXAL1\n9eWujVQ7BTwRCS0zuPNOWL7cd3h/6aVy10jCqKMD7r/fdwfo7tbjWakMmgdPREKtvt7/8j3tNP/Y\n9skn4aijyl0rmcp6e2HLFti4Edrb4emnfT/P+nr49Kdh3bpy11BEAU9EqkBLC/z7v8PZZ8Oll/oV\nL7R8lBRrYMDPa9fe7r+2bvWrVMRisHo13HwztLbCqlV+XWSRSqBLnIhUhdWr4fvfh898Bq67Dm67\nrdw1kko1NOTvyg0Hui1b/HQ7kQisXOl/flpbYc0a9bWTyqWAJyJV48or4fnn/YoXS5f6PlMi2Sy8\n+OJooNu0Cbq6/LGlS/0yeK2tcNZZ0NRU1qqKFE0BT0Sqyre/7X+ZX3MNnHACrF1b7hrJZHMO/vAH\nH+Y2boSHHoK9e/2x446Dyy6Dtjb/SH/27LJWVeSIKeCJSFWJRuHee+H00+GjH4Xt22HBgnLXSkpt\n9+7RO3Tt7bBrl98/bx6cf76/Q3fOOfpZkPBQwBORqpNK+eXMTj8d1q/3faymTSt3rWQi7d3rH7UO\nB7qXX/b7Z8zwYe7GG/2/ixf76XREwkYBT0Sq0gkn+Dt5H/kIXHEF/OQn+kU/lXV1webNo1OXPPec\n39/Q4PvOXXWVD3RLl/rBEiJhp4AnIlXr/PN9n7yvfMUva3bTTeWukRSrrw8efXT0Dt327ZDJQG2t\nH916yy2+H92KFZoSR6qTfuxFpKpdd52/2/P1r8NJJ8FFF5W7RlLIwYN+kurhQPfYYzA46MPbqlWj\nj1zPOMOHPJFqp4AnIlXNDG6/HX73O/jkJ31wOPnkctdKMhl49tnRQLd5s19BwgxOOQWuvdYHurVr\n/WNYEXk3BTwRqXq1tX6li+HlzLZtg1mzyl2r6uIc/Pa3o1OXbNrk13gFWLLE95Mcnotuxoxy1lRk\nalDAExEBmpv9mrUf/jBcfDH8+tdadqrUXn99dFBEezu89Zbfv3Chf1Q+PHVJc3NZqykyJSngiYgE\nTjsN7rgDPvEJ+NKX4J/+qdw1Cpc9e/ykwsOB7rXX/P45c3yYa231AyMWLSpvPUXCQAFPRCTHxz/u\nlzO79VbfF+/qq8tdo6lr/354+OHRQPfSS35/MulXifjyl32oW7JEU9SITDQFPBGRPLfcAi+84Dvy\nn3CCf0woh9fT4yeNHu5H98wzvm9dfT2sWzfaj+6UU6Cmpty1FQk3BTwRkTw1NXDPPX7KjUsu8dNz\nHHvs4d/31sAAD6XTPNXdjZmRMCMRiYx+FXhdm/t6jDKJSISoGVZht7kGBuCJJ0b70W3dCkNDvu/i\n6tVw880+0K1apf6MIpNNAU9EpIDGRr+c2apVfjmzxx47dDqOjoMHeTidpj2dZmNHBy8dOABAIghj\nA9ksboLqYzBuAJzo17UF9tVkI+x8LsKjm4zNGyM8usXo7/crQ6xcCddf7wPdmjX+rp2IlI85N1GX\nnwIfbnYecBtQA/yLc+5beccTwF3ACmAf8DHn3OvBsRuAzwIZ4Frn3IPjfaaZ/QA4C+gMPv4K59yO\n8eq3cuVKt3379vffUBEJrd/8Bs47Dy64AO7+aYbHujtp7+igPZ3m6e5uskBdJMKHm5poS6VoTSZZ\n3tBAjRnOOYacYyCbZWD43+GvEr7uH6fM0ARe8yNZI2ER6qJGbU3pQ+fwV+0YZaJag0xCzsyecs6t\nLKZsye7gmVkN8L+Bc4HdwDYz2+Cceymn2GeBDufccWZ2GXAr8DEz+xBwGXAi0Az8xsyOD94z3md+\nxTl3X6naJCLVZTCbJb6yi3N/1MHP30qTfKSLbMQRM+OMxkZuWriQtlSKVY2NJAqECzMjZkYsEmF6\nGepfSDY39OUFwP6s4w9vZNn6dJbtz2V59iVHV18WYllmzcty/ImOYz+YpeUDWWJ1xYXO3kyG/WN+\nP79vokRg0kJmMa/jkQiRCnusLtWjlI9oVwGvOOdeBTCze4H1QG7AWw/cHGzfB/yj+U4m64F7nXMD\nwGtm9krweRTxmSIiRyTjHM90d7Mxnaa9o4MtnZ0cyGaxOTAz2sA7P57PDeel+NqFTUyboqMEImbU\n1dRQF9R/9254vH20H93u3b7cvHnwF22jc9EtWFCa+jjnODhO6CzF656DB8c9PpF3OaNmxIPwN/Jv\nTgB81/73ePz9fpbueIZbKQPePGBXzuvdwOljlXHODZlZJzAj2P9E3nvnBdvjfeYtZvZ1YCPw1SAg\niogU5JzjpQMHaO/oYGNHB5vSaTozGQBOrK/ns3Pn0ppMclYySX02Rtt34O/vhou3wKmnlrnyR2jv\n3nfPRff73/v9M2aMzkXX2gqLF0/O1CUWBKB4JEKlrDiWcY7BvADY/x5C5ODw+3M+ZzCnzPD2yL/Z\nLN2ZzCFlc49PdPAEf8ezUsJm/vFYBQ4qmmpKGfAKnZn8n86xyoy1v9CfG8OfeQPwFhAHbgf+GvjG\nIZUy+wLwBYAFpfqTVEQq1qt9fSN96No7OvjzwYMALKqt5ZLZs2lNJjknmeToROKQ9/7sZ34y5PXr\nYft2P0Fvpevs9Ou4Dge6557z+xsa/LJfV1/tA93SpX6whEBN3l3OSpENAuNYAfBwAbHYsJlfti+b\npTOTOez3mmiVEjbzj0+VR++lDHi7gZac1/OBP41RZreZRYEmYP9h3ltwv3NuT7BvwMz+Fbi+UKWc\nc7fjAyArV64s3QgTEakIewYGRsJcezrN6/39ABwdj9OWStGWSnFOMsmiurrDftacOfDzn/tRoh/9\nqA9MBXJgWR044Ef8Dge6bdsgm/Xr7a5Z4+f4a2uDFSsgqnkUppSIGbU1NdSWuyIFDD9qL3XYLHS8\ndzh8jnF8IkezD4sG0yDlBsAl9fX8atmyCf5OR66U/3tvAxab2SLgTfygicvzymwAPg08DlwMtDvn\nnJltAO4xs7/DD7JYDDyJv7NX8DPNbK5zbk/Qh+8vgBdK2DYRqVD7Dx5kU06g2xlMXZKMRjknmeT6\nlhZak0lOqK8/okdAy5fDD38Il17q737dcUd5V2EYHPQhbnhy4ccf9/uiUT/Fy9e+5u/QnXGGD3ki\npZD7qL1SBhTlGpqEsDmnwiZ7LFnAC/rU/SXwIH5Kkzudcy+a2TeA7c65DcAdwN3BIIr9+MBGUO4n\n+METQ8AXnXMZgEKfGXzLH5nZLHwI3AFcVaq2iUjl6BkaYktn58jAiGd6enBAfSTCumSSK48+mrZU\nimXTp1MzQUnskkvgb/4GvvlNWLbMr1s7WTIZ2LFj9A7dI49Ab68PmcuX+9U3Wlth7dpD5+0TqVbR\nSIQoUF9hj91LqaTz4FU6zYMnMvUMZLM80dU1MjBia3c3Q84RN2N1YyOtwVx0qxobiZewU1k26x/T\n/uIX8Ktfwbnnlub7OAc7d44Guk2boKPDH1uyZHRQxFln+YESIhJeFTEPnojIRBjKZnm6p2fkkeuW\nzk76slkiwIqGhpFHrmuamib1r/NIBO6+G848Ez72Mb9M1+LFE/PZr702Guja2+Gtt/z+hQvhootG\nQ93cuRPz/UQkfBTwRKSiOOd4sbd35JHrwzlTl5w0bRqfnzuXtlSKdU1NJGOxsta1ocEvZzY8svbx\nx6Gp6b1/zp49fuqS4bnoXn/d758zxwe5tmA+ukWLJrT6IhJiCngiUlbOOV7t7x955PpQOs3bwdQl\nH6it5dLZs2lLpTg7may4TszgQ9d99/lHtB//uB9le7gbifv3+0etw3fodu70+5NJP6nwddf5QLdk\nSXkHcIjI1KWAJyKT7s2BAR7q6Bi5S/fGgJ+TfG48zn8+6ihak0laUykWTpFhn2efDd/9LlxzjR+1\n+q1vvft4T48fDDEc6J55xvetq6+Hdevgyit9oDvllMOHQxGRYijgiUjJ7cuZumRjRwe/6+sD4Khg\n6pK/XrCAtlSK4+vqpuzs9Vdf7ScRvvVW+OAH/Z294UC3dSsMDUE8DqtXw803+0C3apXfJyIy0RTw\nRGTCdQ8N8Uhn58jAiB3B1CXTa2pY19TE55ubaU0mWTZ9+pSYEb5Yt93mH7d+5jP+dSQCK1fC9df7\nfnRnnunv2omIlJoCnoi8b/2ZDE90dY08cn0yZ+qSM5ua+MYxx9CaSnFaQwOxEK+HFY/75cy+9z3/\nuHXduiMbdCEi8n4p4JXYI+k0M2IxWhIJGrQukITEUDbLUz09bOzooL2jg0e7uugPpi45raGBv2pp\noTWV4szGxopbz7PUZsyAm24qdy1EpNopcZTYBc8/T1cwxUNTTQ0LamtpSSRoSSTetd1SW8v8RIJE\niO9uyNSVdY4Xent9H7p0mofTabqDn+uTp03jquZm2pJJPpxM0qQ/ZEREyk5X4hJyzvH/li5l18AA\nbwwMsKu/f2R7a1cX+4aGDnnPnFiMltpaFuQEv9zto+PxCVtuSWQszjle6eujPXjk+lA6zd5g6pLF\ndXVcPns2rakU5ySTzNIoARGRiqOAV0JmxtpkcszjBzIZdg8M8EYQ/HblbO88cIAH9++nN5t913ui\nZsyLxw8Jfi2JhH9dW8tR0eiUHYko5bO7v38k0LWn0+wKpi6ZF49z/lFHjSwB1jJFpi4REalmCnhl\nVF9Tw/H19Rw/xrA65xzpoaFDwt/w9uNdXfx0YICDeesJ10ciBYNf7va0KusXJYd6Z3CQTen0yMCI\nl4OpS2ZEo7SmUtwYzEW3eApPXSIiUq0U8CqYmZGKxUjFYpw8fXrBMlnn+PPgYMEQuKu/n1/19vLW\n4CAu732paLRg8BvuEzgvkSjpQu0y+bqCqUuGB0Y829sL+KlLzmpq4qrmZlpTKZZOmxaqqUtERKqR\nAt4UFzFjbiLB3ESCVWOUGcxm+dNwP8Ag+OVuP9bZyf68/oAGHB2PHzIYJHd7TjyuIFDB+jMZHuvq\nGhkYsa2riwyQMGNNUxO3LFpEazLJipBPXSIiUo0U8KpAPBLhmLo6jqmrG7NMbybzrkEgudvP9/Tw\ny337OJDXHzBmxvzhfoBjBMGk+gNOmqFslm3d3SP96B7t7GTAOWqAVY2NfDVYLWJ1YyO1ekQvIhJq\nCngCwLSaGk6YNo0Tpk0reNw5x/6hoZHgl/84eEtnJ2/u3ctQXn/AaZHIuFPDtCQS1CtsHJGsczzf\n2zvyyHVzZ+fI1CWnTJ/OF+fNozWVYl1Tk+ZgFBGpMrrqS1HMjBmxGDNiMU5paChYJpPTH7DQyODn\ngv6A+WZEo+NODdMcj+sRIj5k/76vzwe6dJqHOjpGpto5vq6OT8yZQ2syydnJJDM1dYmISFVTwJMJ\nU2NGcyJBcyLB6Y2NBcsMZLO8OcaAkNf6+9nc2Uk6rz9gBJh7mKlhZsVioewPuCuYumT4Lt2bQUBu\nSST4rzNn0ppMck4yyXxNXSIiIjkU8GRSJSIRjq2r49hx+gN250wNkz8gZEdPDxv27aM/rz9g3Oyw\nU8NMhRUW9g4O8lDQh25jOs0rwdQlM2MxWpNJ2oK56D6gqUtERGQclf8bT6pOQzTKh6JRPjROf8B9\nBw8WHBCyq7+fh9Np3hwYIJP/uTU1444KbkkkJn3wQefQEJvT6ZG7dM8HU5c01tRwVjLJF5ubaUul\nOFFTl4iIyHuggCdTjpkxMx5nZjzO8nH6A+4ZvvNXoE/g093dvB0svZVrViw2bgicG48TfR/9Afsy\nGR7t7BwZ6bq9u5sMUBuJsLapif8eLAG2Yvr09/V9RESkuingSSjVmDG/tpb5tbWsHqNMf7BU3K4C\nQfCVvj7aOzroyrz7PmAN0HyYqWFmxmIjj08PBlOXDA+MeKyzk0HniJpxekMDNy5cSGsyyeqmJhIK\ndCIiMkEU8KRq1dbUcFx9PceNsVQc+NUfCg0IeWNggKd6erj/nXcYyJsapjZYKm5WLMZzvb30ZDIY\nfuqSa+fPpzWZZK2mLhERkRLSbxiRcTRGo5wYjXLiOP0B9wb9AfMHhLw1OMin5syhLZXirGSSGbHY\nJNdeRESqlQKeyPtgZsyOx5kdj7NijP6AIiIik02dfkRERERCRgFPREREJGQU8ERERERCRgFPRERE\nJGQU8ERERERCRgFPREREJGQU8ERERERCRgFPREREJGQU8ERERERCRgFPREREJGQU8ERERERCRgFP\nREREJGQU8ERERERCRgFPREREJGQU8ERERERCRgFPREREJGQU8ERERERCRgFPREREJGTMOVfuOpSN\nme0F/jgJ32om8M4kfJ9KVM1th+puv9pevaq5/dXcdqju9k9G2xc652YVU7CqA95kMbPtzrmV5a5H\nOVRz26G626+2V2fbobrbX81th+puf6W1XY9oRUREREJGAU9EREQkZBTwJsft5a5AGVVz26G626+2\nV69qbn81tx2qu/0V1Xb1wRMREREJGd3BExEREQkZBbwJZGbnmdnvzOwVM/tqgeMJM/txcHyrmR0z\n+bUsjSLafoWZ7TWzHcHX58pRz1IwszvN7G0ze2GM42Zm3w3+2zxnZqdOdh1LpYi2n21mnTnn/euT\nXcdSMbMWM3vIzHaa2Ytm9qUCZcJ87otpfyjPv5nVmtmTZvZs0Pb/VaBMKK/3RbY9tNf7YWZWY2bP\nmNkDBY5Vxrl3zulrAr6AGuAPwLFAHHgW+FBemWuA7wfblwE/Lne9J7HtVwD/WO66lqj964BTgRfG\nOP5fgP8ADDgD2FruOk9i288GHih3PUvU9rnAqcF2A/BygZ/7MJ/7YtofyvMfnM/pwXYM2AqckVcm\nrNf7Ytoe2ut9Thu/DNxT6Oe7Us697uBNnFXAK865V51zg8C9wPq8MuuBHwbb9wFtZmaTWMdSKabt\noeWc2wzsH6fIeuAu5z0BJM1s7uTUrrSKaHtoOef2OOeeDra7gZ3AvLxiYT73xbQ/lILz2RO8jAVf\n+R3aQ3m9L7LtoWZm84GPAP8yRpGKOPcKeBNnHrAr5/VuDr3YjZRxzg0BncCMSaldaRXTdoD/Fjym\nus/MWianahWh2P8+YbU6eJzzH2Z2YrkrUwrBI5jl+LsZuari3I/Tfgjp+Q8e0e0A3gZ+7Zwb89yH\n7HpfTNsh3Nf7vwf+CsiOcbwizr0C3sQplM7z/6oppsxUVEy7fgEc45w7GfgNo3/dVIOwnvdiPI1f\nWmcZ8A/A/WWuz4Qzs+nAz4D/4Zzryj9c4C2hOveHaX9oz79zLuOcOwWYD6wys5PyioT23BfR9tBe\n783sAuBt59xT4xUrsG/Sz70C3sTZDeT+lTIf+NNYZcwsCjQRjsdbh227c26fc24gePnPwIpJqlsl\nKOZnI5Scc13Dj3Occ78EYmY2s8zVmjBmFsOHmx855/5vgSKhPveHa3/Yzz+Acy4NbALOyzsU1uv9\niLHaHvLr/RrgQjN7Hd8dqdXM/i2vTEWcewW8ibMNWGxmi8wsju9YuSGvzAbg08H2xUC7C3phTnGH\nbXtev6ML8f11qsUG4FPBiMozgE7n3J5yV2oymNnRw31PzGwV/pqzr7y1mhhBu+4Adjrn/m6MYqE9\n98W0P6zn38xmmVky2K4D/hPw27xiobzeF9P2MF/vnXM3OOfmO+eOwf+ua3fOfSKvWEWc++hkf8Ow\ncs4NmdlfAg/iR5Xe6Zx70cy+AWx3zm3AXwzvNrNX8Gn+svLVeOIU2fZrzexCYAjf9ivKVuEJZmb/\nBz9acKaZ7Qb+J77jMc657wO/xI+mfAU4AFxZnppOvCLafjFwtZkNAX3AZWH4JRdYA3wSeD7ojwRw\nI7AAwn/uKa79YT3/c4EfmlkNPrT+xDn3QDVc7ymu7aG93o+lEs+9VrIQERERCRk9ohUREREJGQU8\nERERkZBRwBMREREJGQU8ERERkZBRwBMREREJGQU8ERERkZBRwBMROQLB5MW6hopIRdLFSUSkSGZ2\njJntNLPv4ddZzeQcu9jMfhBs/8DMvmtmj5nZq2Z2cbB/rpltNrMdZvaCmX24LA0RkdBTwBMReW8+\nCNzlnFsO9I5Tbi6wFrgA+Faw73LgwWCh9mXAjjHeKyLyvmipMhGR9+aPzrkniih3v3MuC7xkZnOC\nfduAO80sFhxXwBORktAdPBGR9yb3rl3uWo+1eeUGcrYNwDm3GVgHvIlfq/JTJamhiFQ9BTwRkSP3\nZzNbEgy2uOhwhc1sIfC2c+6f8QuSn1rqCopIddIjWhGRI/dV4AFgF/ACMP0w5c8GvmJmB4EeQHfw\nRKQkzDl3+FIiIiIiMmXoEa2IiIhIyCjgiYiIiISMAp6IiIhIyCjgiYiIiISMAp6IiIhIyCjgiYiI\niISMAp6IiIhIyCjgiYiIiITM/wepOOhgY/V4egAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111749110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "_________________________________________________________________________________________\n",
      "\n",
      "_______The Average of Training MSE of 5 folds cross validation is: 0.000454920006795\n",
      "\n",
      "_______The Average of Validation MSE of 5 folds cross validation is: 0.00122352506108\n",
      "\n",
      "\n",
      "__________________________________Ridge Regularization____________________________________________\n",
      "\n",
      "_________________________Model Training for run 0 with lambda:0_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0 is: \n",
      " [  2.59732623e-01   1.08188670e-05  -5.52567148e-02  -4.48610369e-03\n",
      "   7.97505991e-03   2.04620308e-02  -1.48142532e-02  -1.33152558e-03\n",
      "   8.35788707e-04  -7.39256538e-02   4.13051404e-02  -1.43733305e-04\n",
      "   6.68045740e-02  -3.18884239e-03  -8.82422330e-02   8.78651170e-03\n",
      "   1.89991685e-03   3.79845118e-03  -4.00651668e-02   1.10968011e-02\n",
      "   9.24783712e-04   6.19639339e-02   2.10208842e-03   9.03627743e-03\n",
      "   2.73033233e-04   1.23699437e-03   1.78056309e-03   6.50180799e-05\n",
      "  -1.31985899e-04  -6.43255482e-02  -2.32888043e-02  -9.26040896e-03\n",
      "   5.77550734e-03  -2.52515364e-02  -1.63077471e-02  -1.88835292e-02\n",
      "   7.21657344e-03   9.58284495e-03  -1.55741743e-03   1.06218077e-02\n",
      "  -5.42670377e-02   4.12926265e-05  -1.25810571e-02   4.31940211e-02\n",
      "   3.86427825e-02  -2.80618359e-02   6.57334110e-03   1.82666233e-02\n",
      "   1.64159294e-03   8.37605575e-03  -8.19355174e-03   1.48704341e-01\n",
      "   3.23089480e-04  -9.10201114e-02  -1.03397668e-03   2.34890280e-03\n",
      "  -5.75799084e-03   1.59287697e-04  -4.31106104e-02   1.76154543e-02\n",
      "   4.39563501e-02   4.90314620e-04  -6.98308161e-03  -3.41432634e-02\n",
      "  -1.18875710e-02   5.45467512e-03  -4.69461987e-02   1.77976945e-02\n",
      "  -5.33100539e-03  -7.81973062e-02   3.54594338e-02  -9.72210454e-03\n",
      "  -7.27044978e-03   1.55198926e-03   4.16455002e-03   7.39039847e-02\n",
      "   6.27643855e-03   9.96607642e-05  -1.31911378e-03   1.48835589e-02\n",
      "  -7.87781257e-03   5.21372419e-02  -6.31543944e-02   1.89436278e-02\n",
      "   1.64473389e-02   8.36894671e-03  -1.38033895e-02  -1.39432986e-02\n",
      "   6.10428526e-04   1.04215686e-02   4.49849604e-03  -3.84492115e-02\n",
      "   9.70615383e-03   1.78300982e-02  -2.05269453e-03  -9.86899583e-04\n",
      "  -1.36026960e-03  -2.41442661e-04  -1.14611170e+00   1.48352998e+00\n",
      "  -2.65651930e-02   1.89281140e-01  -2.87887504e-01   1.51861730e-01\n",
      "   1.19131940e-02  -8.44358315e-01   3.37697996e-02  -2.28709343e-01\n",
      "  -1.90595455e-02   5.52346587e-02   7.14356819e-02  -2.50068828e-01\n",
      "  -2.08863912e-01  -2.07470438e-03   2.20110960e-02   1.56062073e-02\n",
      "   1.04767852e-04  -3.97098465e-03   4.88064692e-02   1.52024217e+00\n",
      "  -3.53212161e-02   9.96142925e-03] \n",
      "After Regularization with lambda 0:\n",
      " Train MSE:0.000419789189954, Valid MSE:0.0010956602285\n",
      "\n",
      "_________________________Model Training for run 1 with lambda:0_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0 is: \n",
      " [  6.23557586e-01  -9.30812371e-05   2.70583398e-02   7.85974976e-03\n",
      "   6.19132185e-03   1.51803366e-02  -1.41102980e-02  -7.39805836e-03\n",
      "  -1.06533817e-03  -4.19784086e-02   2.17380784e-02  -1.08739510e-02\n",
      "   2.38027433e-02  -3.05141306e-03  -5.84604826e-02  -2.11975344e-04\n",
      "   1.20916707e-03  -1.96691570e-03   3.87730343e-03   2.42097439e-03\n",
      "  -2.97961558e-03   7.79649446e-02   1.74314449e-02  -1.17015004e-02\n",
      "   1.04072109e-03   2.72183835e-03  -1.61461774e-03  -4.89384418e-04\n",
      "  -2.74072613e-03  -8.62702974e-02   1.24510481e-02  -5.11677702e-03\n",
      "  -1.17899533e-02  -2.84290427e-02  -4.65690100e-03  -4.43834142e-03\n",
      "   3.75683785e-03   3.27627242e-04   1.25606038e-02   1.52206584e-02\n",
      "  -5.42717880e-02   7.73081158e-04  -3.11586391e-02   8.70936392e-02\n",
      "   1.19818851e-02  -2.53395611e-02   3.24944777e-02   1.31535089e-02\n",
      "   4.07415184e-03   4.16799584e-03  -5.18898403e-03   1.86033503e-01\n",
      "   1.45440279e-03  -7.72695540e-02  -7.99871924e-04   1.36164939e-03\n",
      "  -1.01932470e-03  -4.99745855e-03  -1.76061653e-02   1.48979027e-02\n",
      "  -2.21253010e-02   7.24432909e-02  -1.18255893e-02  -3.40829979e-02\n",
      "  -9.38361210e-03   1.39583250e-02  -5.14299794e-02   1.45380941e-02\n",
      "  -5.74319505e-03  -4.00400908e-02   3.61339987e-02  -1.20525557e-02\n",
      "  -4.19535950e-03  -2.22885855e-02   1.78536793e-03   2.94561908e-02\n",
      "   3.58753983e-03   3.90257934e-03   8.74906200e-03  -1.24515019e-03\n",
      "   2.50213591e-03   6.35016137e-02  -6.68905415e-02   3.65976327e-03\n",
      "   1.72501375e-02  -1.41344820e-02  -1.55800121e-02   1.27534866e-02\n",
      "   9.01003898e-04   3.69764529e-03  -1.05208685e-03  -5.11700221e-02\n",
      "  -2.06923563e-03  -1.84432944e-03  -6.37718850e-03   1.30454697e-02\n",
      "  -7.60816338e-03   3.83939389e-03  -1.60246306e+00   2.22356109e+00\n",
      "  -6.34089820e-01  -3.31610930e-01  -1.47632240e-01   1.09163135e-01\n",
      "   4.30924776e-03  -1.01022644e+00   3.64359688e-02  -8.82967370e-02\n",
      "   7.16747347e-02   1.68094314e-01   9.29085871e-02  -2.35262568e-01\n",
      "  -1.49483859e-01   1.43965906e-03   1.57520165e-02  -4.41447095e-03\n",
      "  -8.60115743e-03  -2.41390389e-03   4.86511832e-02   1.18029326e+00\n",
      "   5.27894813e-02   3.93818543e-03] \n",
      "After Regularization with lambda 0:\n",
      " Train MSE:0.000335587567593, Valid MSE:0.0013322577736\n",
      "\n",
      "_________________________Model Training for run 2 with lambda:0_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0 is: \n",
      " [  9.03064839e-01  -3.85465758e-05   7.44650928e-03  -4.29465651e-03\n",
      "   1.21436429e-02   2.62930116e-02  -1.77357333e-02  -1.23906267e-02\n",
      "   1.43193630e-03  -1.00563215e-01   4.93465075e-02   5.78643851e-04\n",
      "   4.24291695e-02  -3.08523484e-03  -8.90989399e-02   1.26766396e-02\n",
      "  -5.47656622e-04   3.90632736e-03  -2.59812740e-02   1.47260082e-02\n",
      "  -1.46893401e-04   7.81483720e-02  -1.72598825e-02   1.82104651e-02\n",
      "   1.74936108e-03   1.39809310e-03   1.95156321e-03  -2.58967464e-04\n",
      "  -1.88988305e-03  -8.25347821e-02  -1.31167302e-02  -1.71064758e-03\n",
      "  -6.10715316e-05  -2.30567398e-02  -7.98996364e-03  -8.12821228e-03\n",
      "   7.93203053e-03   7.31540337e-03  -3.41217864e-03   3.50288845e-03\n",
      "  -2.99757990e-02   9.07668177e-03   8.48491032e-03   1.53735737e-02\n",
      "  -7.09521831e-04  -5.92323521e-02   5.86691086e-02   2.69148250e-02\n",
      "   8.15187104e-03   7.53605395e-03  -4.05221768e-03   2.09178901e-01\n",
      "   4.88528250e-03  -7.47276868e-02  -1.74223186e-03   2.47841984e-03\n",
      "  -7.92292551e-03   6.65559803e-03  -1.67589966e-02  -2.57349100e-02\n",
      "   7.95637386e-02  -8.71499701e-03  -4.33308992e-03  -2.83648522e-02\n",
      "  -1.77943374e-02   2.82121203e-02  -2.45639093e-02   2.08127831e-02\n",
      "  -1.09826388e-02  -8.49619455e-02   4.14722477e-02  -1.40951527e-02\n",
      "  -6.17212062e-03  -2.79558959e-02  -2.62068952e-03   7.54368495e-02\n",
      "   6.93684460e-03   3.48790583e-03   3.53133831e-03   1.71971563e-02\n",
      "  -5.32966098e-03   3.87078847e-02  -3.17655549e-02   2.19782988e-03\n",
      "   2.17140319e-02   8.39136865e-03  -1.50718985e-02  -9.91503138e-03\n",
      "   3.15880887e-03  -3.49210557e-03   5.07546144e-03  -5.34879067e-02\n",
      "  -3.47167209e-03   8.73482572e-03  -1.01667229e-02  -8.93483467e-03\n",
      "  -3.10105974e-03   1.36334071e-02  -1.92508856e+00   2.70816544e-01\n",
      "  -7.60543966e-01   3.87233838e-01  -7.66256187e-02  -7.57565784e-03\n",
      "   3.73179082e-02   3.01014551e-01   4.85113985e-02  -1.77978467e-01\n",
      "  -1.05739785e-02   9.17347371e-02   8.64922848e-02  -2.43763061e-01\n",
      "  -4.08151471e-02  -1.08718292e-02   2.30062672e-02  -4.18428919e-03\n",
      "  -1.80501431e-03  -3.74507854e-03   6.13637328e-02   1.20326165e+00\n",
      "  -7.05240964e-02   1.17448543e-02] \n",
      "After Regularization with lambda 0:\n",
      " Train MSE:0.00052609856696, Valid MSE:0.000435530602423\n",
      "\n",
      "_________________________Model Training for run 3 with lambda:0_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0 is: \n",
      " [  1.82511132e+00  -5.77332171e-05   8.09258747e-02  -1.10708562e-03\n",
      "   1.94150391e-02   3.63049769e-02  -1.08405785e-02  -4.94907523e-03\n",
      "  -1.00177804e-03  -7.35005345e-02   3.53601090e-02  -3.39780562e-03\n",
      "  -3.84683606e-02  -1.01772407e-03  -8.50625851e-02   8.13717576e-03\n",
      "   6.74931102e-04  -9.02861238e-04  -3.36269473e-02   1.40846394e-02\n",
      "   7.43181616e-03   7.32760683e-02  -3.83572437e-02   4.13619781e-02\n",
      "   9.51322285e-04   4.56122028e-03   1.94266325e-03  -3.31083857e-04\n",
      "  -4.30365764e-03  -2.24613036e-02  -1.82880200e-02  -2.81460039e-03\n",
      "  -1.23876887e-03  -3.72926476e-02  -1.03278394e-02  -6.55277694e-03\n",
      "   9.93296387e-03   5.63527633e-03  -2.55226136e-04   1.60867420e-02\n",
      "  -7.45901604e-02   1.18129948e-04  -3.82691656e-02   8.20272289e-02\n",
      "   1.09827831e-02  -6.22663564e-02   5.02887523e-02   2.08827511e-02\n",
      "   9.54747659e-03   6.04328708e-03  -2.02265372e-03   1.51499693e-01\n",
      "   4.88519241e-03  -1.18439377e-01  -2.83743620e-03   2.93836106e-03\n",
      "  -1.42144712e-02   1.00963846e-02  -3.96630177e-02  -2.90170482e-03\n",
      "   1.18473879e-01  -3.62241166e-02  -8.86521379e-03  -3.81621141e-02\n",
      "  -6.18620893e-02   6.42042096e-02  -7.47066931e-02   5.26676117e-02\n",
      "  -2.04718121e-02  -1.52827719e-01   5.68527988e-02  -7.05828588e-03\n",
      "  -4.68580403e-03  -5.79220409e-02  -9.39512212e-03   1.53342796e-01\n",
      "   1.04423127e-02   1.20450470e-03   1.36882952e-03   1.32581101e-02\n",
      "  -6.02484828e-03   4.07879233e-02  -5.96746541e-02   2.73195946e-02\n",
      "   2.03053913e-02   2.91419580e-03  -1.45709239e-02  -3.41254869e-03\n",
      "   6.97177242e-04   3.48521484e-03   2.43962228e-03  -2.45759394e-02\n",
      "  -3.17182580e-03  -1.86270840e-03  -9.09781774e-03  -1.82046092e-03\n",
      "  -8.36245172e-03   1.25692616e-02  -2.92479851e+00   9.82573941e-01\n",
      "  -1.67954025e+00   1.27486434e-01  -9.53503861e-02   9.98370937e-02\n",
      "   5.90395517e-03  -1.92583538e-01   4.79157590e-02  -1.86129965e-01\n",
      "  -7.27800318e-02   2.77654899e-02   6.08032586e-02  -1.38059583e-01\n",
      "  -2.99818966e-01   1.72680518e-02   2.04115706e-02  -2.93909550e-03\n",
      "   2.23146549e-03  -3.43811608e-03   6.04602456e-02   1.56034645e+00\n",
      "  -5.96174416e-02   6.21450522e-03] \n",
      "After Regularization with lambda 0:\n",
      " Train MSE:0.000505915462904, Valid MSE:0.000625730530825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 4 with lambda:0_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0 is: \n",
      " [  1.38610598e+00  -1.13809733e-04   2.65799989e-03  -1.92964304e-02\n",
      "   1.06871567e-02   1.31830255e-02  -8.54447674e-03  -6.07444607e-03\n",
      "  -2.85190066e-02  -4.73156266e-02   4.18033234e-02  -1.34607172e-02\n",
      "   8.26658145e-02  -6.47349253e-03  -3.66595327e-02   1.02418533e-03\n",
      "   1.68063826e-03   1.18958569e-02  -2.54191520e-02   4.20571388e-04\n",
      "   2.29975570e-03   2.43063727e-02   1.94551275e-02  -2.07564404e-02\n",
      "  -1.61931705e-03   3.26330518e-03  -3.86038640e-04  -8.85498217e-05\n",
      "  -6.21700989e-03  -1.05268221e-01  -1.39147119e-02   2.36670738e-04\n",
      "  -1.06935519e-02  -2.75318112e-02  -6.38926109e-03  -2.20618248e-02\n",
      "   6.85581273e-03   7.34435792e-03  -2.90344371e-03   7.17138543e-03\n",
      "  -6.48421898e-02   7.76120063e-03  -2.89212714e-02   6.83654603e-02\n",
      "   9.08791401e-03  -2.86839891e-02   1.14148742e-02   1.48161590e-02\n",
      "   2.76080971e-03   5.02935635e-03  -3.05981951e-04   1.92539822e-01\n",
      "  -6.57200353e-03  -1.12577311e-01  -6.27813227e-03   3.10969826e-03\n",
      "  -7.68753238e-03   1.13388735e-02  -1.19075158e-02  -5.77975558e-03\n",
      "   6.05909229e-02  -3.20926018e-02   3.30817022e-03  -9.47198178e-03\n",
      "  -2.47087384e-02   2.49234815e-02  -3.96136854e-02   5.55402688e-02\n",
      "  -1.84345196e-02  -1.54164024e-01   3.78061317e-02  -2.58393935e-03\n",
      "  -4.31462419e-03  -1.70549855e-02  -1.07544668e-03   1.64528994e-01\n",
      "   9.16978489e-03  -1.32732264e-03  -8.26652692e-05   8.77263834e-03\n",
      "  -5.01083585e-03   1.10171178e-02  -5.61045589e-02   5.51108781e-02\n",
      "   1.58038209e-02  -3.51047360e-03  -7.61848242e-03  -5.66842779e-03\n",
      "   1.56561667e-03  -6.02076248e-04   6.05848307e-03  -6.30716955e-02\n",
      "   1.49186472e-03   7.20410539e-03  -4.77678733e-03  -1.51007808e-02\n",
      "   4.30394873e-03   6.84349075e-03  -2.37591733e+00  -8.81039186e+01\n",
      "  -1.22457562e+00   2.33418038e-01  -2.25709422e-01   4.96199629e-03\n",
      "   6.59618080e-02   8.88342929e+01   2.53147228e-02  -1.61575437e-01\n",
      "  -3.28276062e-02   6.69169395e-02   4.95914757e-02  -1.83791083e-01\n",
      "  -1.72384095e-01  -2.09397638e-03   1.64552707e-02  -5.54903957e-03\n",
      "   2.86539721e-03  -5.35877105e-04   7.59785810e-02   1.46800978e+00\n",
      "  -6.17981409e-02   6.64003665e-03] \n",
      "After Regularization with lambda 0:\n",
      " Train MSE:0.000487209246567, Valid MSE:0.00262845253556\n",
      "Average validation error over 5 runs 0.00122352569763 with lambda:0\n",
      "Average training error over 5 runs 0.000454920006795 with lambda:0\n",
      "\n",
      "_________________________Model Training for run 0 with lambda:0.1_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.1 is: \n",
      " [ -1.16223020e-04  -5.17025428e-05  -1.30314747e-02  -7.68642527e-03\n",
      "   1.30360795e-02   2.04454125e-02  -1.35816170e-02  -2.34424766e-04\n",
      "   1.52019272e-02  -5.68315613e-02   1.10580329e-02  -6.74237011e-03\n",
      "   2.31452701e-02  -2.04952834e-03  -7.37662206e-02   6.79228291e-03\n",
      "   2.64286084e-03   7.60523394e-03  -3.28060296e-02   1.65503999e-02\n",
      "  -1.85373846e-03   5.57744063e-02   3.61273308e-02  -2.56978940e-02\n",
      "  -1.17431485e-03   1.50311626e-03   7.03039022e-04  -4.71293071e-04\n",
      "  -6.29811429e-04  -2.33442422e-02  -3.03323618e-02  -7.99245291e-03\n",
      "   8.39806663e-04  -2.78303397e-02  -1.77788066e-02  -1.75596331e-02\n",
      "   6.29145634e-03   1.27735306e-02  -6.59592135e-04   7.42793020e-03\n",
      "  -4.61952604e-02   5.58458563e-03  -4.15927668e-03   2.22501406e-02\n",
      "   2.28631363e-02  -3.43207606e-02   1.49225418e-02   2.11377821e-02\n",
      "   3.80089697e-03   5.06813836e-03  -7.57992396e-03   6.07340189e-02\n",
      "   9.01993612e-03  -4.68584756e-02  -3.99194424e-03   4.49778693e-03\n",
      "  -2.78329207e-03  -4.29885968e-03  -3.75933820e-02   2.95759311e-03\n",
      "   3.68715261e-02   2.06414822e-02  -1.52498482e-03  -2.99342233e-02\n",
      "   2.19842599e-02  -2.81851487e-02  -2.60839482e-02  -2.85849046e-03\n",
      "   1.98819966e-03  -2.60941263e-02   3.49086930e-02  -8.10105405e-03\n",
      "  -6.30864760e-03   2.51223806e-03   4.34198028e-03   1.67937744e-02\n",
      "   5.69735657e-03  -9.11625822e-04  -3.11466342e-03   1.92171698e-02\n",
      "  -6.74955920e-03   3.21213370e-02  -2.15946674e-02  -3.66456803e-03\n",
      "   1.27032677e-02  -1.77349606e-03  -1.40196456e-02  -3.54950720e-03\n",
      "   3.90990030e-03   9.63905220e-03   4.71200078e-03  -4.22362041e-02\n",
      "   2.43812924e-02   1.38433030e-02  -7.43048929e-03  -1.88482504e-03\n",
      "  -5.72972529e-04   3.87498200e-03  -2.97375511e-01   3.00663131e-01\n",
      "   2.38760998e-01   1.89910008e-01  -1.68206807e-01   1.85350234e-01\n",
      "  -3.02138256e-02   3.00267253e-01   2.23981012e-02  -2.21094310e-01\n",
      "  -4.27100602e-02   2.27569301e-02   9.31570818e-02  -2.19056072e-01\n",
      "  -2.13801550e-01  -3.13563433e-03   2.98441325e-02   7.34076730e-03\n",
      "  -3.54109555e-03  -4.54791429e-03   5.07293259e-02   8.11333918e-01\n",
      "  -2.48717799e-02   1.23153124e-02] \n",
      "After Regularization with lambda 0.1:\n",
      " Train MSE:0.129204708965, Valid MSE:0.129703848892\n",
      "\n",
      "_________________________Model Training for run 1 with lambda:0.1_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.1 is: \n",
      " [ -8.82767931e-02  -6.71582897e-05   5.26226966e-03   7.06492229e-03\n",
      "   1.32459381e-02   1.82215739e-02  -1.01028049e-02  -1.11309560e-02\n",
      "   1.01258980e-03  -3.18495705e-02   1.04475284e-02  -1.72259370e-02\n",
      "   2.12633247e-02  -3.77181629e-03  -4.06991267e-02  -8.17347973e-03\n",
      "   1.20630424e-03  -5.19467870e-03   3.25036920e-03   4.02782445e-03\n",
      "  -4.46549129e-03   6.86558657e-02   7.71975180e-04  -1.18643602e-03\n",
      "   5.29150939e-04   2.54805075e-03  -1.55200655e-03  -8.73471445e-05\n",
      "  -1.93484416e-03  -5.15408623e-02   5.10652972e-03  -3.87959595e-03\n",
      "  -1.38955514e-02  -2.98864040e-02  -7.98732567e-03  -5.67500585e-03\n",
      "   3.68460284e-03   2.66550436e-03   1.39163138e-02   1.60807559e-02\n",
      "  -3.01117447e-02   3.87342926e-03  -4.75716991e-03   3.68378095e-02\n",
      "   5.23126010e-03  -3.10653693e-02   3.95488191e-02   1.39111327e-02\n",
      "   6.20054724e-03   1.49169613e-03  -5.18770194e-03   1.02789080e-01\n",
      "   3.78458520e-03  -6.13992514e-02  -1.70649461e-03   3.04678942e-03\n",
      "  -6.01813953e-04  -5.32118182e-03  -1.43897341e-02   1.05517910e-02\n",
      "  -8.90255363e-03   5.44699671e-02  -1.48224183e-02  -2.63317470e-02\n",
      "  -6.43010982e-03   6.60397789e-03  -3.90783605e-02   5.21086172e-03\n",
      "  -4.17748381e-03  -1.06504145e-02   4.29720026e-02  -8.74246890e-03\n",
      "  -3.24518233e-03  -1.28179968e-03   6.35143427e-03   1.83722692e-03\n",
      "   1.71532363e-03   3.47809693e-03   6.94943441e-03  -6.06785773e-04\n",
      "   1.90987273e-03   4.12386382e-02  -5.00520576e-02   4.78675921e-03\n",
      "   1.85672801e-02  -1.55094342e-02  -1.78287670e-02   1.41743102e-02\n",
      "   6.12566075e-03   3.47918180e-03  -1.76952732e-03  -2.58665435e-02\n",
      "   1.24026827e-02  -8.60834800e-03  -1.24363189e-02   1.09023964e-02\n",
      "  -3.72525247e-03   8.04501184e-03  -2.60151384e-01   4.36713884e-01\n",
      "   9.19325153e-02   4.39043756e-02  -1.12079318e-01   8.08605211e-02\n",
      "   1.54607751e-02   4.36913210e-01   3.29996424e-02  -8.91129913e-02\n",
      "   9.95979468e-03   9.58251859e-02   1.18031757e-01  -1.57374223e-01\n",
      "  -2.15182679e-01   5.76785550e-03   2.76171736e-02  -5.34823961e-03\n",
      "  -8.26015422e-03  -1.63196601e-03   3.89585828e-02   5.68324979e-01\n",
      "   2.21746920e-02   6.02261986e-03] \n",
      "After Regularization with lambda 0.1:\n",
      " Train MSE:0.0967679147616, Valid MSE:0.097827345141\n",
      "\n",
      "_________________________Model Training for run 2 with lambda:0.1_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.1 is: \n",
      " [ -2.23074383e-02   1.20948812e-05   8.63082413e-03  -6.46168004e-03\n",
      "   1.09196099e-02   2.19554397e-02  -1.65191166e-02  -1.31167496e-02\n",
      "   1.38686731e-02  -7.85717764e-02   1.70457123e-02  -1.41462820e-02\n",
      "   2.64767582e-02  -3.31409571e-03  -6.41224453e-02   7.21323815e-03\n",
      "  -4.88712008e-04   3.46133075e-03  -1.66981700e-02   1.75351328e-02\n",
      "  -4.01465773e-03   5.71022118e-02   7.54429121e-03  -2.50161472e-03\n",
      "  -2.97302203e-05   2.79794974e-03   1.49595912e-03  -7.92397792e-04\n",
      "  -1.86990237e-03  -5.38403269e-02  -2.15818975e-02  -3.43407828e-03\n",
      "  -1.68889523e-03  -2.58547352e-02  -1.26007315e-02  -1.22732929e-02\n",
      "   7.46007069e-03   1.03881241e-02  -3.83283186e-03   1.44179139e-04\n",
      "  -3.42084387e-02   1.73638545e-02   5.99120939e-03   1.65628785e-02\n",
      "  -1.16265552e-02  -5.37268027e-02   5.70810289e-02   2.71423112e-02\n",
      "   9.64386574e-03   5.26325292e-03  -4.53748063e-03   1.18568227e-01\n",
      "   1.26897752e-02  -4.58417359e-02  -3.37379204e-03   2.25387895e-03\n",
      "  -1.86963963e-03   1.05573952e-03  -9.76486414e-03  -2.48963402e-02\n",
      "   3.98788585e-02   2.18175892e-02  -4.38784696e-03  -2.66927116e-02\n",
      "   9.16322489e-03   3.61679705e-03  -3.64322457e-03  -8.92601293e-03\n",
      "  -2.85914012e-03  -2.53777920e-02   3.48731762e-02  -1.36375310e-02\n",
      "  -5.95675846e-03  -6.69591306e-03   1.69026214e-03   8.39798192e-03\n",
      "   4.71408808e-03   2.48681796e-03   3.99759798e-03   2.12793583e-02\n",
      "  -5.42525302e-03   2.63848138e-02  -1.03135565e-02  -6.63658986e-03\n",
      "   1.96797445e-02  -5.14402274e-03  -1.72173341e-02   1.88350561e-03\n",
      "   7.10803257e-03  -4.64232234e-03   5.78863493e-03  -4.12969636e-02\n",
      "   1.12647677e-02   9.20684080e-03  -1.50665436e-02  -8.02839350e-03\n",
      "  -1.16952887e-03   1.74041126e-02  -4.34432696e-01   2.98375238e-01\n",
      "   1.67526471e-01   3.31652455e-01  -2.09681035e-02   4.49614535e-03\n",
      "   2.40396782e-02   2.99506566e-01   3.29702790e-02  -1.66088676e-01\n",
      "  -3.16706503e-02   5.67544186e-02   1.02990576e-01  -2.08914891e-01\n",
      "  -1.07381144e-01  -1.11203423e-02   3.14260043e-02  -7.94381475e-03\n",
      "  -4.65034044e-03  -2.64584282e-03   4.53933970e-02   6.29804853e-01\n",
      "  -2.65094418e-02   1.40265333e-02] \n",
      "After Regularization with lambda 0.1:\n",
      " Train MSE:0.106964256884, Valid MSE:0.106887119373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 3 with lambda:0.1_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.1 is: \n",
      " [  2.10399874e-02  -8.53066472e-05   2.42511715e-02  -3.79165842e-03\n",
      "   1.28869335e-02   2.39725003e-02  -1.05988502e-02  -1.11386685e-02\n",
      "   8.37719577e-03  -5.35155395e-02   6.24501461e-03  -2.58084711e-02\n",
      "   1.76583097e-02  -3.85372351e-03  -7.44828073e-02   6.90268716e-03\n",
      "   9.36511183e-04   3.82353415e-03  -1.50622512e-02   1.49309389e-02\n",
      "   4.38509470e-03   5.75681748e-02   9.84196358e-03   3.93704167e-03\n",
      "  -1.53849951e-03   5.88078664e-03   1.38377409e-03  -6.58926745e-04\n",
      "  -4.95868107e-03  -9.89346345e-03  -2.42653910e-02  -7.95034703e-04\n",
      "  -8.30304169e-03  -3.48416305e-02  -1.51981813e-02  -1.16599672e-02\n",
      "   1.08463300e-02   1.08386966e-02  -1.10641138e-03   1.08128181e-02\n",
      "  -5.50700137e-02   5.56333981e-03  -9.31923020e-03   3.26121504e-02\n",
      "  -2.60032144e-03  -5.18142355e-02   3.48537631e-02   2.42386818e-02\n",
      "   1.04429577e-02   3.30993348e-03  -4.55352847e-03   5.20752534e-02\n",
      "   1.09065172e-02  -7.32180839e-02  -6.52823916e-03   3.45296464e-03\n",
      "  -2.70297170e-03   8.73291728e-04  -2.56148715e-02  -1.30511619e-02\n",
      "   6.74935356e-02   7.96413545e-03  -5.00311256e-03  -2.93568758e-02\n",
      "  -6.80269229e-03   1.11831784e-02  -2.90443814e-02   2.61544889e-03\n",
      "  -2.64998734e-03  -3.60933509e-02   4.73633520e-02  -3.89775811e-03\n",
      "  -4.83676458e-03  -2.85630316e-02  -3.59848591e-03   3.10414825e-02\n",
      "   9.58322565e-03  -1.23795284e-03   3.82827605e-03   1.63663516e-02\n",
      "  -5.00187833e-03   3.24873867e-02  -1.22110375e-02  -1.00617974e-02\n",
      "   1.66803130e-02  -1.09381116e-02  -1.52601216e-02   6.31383056e-03\n",
      "   4.38330493e-03   1.39595198e-03   6.03985205e-03  -1.87646020e-02\n",
      "   4.16913159e-03  -2.82991397e-03  -1.36831744e-02  -4.72657858e-03\n",
      "  -3.52489828e-03   1.74616662e-02  -4.38013694e-01   3.42987241e-01\n",
      "   1.23289365e-01   2.39349391e-01  -2.98172203e-02   6.51480253e-02\n",
      "   1.55350348e-02   3.43425115e-01   2.73815935e-02  -1.76436980e-01\n",
      "  -5.37214682e-02   3.40245065e-02   8.56919350e-02  -1.62295634e-01\n",
      "  -2.46838866e-01   1.01925105e-02   2.19817881e-02  -4.56180894e-03\n",
      "  -1.18086783e-03  -1.28552898e-03   4.35943994e-02   7.31816458e-01\n",
      "  -5.54092070e-03   9.09883880e-03] \n",
      "After Regularization with lambda 0.1:\n",
      " Train MSE:0.12314857694, Valid MSE:0.122867141236\n",
      "\n",
      "_________________________Model Training for run 4 with lambda:0.1_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.1 is: \n",
      " [ -1.38235421e-02  -1.49444857e-04   1.44071055e-02  -1.99969501e-02\n",
      "   1.50552050e-02   1.65228560e-02  -8.03549292e-03  -8.84616307e-03\n",
      "  -1.22815951e-02  -3.73904959e-02   1.79315125e-02  -2.46395361e-02\n",
      "   3.84795462e-02  -5.65424904e-03  -2.45829764e-02   6.22865974e-04\n",
      "   1.63394838e-03   1.43549573e-02  -1.70094829e-02   5.91795243e-03\n",
      "   2.11230782e-03   1.85348513e-02   1.76309236e-02  -1.56635841e-02\n",
      "  -3.24781638e-03   4.79701488e-03  -4.22926544e-04  -9.84294403e-04\n",
      "  -5.93417395e-03  -6.41943986e-02  -2.32650503e-02  -1.55559000e-03\n",
      "  -9.92611304e-03  -2.76917813e-02  -1.09074509e-02  -2.16840297e-02\n",
      "   7.70157023e-03   1.09924783e-02  -4.87366540e-03   1.54637882e-03\n",
      "  -4.50001131e-02   1.11147838e-02  -4.01732499e-03   2.30500098e-02\n",
      "  -3.80929361e-04  -3.27656089e-02   1.87583736e-02   1.61197440e-02\n",
      "   5.65906308e-03   2.89974467e-03  -1.97188806e-03   7.39876556e-02\n",
      "   8.97893962e-04  -5.95811435e-02  -9.83667771e-03   5.62105328e-03\n",
      "  -2.20325904e-03   6.05800692e-03  -2.16077604e-03  -1.23163448e-02\n",
      "   2.71899243e-02   6.28895523e-04   2.54770785e-03  -7.55480806e-03\n",
      "   6.80022289e-03  -9.69217732e-03  -1.09313579e-02   1.32903238e-02\n",
      "  -7.57461418e-04  -4.66461526e-02   4.00215297e-02   2.11850172e-04\n",
      "  -3.83354783e-03  -3.74278010e-03   2.70977054e-03   5.05714888e-02\n",
      "   8.75102880e-03  -1.95463332e-03   1.64960589e-03   1.27572192e-02\n",
      "  -3.92608219e-03   2.91880058e-04  -2.83374315e-02   3.38875760e-02\n",
      "   1.26838970e-02  -7.44870560e-03  -9.11086092e-03  -1.60623893e-03\n",
      "   6.09551457e-03   1.60634158e-03   7.23747827e-03  -3.54271520e-02\n",
      "   1.24363866e-02   2.12216976e-03  -9.77279373e-03  -1.51112158e-02\n",
      "   6.35785103e-03   1.15753256e-02  -4.00266238e-01   3.53806886e-01\n",
      "   1.50966909e-01   2.34097777e-01  -6.88564458e-02   1.77664946e-02\n",
      "   3.68913353e-02   3.53776235e-01   1.40946228e-02  -1.51245295e-01\n",
      "  -4.25259105e-02   3.53485989e-02   7.58503463e-02  -1.57022395e-01\n",
      "  -1.89713290e-01   3.28627375e-03   3.33783661e-02  -1.43962086e-02\n",
      "   3.19725499e-04   7.90986572e-04   7.40113410e-02   6.75446917e-01\n",
      "  -1.75378248e-02   6.57828058e-03] \n",
      "After Regularization with lambda 0.1:\n",
      " Train MSE:0.109585166145, Valid MSE:0.109588854465\n",
      "Average validation error over 5 runs 0.0386073044055 with lambda:0.1\n",
      "Average training error over 5 runs 0.0380146549176 with lambda:0.1\n",
      "\n",
      "_________________________Model Training for run 0 with lambda:0.2_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.2 is: \n",
      " [  2.64861532e-02  -7.44190425e-05  -6.42258795e-03  -9.43365897e-03\n",
      "   1.53850073e-02   2.29273458e-02  -1.28173466e-02   1.57851507e-03\n",
      "   1.97710963e-02  -4.71540170e-02   1.46012232e-04  -5.46027677e-03\n",
      "   1.55805157e-02  -1.96390301e-03  -6.07693790e-02   7.00865266e-03\n",
      "   2.97781257e-03   9.79785077e-03  -2.95975225e-02   1.91581839e-02\n",
      "  -1.91241181e-03   4.88088056e-02   3.56314039e-02  -2.68424718e-02\n",
      "  -1.05088985e-03   1.56554516e-03   7.43864777e-04  -6.61917009e-04\n",
      "  -8.52593239e-04  -1.10030074e-02  -3.07355500e-02  -7.62540594e-03\n",
      "  -4.14527832e-04  -2.72266190e-02  -1.75206980e-02  -1.55385887e-02\n",
      "   5.98925603e-03   1.37065546e-02  -1.80418940e-04   6.06829770e-03\n",
      "  -4.13018971e-02   7.99878647e-03   6.02378593e-04   1.45109041e-02\n",
      "   1.74871763e-02  -3.21710674e-02   1.68274528e-02   2.20683319e-02\n",
      "   4.34644058e-03   4.15489412e-03  -7.25554420e-03   3.90330227e-02\n",
      "   1.19669461e-02  -3.08795412e-02  -5.05626594e-03   4.30841651e-03\n",
      "  -1.70041579e-03  -5.00703775e-03  -3.38010977e-02   2.87500147e-05\n",
      "   3.42281941e-02   2.41364696e-02   2.17135104e-03  -2.86986117e-02\n",
      "   2.21204103e-02  -2.77658255e-02  -2.25865928e-02  -4.72976769e-03\n",
      "   3.85851910e-03  -1.76133187e-02   3.41211869e-02  -5.87598613e-03\n",
      "  -5.47226308e-03   1.34008146e-04   3.68304386e-03   6.89121583e-03\n",
      "   5.93651494e-03  -8.66751919e-04  -3.44946982e-03   1.99416335e-02\n",
      "  -6.41117166e-03   2.61422954e-02  -1.14432103e-02  -9.22784657e-03\n",
      "   1.11110759e-02  -3.14643974e-03  -1.45166116e-02  -1.62392384e-03\n",
      "   5.69870489e-03   9.32243198e-03   4.57855589e-03  -4.24890643e-02\n",
      "   2.70929011e-02   1.31808409e-02  -9.15609662e-03  -2.52758689e-03\n",
      "  -2.03564224e-04   5.74862809e-03  -1.79925127e-01   2.89405477e-01\n",
      "   1.90673838e-01   2.00989532e-01  -1.16295830e-01   1.91208269e-01\n",
      "  -4.37076517e-02   2.89206733e-01   1.76738140e-02  -2.06638977e-01\n",
      "  -5.76486218e-02   9.77325184e-03   9.86845008e-02  -1.91145695e-01\n",
      "  -1.91811861e-01  -3.17746803e-03   3.31209778e-02   3.48995990e-03\n",
      "  -4.42756581e-03  -4.56314119e-03   5.48247682e-02   5.70612031e-01\n",
      "  -3.01729938e-02   1.31509061e-02] \n",
      "After Regularization with lambda 0.2:\n",
      " Train MSE:0.165386483803, Valid MSE:0.165812428928\n",
      "\n",
      "_________________________Model Training for run 1 with lambda:0.2_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.2 is: \n",
      " [ -6.55892915e-02  -7.17030163e-05   3.26066692e-04   5.77788734e-03\n",
      "   1.48123736e-02   1.80747999e-02  -9.58635313e-03  -1.10026988e-02\n",
      "   2.16705982e-03  -2.68841674e-02   5.36176144e-03  -1.72492702e-02\n",
      "   1.43762317e-02  -3.91700984e-03  -3.08105844e-02  -1.08568672e-02\n",
      "   1.24127031e-03  -5.87674405e-03   1.42693006e-03   5.00974558e-03\n",
      "  -4.78183126e-03   6.11068344e-02   1.16959985e-03  -1.60782525e-03\n",
      "   1.13580567e-04   2.50680697e-03  -1.38186709e-03   2.18825048e-05\n",
      "  -1.74142083e-03  -3.76734639e-02   2.55561052e-03  -3.80406547e-03\n",
      "  -1.35311872e-02  -2.78715060e-02  -9.15800227e-03  -6.78451348e-03\n",
      "   3.65123230e-03   2.90650935e-03   1.36875989e-02   1.36315254e-02\n",
      "  -2.42388613e-02   5.58422544e-03   1.60176053e-03   2.46883681e-02\n",
      "   1.67804715e-03  -2.78658724e-02   3.69150037e-02   1.41318810e-02\n",
      "   6.33556387e-03   6.86765657e-04  -4.66079525e-03   8.00713067e-02\n",
      "   3.42297342e-03  -5.16963389e-02  -2.00482279e-03   3.33918719e-03\n",
      "  -1.16075610e-03  -4.65465975e-03  -1.31570394e-02   8.23610594e-03\n",
      "  -4.06125624e-04   4.48333983e-02  -1.40710742e-02  -2.52512671e-02\n",
      "  -2.44108793e-03   2.28305687e-03  -3.17801005e-02   2.46708046e-03\n",
      "  -4.94359748e-03  -5.59662558e-03   4.31338042e-02  -7.85669003e-03\n",
      "  -3.11467644e-03   3.29054397e-03   7.22039081e-03  -3.56593783e-03\n",
      "   1.29267574e-03   3.33840659e-03   6.05463257e-03  -9.15886025e-04\n",
      "   1.60723887e-03   3.05553320e-02  -3.73989586e-02   9.60592882e-04\n",
      "   1.79733389e-02  -1.29751477e-02  -1.80249324e-02   1.16672879e-02\n",
      "   7.65204409e-03   3.13739173e-03  -1.68857613e-03  -1.82212920e-02\n",
      "   1.54561368e-02  -8.22245945e-03  -1.37737608e-02   9.33250388e-03\n",
      "  -2.49474126e-03   9.12706173e-03  -1.53552990e-01   3.88229661e-01\n",
      "   8.10332279e-02   1.27536269e-01  -8.82790846e-02   8.97690022e-02\n",
      "   4.55061844e-03   3.88351672e-01   3.10101939e-02  -8.30504682e-02\n",
      "  -9.29360986e-03   7.42938046e-02   1.23370273e-01  -1.28649562e-01\n",
      "  -2.01368120e-01   7.14698703e-03   3.18351820e-02  -6.44636029e-03\n",
      "  -8.39210541e-03  -1.65139413e-03   4.29618538e-02   4.10667470e-01\n",
      "   1.86127253e-03   6.58976872e-03] \n",
      "After Regularization with lambda 0.2:\n",
      " Train MSE:0.13218525802, Valid MSE:0.133271080381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 2 with lambda:0.2_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.2 is: \n",
      " [ -1.55605708e-02   2.75898904e-05   4.10702711e-03  -6.59904707e-03\n",
      "   1.06968007e-02   2.14266641e-02  -1.60260678e-02  -1.24254409e-02\n",
      "   1.82821091e-02  -6.74813312e-02   4.55435923e-03  -1.49917334e-02\n",
      "   1.90257620e-02  -3.40652062e-03  -5.08250528e-02   4.93728175e-03\n",
      "  -3.74064694e-04   3.29564156e-03  -1.49995493e-02   1.83897229e-02\n",
      "  -4.97344468e-03   4.81330147e-02   1.08536404e-02  -5.45946615e-03\n",
      "  -3.31358827e-04   3.05593314e-03   1.70694240e-03  -8.01706113e-04\n",
      "  -1.91905223e-03  -3.90436419e-02  -2.33922671e-02  -3.52135694e-03\n",
      "  -2.56662680e-03  -2.50827790e-02  -1.37951043e-02  -1.24484163e-02\n",
      "   7.22217204e-03   1.06355263e-02  -3.78414074e-03  -1.76765262e-03\n",
      "  -3.07023492e-02   1.97839981e-02   8.88461007e-03   9.96808348e-03\n",
      "  -1.21611357e-02  -4.67189108e-02   5.17398998e-02   2.68867462e-02\n",
      "   9.98528649e-03   4.43346641e-03  -4.26452686e-03   9.39725288e-02\n",
      "   1.44086914e-02  -3.00738026e-02  -3.39372121e-03   1.16080447e-03\n",
      "  -8.30954759e-04   3.17085333e-04  -8.56611405e-03  -2.06851149e-02\n",
      "   2.96003440e-02   2.52675538e-02  -3.19410253e-03  -2.68934765e-02\n",
      "   1.28603957e-02   3.04028427e-04  -4.47713122e-03  -1.28924916e-02\n",
      "  -1.05136620e-03  -1.85666527e-02   3.31999903e-02  -1.34662943e-02\n",
      "  -5.93264788e-03  -1.37161545e-03   3.07766034e-03  -4.73698693e-04\n",
      "   3.89079597e-03   2.01864273e-03   3.81711615e-03   2.13817172e-02\n",
      "  -5.46871617e-03   2.18203803e-02  -6.28372900e-03  -7.30154393e-03\n",
      "   1.86685419e-02  -6.57900387e-03  -1.72465067e-02   2.13173393e-03\n",
      "   8.75941671e-03  -5.00703799e-03   5.97591338e-03  -3.89076304e-02\n",
      "   1.57111349e-02   1.07510002e-02  -1.67445094e-02  -8.13777139e-03\n",
      "  -1.39089383e-04   1.86173242e-02  -2.74578928e-01   2.96264662e-01\n",
      "   1.59222568e-01   3.01220294e-01  -1.25025109e-02   3.21037840e-02\n",
      "   7.37428406e-03   2.96702168e-01   2.88547688e-02  -1.52771010e-01\n",
      "  -4.26672059e-02   4.26573211e-02   1.06600616e-01  -1.84196810e-01\n",
      "  -1.12531121e-01  -9.22194353e-03   3.60110291e-02  -8.77095016e-03\n",
      "  -5.84444747e-03  -2.44272906e-03   4.37316011e-02   4.61245660e-01\n",
      "  -2.98318151e-02   1.40523210e-02] \n",
      "After Regularization with lambda 0.2:\n",
      " Train MSE:0.143386328432, Valid MSE:0.143316780073\n",
      "\n",
      "_________________________Model Training for run 3 with lambda:0.2_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.2 is: \n",
      " [  5.12244541e-03  -6.76389040e-05   1.33256829e-02  -4.88020073e-03\n",
      "   1.26436198e-02   2.21977245e-02  -1.06570786e-02  -1.06190178e-02\n",
      "   1.19341648e-02  -4.50859536e-02  -3.35172245e-03  -2.60861079e-02\n",
      "   1.45408210e-02  -4.05101322e-03  -6.16856281e-02   6.01137021e-03\n",
      "   1.15471284e-03   4.18237075e-03  -1.24898261e-02   1.65102459e-02\n",
      "   3.67858218e-03   5.04453708e-02   1.59628201e-02  -3.66502048e-03\n",
      "  -2.07672744e-03   5.94988499e-03   1.61007428e-03  -6.77335697e-04\n",
      "  -5.10121258e-03  -2.51356257e-03  -2.47193930e-02  -1.11560623e-03\n",
      "  -8.77777364e-03  -3.20205761e-02  -1.59739599e-02  -1.14634569e-02\n",
      "   1.06939997e-02   1.15506615e-02  -1.43003862e-03   7.61688464e-03\n",
      "  -4.72437202e-02   7.00642269e-03  -1.25589609e-03   1.80417217e-02\n",
      "  -5.77482153e-03  -4.49169977e-02   3.06961877e-02   2.42431727e-02\n",
      "   1.02310370e-02   2.51806850e-03  -4.36631049e-03   3.70244229e-02\n",
      "   1.15251055e-02  -5.39359785e-02  -7.06426542e-03   2.58730077e-03\n",
      "  -2.42620804e-04  -6.80551738e-04  -2.26022506e-02  -1.00437027e-02\n",
      "   5.18616559e-02   1.67408317e-02  -2.88850548e-03  -2.92663773e-02\n",
      "   1.84801948e-03   3.45836594e-03  -2.34660070e-02  -3.94656986e-03\n",
      "   1.50062458e-04  -2.12635219e-02   4.48433249e-02  -3.49047273e-03\n",
      "  -5.10372701e-03  -2.22388772e-02  -2.16328370e-03   1.39124240e-02\n",
      "   9.43511826e-03  -1.90320954e-03   3.92126752e-03   1.62634876e-02\n",
      "  -4.70653072e-03   2.97103471e-02  -7.67704910e-03  -1.26100127e-02\n",
      "   1.57126516e-02  -1.02263382e-02  -1.58622110e-02   5.23536475e-03\n",
      "   5.70637812e-03   5.47299653e-04   6.47251767e-03  -1.46590909e-02\n",
      "   7.28385091e-03  -1.51054451e-03  -1.48738460e-02  -5.26288801e-03\n",
      "  -2.60755678e-03   1.85832624e-02  -2.65590056e-01   3.28311113e-01\n",
      "   1.39174859e-01   2.43917204e-01  -1.21054301e-02   8.36919812e-02\n",
      "   2.79063706e-03   3.28365414e-01   2.31583949e-02  -1.67600030e-01\n",
      "  -6.04530504e-02   2.74032667e-02   8.96114138e-02  -1.49206560e-01\n",
      "  -2.29424579e-01   1.15401292e-02   2.48538217e-02  -5.83678669e-03\n",
      "  -1.96364058e-03  -1.36061817e-03   4.48598992e-02   5.37781075e-01\n",
      "  -1.18980786e-02   9.36430573e-03] \n",
      "After Regularization with lambda 0.2:\n",
      " Train MSE:0.163891979684, Valid MSE:0.163575997299\n",
      "\n",
      "_________________________Model Training for run 4 with lambda:0.2_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.2 is: \n",
      " [ -1.41644419e-02  -1.44281343e-04   1.17622066e-02  -1.94015124e-02\n",
      "   1.53766557e-02   1.64701911e-02  -8.14610897e-03  -8.54424918e-03\n",
      "  -7.08296230e-03  -3.15464871e-02   8.76377905e-03  -2.50832436e-02\n",
      "   2.82506871e-02  -5.60352018e-03  -1.94147255e-02   5.54130289e-05\n",
      "   1.67646226e-03   1.44434099e-02  -1.54880970e-02   7.65287024e-03\n",
      "   2.16051151e-03   1.69562296e-02   1.75918121e-02  -1.44800234e-02\n",
      "  -3.79673067e-03   5.03735993e-03  -1.91116347e-04  -1.14649449e-03\n",
      "  -6.25078163e-03  -5.20322673e-02  -2.48350614e-02  -2.59141919e-03\n",
      "  -8.92435417e-03  -2.63931490e-02  -1.19615425e-02  -2.01975486e-02\n",
      "   7.76168062e-03   1.18307406e-02  -5.24594345e-03  -9.78542343e-04\n",
      "  -3.90586939e-02   1.19923655e-02   1.93189621e-03   1.17852416e-02\n",
      "  -1.48376824e-03  -3.06357055e-02   1.83934634e-02   1.62640962e-02\n",
      "   5.91274147e-03   2.14931482e-03  -2.29969973e-03   5.10660457e-02\n",
      "   2.28563031e-03  -4.00535115e-02  -1.02779034e-02   5.05717688e-03\n",
      "  -6.72643435e-04   4.80179939e-03  -9.11686658e-04  -1.09651066e-02\n",
      "   1.95049221e-02   6.40860090e-03   3.33381857e-03  -8.17092445e-03\n",
      "   9.64604522e-03  -1.17406787e-02  -7.33279924e-03   4.36751834e-03\n",
      "   2.04803749e-03  -2.82903375e-02   3.87317613e-02   2.85098904e-04\n",
      "  -4.06455828e-03  -9.34750670e-04   3.56603087e-03   3.00584562e-02\n",
      "   8.88657275e-03  -2.33222268e-03   1.90628690e-03   1.30389791e-02\n",
      "  -3.75991823e-03  -1.53034473e-03  -1.93680002e-02   2.49157535e-02\n",
      "   1.18830900e-02  -7.61186171e-03  -9.33963592e-03  -1.59577791e-03\n",
      "   7.54137480e-03   1.68244631e-03   7.42934578e-03  -3.01653699e-02\n",
      "   1.68021287e-02   2.16590154e-03  -1.10330591e-02  -1.47426107e-02\n",
      "   6.43254762e-03   1.25616578e-02  -2.47720447e-01   3.37691249e-01\n",
      "   1.50225992e-01   2.37763562e-01  -4.86948457e-02   4.24056964e-02\n",
      "   2.07436914e-02   3.37670023e-01   1.02666809e-02  -1.41910014e-01\n",
      "  -4.87236587e-02   2.69680967e-02   8.12796925e-02  -1.42439898e-01\n",
      "  -1.76661770e-01   7.01442246e-03   3.85279136e-02  -1.52994500e-02\n",
      "  -2.47089757e-04   7.52418194e-04   7.83259118e-02   4.85453806e-01\n",
      "  -2.58149602e-02   6.64951715e-03] \n",
      "After Regularization with lambda 0.2:\n",
      " Train MSE:0.145552237336, Valid MSE:0.145557857032\n",
      "Average validation error over 5 runs 0.0665321854898 with lambda:0.2\n",
      "Average training error over 5 runs 0.0660311055519 with lambda:0.2\n",
      "\n",
      "_________________________Model Training for run 0 with lambda:0.3_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.3 is: \n",
      " [  3.47846029e-02  -8.34083771e-05  -3.59575601e-03  -1.03902166e-02\n",
      "   1.65577068e-02   2.47126592e-02  -1.22351937e-02   2.88050113e-03\n",
      "   2.10520912e-02  -4.05664831e-02  -4.99821630e-03  -4.22564442e-03\n",
      "   1.25715399e-02  -2.00236164e-03  -5.13152561e-02   7.26046876e-03\n",
      "   3.18712794e-03   1.12434470e-02  -2.69690236e-02   2.06067998e-02\n",
      "  -1.68026027e-03   4.33269075e-02   3.21636627e-02  -2.45234463e-02\n",
      "  -8.06468848e-04   1.66001814e-03   9.17987879e-04  -7.81206923e-04\n",
      "  -9.53992685e-04  -5.62448717e-03  -3.00392877e-02  -7.41774429e-03\n",
      "  -1.07489795e-03  -2.62576149e-02  -1.72089973e-02  -1.38652571e-02\n",
      "   5.77960087e-03   1.40867203e-02   1.47504428e-04   5.28331664e-03\n",
      "  -3.80359319e-02   9.21079879e-03   3.37160972e-03   1.06379756e-02\n",
      "   1.42190845e-02  -2.92292232e-02   1.66408341e-02   2.24336762e-02\n",
      "   4.50279375e-03   3.74462001e-03  -7.06266825e-03   2.83333345e-02\n",
      "   1.34248426e-02  -2.23793531e-02  -5.75948424e-03   3.89936787e-03\n",
      "  -9.37060235e-04  -5.16235084e-03  -3.06687824e-02  -7.12186511e-04\n",
      "   3.18062909e-02   2.50387766e-02   4.82425343e-03  -2.77780250e-02\n",
      "   1.99586143e-02  -2.52217462e-02  -2.02840674e-02  -4.67332139e-03\n",
      "   4.88858928e-03  -1.38786718e-02   3.29153633e-02  -3.94478125e-03\n",
      "  -4.85585751e-03  -1.45575944e-03   3.16812860e-03   2.56666689e-03\n",
      "   6.14980988e-03  -6.93698826e-04  -3.44608643e-03   2.00458280e-02\n",
      "  -6.23424334e-03   2.26891637e-02  -7.25140396e-03  -1.09371760e-02\n",
      "   1.00791156e-02  -3.21325462e-03  -1.49543542e-02  -1.05505182e-03\n",
      "   6.79070830e-03   9.06663263e-03   4.45826130e-03  -4.13215037e-02\n",
      "   2.76901805e-02   1.32571871e-02  -9.98292370e-03  -2.95051955e-03\n",
      "   1.27979914e-04   6.79463740e-03  -1.29632446e-01   2.82056197e-01\n",
      "   1.60624852e-01   2.08587380e-01  -8.81915908e-02   1.92287983e-01\n",
      "  -4.95486837e-02   2.81907321e-01   1.43210130e-02  -1.92566063e-01\n",
      "  -6.61685150e-02   3.39947247e-03   1.00642150e-01  -1.69909111e-01\n",
      "  -1.72383883e-01  -3.26575979e-03   3.49112190e-02   1.07505379e-03\n",
      "  -4.78225282e-03  -4.48973720e-03   5.72808083e-02   4.43784282e-01\n",
      "  -3.37569131e-02   1.35511358e-02] \n",
      "After Regularization with lambda 0.3:\n",
      " Train MSE:0.191327370254, Valid MSE:0.191715207653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 1 with lambda:0.3_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.3 is: \n",
      " [ -4.96696274e-02  -7.47762273e-05  -1.46134097e-03   4.71321421e-03\n",
      "   1.53740417e-02   1.76687754e-02  -9.31071101e-03  -1.05475030e-02\n",
      "   2.73090585e-03  -2.32888742e-02   2.31988554e-03  -1.65731351e-02\n",
      "   1.02956892e-02  -4.02775134e-03  -2.42386274e-02  -1.20890082e-02\n",
      "   1.29194506e-03  -5.71547064e-03   3.42638445e-04   5.61601112e-03\n",
      "  -4.95830145e-03   5.51254228e-02   2.23538905e-03  -2.21655788e-03\n",
      "  -1.50825093e-04   2.54696968e-03  -1.22019039e-03   6.72730608e-05\n",
      "  -1.60619546e-03  -2.97614774e-02   1.04062572e-03  -3.83821837e-03\n",
      "  -1.31107599e-02  -2.60704592e-02  -9.83489682e-03  -7.48619992e-03\n",
      "   3.65473968e-03   3.00575707e-03   1.33990606e-02   1.16960610e-02\n",
      "  -2.12601948e-02   6.68861829e-03   4.45316663e-03   1.92493857e-02\n",
      "  -6.89520069e-06  -2.44469806e-02   3.36906463e-02   1.41963330e-02\n",
      "   6.24780802e-03   3.10270535e-04  -4.44909877e-03   6.61316830e-02\n",
      "   3.18187235e-03  -4.56973120e-02  -2.29361346e-03   3.37792637e-03\n",
      "  -1.39802177e-03  -4.17721313e-03  -1.18146254e-02   7.14868599e-03\n",
      "   3.51022727e-03   3.90901204e-02  -1.29369893e-02  -2.44125705e-02\n",
      "  -4.94852652e-04   1.70308665e-04  -2.71714490e-02   1.43257884e-03\n",
      "  -5.37664702e-03  -3.77884201e-03   4.20816008e-02  -7.06405064e-03\n",
      "  -2.98480805e-03   5.26845536e-03   7.49924004e-03  -5.49819778e-03\n",
      "   1.06395600e-03   3.28277977e-03   5.57171474e-03  -1.17106762e-03\n",
      "   1.41069383e-03   2.44123546e-02  -2.98600123e-02  -1.35359627e-03\n",
      "   1.72853582e-02  -1.08508262e-02  -1.79916551e-02   9.52213539e-03\n",
      "   8.51586146e-03   2.88778215e-03  -1.58861137e-03  -1.34260191e-02\n",
      "   1.67036250e-02  -7.25840583e-03  -1.44403319e-02   8.23041715e-03\n",
      "  -1.75206065e-03   9.68248525e-03  -1.10619236e-01   3.63070369e-01\n",
      "   6.93528094e-02   1.64407705e-01  -7.30763483e-02   9.77706889e-02\n",
      "  -3.72968361e-03   3.63140089e-01   2.93290309e-02  -7.69978052e-02\n",
      "  -2.00531665e-02   6.18519333e-02   1.25329228e-01  -1.09846047e-01\n",
      "  -1.83765811e-01   7.62110868e-03   3.41992779e-02  -7.49225024e-03\n",
      "  -8.52279283e-03  -1.61786677e-03   4.57758143e-02   3.24313695e-01\n",
      "  -9.06634269e-03   6.94605649e-03] \n",
      "After Regularization with lambda 0.3:\n",
      " Train MSE:0.160108031663, Valid MSE:0.161206915909\n",
      "\n",
      "_________________________Model Training for run 2 with lambda:0.3_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.3 is: \n",
      " [ -6.00054096e-03   3.99314065e-05   2.26219969e-03  -6.52769061e-03\n",
      "   1.05943064e-02   2.14757864e-02  -1.54513645e-02  -1.16304858e-02\n",
      "   1.99741187e-02  -5.94077775e-02  -2.31693698e-03  -1.40880987e-02\n",
      "   1.47919092e-02  -3.49418199e-03  -4.20440434e-02   3.75314164e-03\n",
      "  -2.70756685e-04   3.65380479e-03  -1.38624634e-02   1.87667831e-02\n",
      "  -5.36087565e-03   4.20438120e-02   1.16932798e-02  -6.09920287e-03\n",
      "  -4.44489132e-04   3.21493763e-03   1.89475658e-03  -7.68566655e-04\n",
      "  -1.93920588e-03  -3.08388991e-02  -2.37712159e-02  -3.54396655e-03\n",
      "  -3.12084196e-03  -2.41582941e-02  -1.43926261e-02  -1.21010709e-02\n",
      "   7.04020631e-03   1.06204710e-02  -3.61824742e-03  -2.74483435e-03\n",
      "  -2.83017518e-02   2.10109104e-02   1.01852703e-02   7.07714790e-03\n",
      "  -1.13767738e-02  -4.10498243e-02   4.69949544e-02   2.67227767e-02\n",
      "   1.01600792e-02   4.00901925e-03  -4.13353770e-03   7.93983909e-02\n",
      "   1.54229655e-02  -2.11401115e-02  -3.35538325e-03   2.63150206e-04\n",
      "  -3.71309817e-04   1.41269363e-04  -7.85639662e-03  -1.74346087e-02\n",
      "   2.46300483e-02   2.51951462e-02  -1.74559390e-03  -2.65893082e-02\n",
      "   1.35130019e-02  -8.61948455e-04  -5.47982528e-03  -1.37019832e-02\n",
      "   9.83362975e-05  -1.61058115e-02   3.17301983e-02  -1.26978750e-02\n",
      "  -5.77198596e-03   1.24071359e-03   3.78998208e-03  -3.68208124e-03\n",
      "   3.40970096e-03   1.85509127e-03   3.80356656e-03   2.12119054e-02\n",
      "  -5.53221841e-03   1.85988319e-02  -4.43764543e-03  -6.90473825e-03\n",
      "   1.78250201e-02  -6.76059205e-03  -1.72134338e-02   1.74602576e-03\n",
      "   9.75779554e-03  -5.10973706e-03   5.99246031e-03  -3.68378983e-02\n",
      "   1.77298790e-02   1.20650804e-02  -1.75774926e-02  -8.27420937e-03\n",
      "   6.28757775e-04   1.92378814e-02  -2.02913774e-01   2.91592046e-01\n",
      "   1.41205637e-01   2.86539237e-01  -8.09034150e-03   4.94275700e-02\n",
      "  -3.02192218e-03   2.91819195e-01   2.60271811e-02  -1.40670836e-01\n",
      "  -5.00971279e-02   3.37267385e-02   1.07704328e-01  -1.64233756e-01\n",
      "  -1.07752709e-01  -8.19269313e-03   3.88238258e-02  -9.43617979e-03\n",
      "  -6.40038290e-03  -2.33942054e-03   4.29745366e-02   3.66959468e-01\n",
      "  -3.33893291e-02   1.40046120e-02] \n",
      "After Regularization with lambda 0.3:\n",
      " Train MSE:0.169808132899, Valid MSE:0.169745978346\n",
      "\n",
      "_________________________Model Training for run 3 with lambda:0.3_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.3 is: \n",
      " [  7.23871108e-03  -5.28439526e-05   8.88114726e-03  -5.48459398e-03\n",
      "   1.25953819e-02   2.15601987e-02  -1.05186573e-02  -9.75934699e-03\n",
      "   1.32227256e-02  -3.92962209e-02  -8.33288363e-03  -2.45364286e-02\n",
      "   1.18732672e-02  -4.15870667e-03  -5.22635617e-02   5.53620999e-03\n",
      "   1.36238300e-03   4.74349260e-03  -1.11501451e-02   1.74847515e-02\n",
      "   3.35243583e-03   4.50753696e-02   1.73676102e-02  -6.45995563e-03\n",
      "  -2.32711552e-03   5.99369927e-03   1.85889164e-03  -6.80623582e-04\n",
      "  -5.11629406e-03   4.91964001e-04  -2.42428075e-02  -1.48304148e-03\n",
      "  -8.79446102e-03  -2.97964102e-02  -1.61938329e-02  -1.08213241e-02\n",
      "   1.04952193e-02   1.18604249e-02  -1.42444223e-03   5.66150702e-03\n",
      "  -4.28422015e-02   7.99165277e-03   2.37174168e-03   1.17924405e-02\n",
      "  -6.87648540e-03  -3.92367835e-02   2.72214105e-02   2.41343143e-02\n",
      "   9.87831720e-03   2.14049418e-03  -4.23696145e-03   2.93794075e-02\n",
      "   1.16709224e-02  -4.30498800e-02  -7.42949866e-03   2.00847333e-03\n",
      "   8.80610698e-04  -1.21506504e-03  -2.02122204e-02  -8.01799282e-03\n",
      "   4.36041276e-02   1.97864119e-02  -8.00396921e-04  -2.89156754e-02\n",
      "   4.77570966e-03   5.12736484e-04  -2.07233542e-02  -5.88911897e-03\n",
      "   1.55444414e-03  -1.56731425e-02   4.28372876e-02  -2.77170011e-03\n",
      "  -5.12890468e-03  -1.86880368e-02  -1.40184578e-03   7.18991360e-03\n",
      "   9.37091909e-03  -2.16619796e-03   4.03359430e-03   1.59088475e-02\n",
      "  -4.55009568e-03   2.73364918e-02  -5.31762106e-03  -1.32634944e-02\n",
      "   1.50208983e-02  -9.00582198e-03  -1.63735842e-02   4.01596827e-03\n",
      "   6.50685132e-03  -3.48979889e-05   6.64691778e-03  -1.22864690e-02\n",
      "   8.88601245e-03  -6.92544890e-05  -1.54196688e-02  -5.67188245e-03\n",
      "  -2.04136017e-03   1.91250111e-02  -1.92995928e-01   3.17969920e-01\n",
      "   1.30074542e-01   2.45787917e-01  -4.99356838e-03   9.68063338e-02\n",
      "  -5.29741816e-03   3.17919170e-01   2.03166063e-02  -1.58617685e-01\n",
      "  -6.58590947e-02   2.26411315e-02   9.08681425e-02  -1.36012070e-01\n",
      "  -2.09434648e-01   1.21481900e-02   2.67719444e-02  -7.10404077e-03\n",
      "  -2.34888938e-03  -1.36962847e-03   4.61999917e-02   4.29643430e-01\n",
      "  -1.71275719e-02   9.47701859e-03] \n",
      "After Regularization with lambda 0.3:\n",
      " Train MSE:0.193917089642, Valid MSE:0.193576428105\n",
      "\n",
      "_________________________Model Training for run 4 with lambda:0.3_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.3 is: \n",
      " [ -6.25646466e-03  -1.36688358e-04   1.01642101e-02  -1.86021214e-02\n",
      "   1.54835104e-02   1.66199110e-02  -8.03996692e-03  -7.79972685e-03\n",
      "  -4.32020238e-03  -2.72400014e-02   3.44553657e-03  -2.39545684e-02\n",
      "   2.28785544e-02  -5.60580019e-03  -1.58271519e-02   1.12668641e-04\n",
      "   1.72623013e-03   1.47669622e-02  -1.45021480e-02   8.66121711e-03\n",
      "   2.39235543e-03   1.54455334e-02   1.67731807e-02  -1.28956368e-02\n",
      "  -4.06784042e-03   5.19253832e-03   3.87560375e-05  -1.26001302e-03\n",
      "  -6.43317227e-03  -4.45758849e-02  -2.51797413e-02  -3.19346692e-03\n",
      "  -8.31318261e-03  -2.52391451e-02  -1.24056775e-02  -1.87055606e-02\n",
      "   7.76349866e-03   1.22343542e-02  -5.27851682e-03  -2.34398568e-03\n",
      "  -3.57714904e-02   1.24796731e-02   4.53639380e-03   7.14711245e-03\n",
      "  -1.64530949e-03  -2.79496532e-02   1.70705429e-02   1.63146246e-02\n",
      "   5.98340171e-03   1.76586114e-03  -2.55083817e-03   3.76292323e-02\n",
      "   3.17500677e-03  -2.94425272e-02  -1.03939036e-02   4.45346825e-03\n",
      "   1.31820290e-04   4.24725421e-03  -4.42016793e-04  -9.39484431e-03\n",
      "   1.59322921e-02   8.31978976e-03   4.46034556e-03  -8.41991561e-03\n",
      "   9.51164569e-03  -1.14405804e-02  -6.21375090e-03   8.39980369e-04\n",
      "   3.47976225e-03  -2.05621117e-02   3.74484159e-02   7.97169254e-04\n",
      "  -4.08538347e-03   8.57160644e-05   3.89244535e-03   2.13935801e-02\n",
      "   8.99865963e-03  -2.45564534e-03   2.19243803e-03   1.29631449e-02\n",
      "  -3.72923270e-03  -2.25701136e-03  -1.47478105e-02   1.99137800e-02\n",
      "   1.13343742e-02  -7.26341036e-03  -9.62791106e-03  -1.93040751e-03\n",
      "   8.47677072e-03   1.72153213e-03   7.48993026e-03  -2.67107848e-02\n",
      "   1.89281249e-02   2.52284836e-03  -1.15887090e-02  -1.44301579e-02\n",
      "   6.44588014e-03   1.30098559e-02  -1.81972083e-01   3.26207561e-01\n",
      "   1.35279169e-01   2.39861889e-01  -3.81278387e-02   5.93404142e-02\n",
      "   1.00524896e-02   3.26190358e-01   7.59952094e-03  -1.32889308e-01\n",
      "  -5.32193385e-02   2.15685757e-02   8.34162683e-02  -1.29527710e-01\n",
      "  -1.61187977e-01   9.01816281e-03   4.14977430e-02  -1.57667803e-02\n",
      "  -4.34175201e-04   7.72265830e-04   7.98481876e-02   3.82143202e-01\n",
      "  -3.16760257e-02   6.77085677e-03] \n",
      "After Regularization with lambda 0.3:\n",
      " Train MSE:0.172540570017, Valid MSE:0.172548196952\n",
      "Average validation error over 5 runs 0.0887774574704 with lambda:0.3\n",
      "Average training error over 5 runs 0.0883329322205 with lambda:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 0 with lambda:0.4_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.4 is: \n",
      " [  3.69535893e-02  -8.67832290e-05  -2.03541931e-03  -1.08836800e-02\n",
      "   1.71288491e-02   2.59075101e-02  -1.18129123e-02   3.73595588e-03\n",
      "   2.09716991e-02  -3.57815776e-02  -7.58541737e-03  -3.35113125e-03\n",
      "   1.09061692e-02  -2.06821263e-03  -4.43205556e-02   7.38000525e-03\n",
      "   3.34218959e-03   1.22039499e-02  -2.47779180e-02   2.14826305e-02\n",
      "  -1.42025640e-03   3.90706112e-02   2.87850616e-02  -2.18949630e-02\n",
      "  -5.81636445e-04   1.76336780e-03   1.11106559e-03  -8.63303151e-04\n",
      "  -1.00282648e-03  -2.78968380e-03  -2.91400443e-02  -7.26683984e-03\n",
      "  -1.53278051e-03  -2.52875230e-02  -1.69706808e-02  -1.25046210e-02\n",
      "   5.61110577e-03   1.42351766e-02   3.81518680e-04   4.76010594e-03\n",
      "  -3.55639795e-02   9.82561213e-03   5.10629426e-03   8.29466125e-03\n",
      "   1.19551031e-02  -2.64672056e-02   1.56447142e-02   2.25268363e-02\n",
      "   4.50376465e-03   3.51635968e-03  -6.92540074e-03   2.17511035e-02\n",
      "   1.42080044e-02  -1.70046564e-02  -6.30244582e-03   3.48886302e-03\n",
      "  -3.52921662e-04  -5.13776835e-03  -2.79875417e-02  -8.07576367e-04\n",
      "   2.97205400e-02   2.50605915e-02   6.74477976e-03  -2.69650864e-02\n",
      "   1.78540703e-02  -2.28030977e-02  -1.84290932e-02  -4.20212653e-03\n",
      "   5.51224322e-03  -1.17458470e-02   3.16508335e-02  -2.39105888e-03\n",
      "  -4.39105640e-03  -2.54042709e-03   2.77521136e-03   1.94223665e-04\n",
      "   6.32973498e-03  -5.29213766e-04  -3.34848199e-03   1.99277139e-02\n",
      "  -6.10952998e-03   2.02521621e-02  -5.11844895e-03  -1.13387067e-02\n",
      "   9.35063672e-03  -3.01268723e-03  -1.52397083e-02  -9.00599539e-04\n",
      "   7.51391152e-03   8.84755353e-03   4.34944765e-03  -3.97892491e-02\n",
      "   2.75753540e-02   1.34904195e-02  -1.04634876e-02  -3.25576045e-03\n",
      "   4.01661869e-04   7.45381430e-03  -1.01985424e-01   2.76670282e-01\n",
      "   1.40108613e-01   2.13295927e-01  -7.09030056e-02   1.92191276e-01\n",
      "  -5.24370168e-02   2.76542117e-01   1.16817826e-02  -1.79980897e-01\n",
      "  -7.11112590e-02   2.86092603e-04   1.01256437e-01  -1.53671981e-01\n",
      "  -1.56342065e-01  -3.33937383e-03   3.59599417e-02  -6.82704747e-04\n",
      "  -4.96952588e-03  -4.38961541e-03   5.88065660e-02   3.64495318e-01\n",
      "  -3.63218842e-02   1.37708803e-02] \n",
      "After Regularization with lambda 0.4:\n",
      " Train MSE:0.214596111648, Valid MSE:0.214958859659\n",
      "\n",
      "_________________________Model Training for run 1 with lambda:0.4_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.4 is: \n",
      " [ -3.94467700e-02  -7.67294000e-05  -2.32057447e-03   3.88679581e-03\n",
      "   1.55418857e-02   1.72246319e-02  -9.12700955e-03  -1.00742304e-02\n",
      "   2.94386633e-03  -2.06450404e-02   4.42775798e-04  -1.57704698e-02\n",
      "   7.67610809e-03  -4.12195507e-03  -1.96085677e-02  -1.26932032e-02\n",
      "   1.34357038e-03  -5.31012424e-03  -3.63062434e-04   6.01901555e-03\n",
      "  -5.09073293e-03   5.03831966e-02   3.11322634e-03  -2.61041554e-03\n",
      "  -3.46384464e-04   2.60918151e-03  -1.06173333e-03   9.38639842e-05\n",
      "  -1.49764413e-03  -2.45010420e-02  -1.89402740e-05  -3.89826646e-03\n",
      "  -1.27193713e-02  -2.45145140e-02  -1.02957550e-02  -7.96100404e-03\n",
      "   3.66231634e-03   3.05815019e-03   1.31061771e-02   1.01453021e-02\n",
      "  -1.92689611e-02   7.40314613e-03   6.06253436e-03   1.61037978e-02\n",
      "  -8.91439839e-04  -2.14882876e-02   3.06636116e-02   1.41655771e-02\n",
      "   6.09280421e-03   9.71222491e-05  -4.38450004e-03   5.62686868e-02\n",
      "   2.98588520e-03  -4.13673940e-02  -2.55706935e-03   3.32028628e-03\n",
      "  -1.49731404e-03  -3.80013670e-03  -1.05352498e-02   6.54550803e-03\n",
      "   5.54788421e-03   3.51292089e-02  -1.18252278e-02  -2.36124484e-02\n",
      "   5.69082212e-04  -9.79473796e-04  -2.38764492e-02   1.00193198e-03\n",
      "  -5.64714800e-03  -2.93815786e-03   4.06923128e-02  -6.38655878e-03\n",
      "  -2.86901509e-03   6.24171430e-03   7.57843546e-03  -6.32854952e-03\n",
      "   9.30015252e-04   3.24010002e-03   5.26945611e-03  -1.38783529e-03\n",
      "   1.27526968e-03   2.04299360e-02  -2.49111897e-02  -2.81507064e-03\n",
      "   1.66558014e-02  -9.21798871e-03  -1.78265371e-02   7.81283275e-03\n",
      "   9.06660696e-03   2.69302742e-03  -1.50248958e-03  -1.01106576e-02\n",
      "   1.72814110e-02  -6.25225388e-03  -1.48478397e-02   7.37998400e-03\n",
      "  -1.23559219e-03   1.00071461e-02  -8.75592956e-02   3.46886204e-01\n",
      "   6.09209700e-02   1.84453241e-01  -6.27648265e-02   1.04414904e-01\n",
      "  -9.96032274e-03   3.46924861e-01   2.78895483e-02  -7.16056161e-02\n",
      "  -2.67331203e-02   5.37856190e-02   1.25879723e-01  -9.64808588e-02\n",
      "  -1.67713183e-01   7.79587105e-03   3.56887383e-02  -8.39871592e-03\n",
      "  -8.63851149e-03  -1.56051370e-03   4.77485973e-02   2.68817712e-01\n",
      "  -1.60758142e-02   7.20701788e-03] \n",
      "After Regularization with lambda 0.4:\n",
      " Train MSE:0.185593627789, Valid MSE:0.186699129795\n",
      "\n",
      "_________________________Model Training for run 2 with lambda:0.4_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.4 is: \n",
      " [  1.60537447e-04   5.04730929e-05   1.33347782e-03  -6.37958378e-03\n",
      "   1.04890036e-02   2.16276144e-02  -1.49286651e-02  -1.09532414e-02\n",
      "   2.03813841e-02  -5.32714957e-02  -6.33070816e-03  -1.28947367e-02\n",
      "   1.20763116e-02  -3.57639576e-03  -3.58199749e-02   2.98484677e-03\n",
      "  -1.77119245e-04   4.10810197e-03  -1.29613957e-02   1.89054434e-02\n",
      "  -5.55025170e-03   3.75976247e-02   1.17877974e-02  -6.01339284e-03\n",
      "  -5.06351322e-04   3.33820547e-03   2.05654104e-03  -7.19862108e-04\n",
      "  -1.95268069e-03  -2.55251113e-02  -2.36622686e-02  -3.56467260e-03\n",
      "  -3.50694679e-03  -2.33048096e-02  -1.47701711e-02  -1.16563963e-02\n",
      "   6.88926715e-03   1.05258773e-02  -3.43859739e-03  -3.28518037e-03\n",
      "  -2.63828151e-02   2.16127691e-02   1.08580021e-02   5.49688618e-03\n",
      "  -1.03478557e-02  -3.65205246e-02   4.28780303e-02   2.65600268e-02\n",
      "   1.02451897e-02   3.75495908e-03  -4.06915095e-03   6.91947578e-02\n",
      "   1.60706588e-02  -1.53555714e-02  -3.32708672e-03  -4.73518656e-04\n",
      "  -9.58066041e-05   1.20567300e-04  -7.23992277e-03  -1.49710015e-02\n",
      "   2.16009424e-02   2.43497016e-02  -4.31441867e-04  -2.60374830e-02\n",
      "   1.33724622e-02  -1.43116005e-03  -6.07489310e-03  -1.35956593e-02\n",
      "   9.18809308e-04  -1.47952059e-02   3.03605285e-02  -1.18157110e-02\n",
      "  -5.58535674e-03   2.75485947e-03   4.21472177e-03  -5.24023261e-03\n",
      "   3.07951757e-03   1.79002575e-03   3.85220302e-03   2.09496158e-02\n",
      "  -5.59128369e-03   1.61086508e-02  -3.41625300e-03  -6.20534491e-03\n",
      "   1.71140300e-02  -6.61583344e-03  -1.71059039e-02   1.27370894e-03\n",
      "   1.04145271e-02  -5.12096757e-03   5.95656427e-03  -3.49632672e-02\n",
      "   1.87594745e-02   1.30614942e-02  -1.80403443e-02  -8.37220356e-03\n",
      "   1.23069091e-03   1.95590930e-02  -1.61476761e-01   2.86920756e-01\n",
      "   1.25759208e-01   2.77251961e-01  -5.64856509e-03   6.15445159e-02\n",
      "  -1.01524224e-02   2.87052084e-01   2.38599751e-02  -1.30164244e-01\n",
      "  -5.49669746e-02   2.79105609e-02   1.07813868e-01  -1.48518007e-01\n",
      "  -1.00846824e-01  -7.57944974e-03   4.07146625e-02  -1.00064660e-02\n",
      "  -6.68225891e-03  -2.27235709e-03   4.24150638e-02   3.05788011e-01\n",
      "  -3.63531637e-02   1.39360600e-02] \n",
      "After Regularization with lambda 0.4:\n",
      " Train MSE:0.192758207917, Valid MSE:0.192703115668\n",
      "\n",
      "_________________________Model Training for run 3 with lambda:0.4_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.4 is: \n",
      " [  9.79006168e-03  -4.03699324e-05   6.52990852e-03  -5.81359226e-03\n",
      "   1.25122917e-02   2.12408803e-02  -1.03356946e-02  -8.96527169e-03\n",
      "   1.34350822e-02  -3.50361095e-02  -1.10463443e-02  -2.27431963e-02\n",
      "   9.96840663e-03  -4.23956508e-03  -4.52091464e-02   5.18967497e-03\n",
      "   1.55139874e-03   5.29244532e-03  -1.02517060e-02   1.80899338e-02\n",
      "   3.14813850e-03   4.09095552e-02   1.73565902e-02  -7.54704452e-03\n",
      "  -2.47087614e-03   6.03381762e-03   2.09114215e-03  -6.71591772e-04\n",
      "  -5.09240753e-03   1.94901856e-03  -2.35612892e-02  -1.84008939e-03\n",
      "  -8.69500790e-03  -2.79835604e-02  -1.62485308e-02  -1.01577287e-02\n",
      "   1.03064456e-02   1.20060238e-02  -1.33004603e-03   4.32593980e-03\n",
      "  -3.97137350e-02   8.62363929e-03   4.40098552e-03   8.35667695e-03\n",
      "  -7.21372319e-03  -3.46772974e-02   2.42208910e-02   2.39649394e-02\n",
      "   9.49453058e-03   1.91668245e-03  -4.14153196e-03   2.44096814e-02\n",
      "   1.16385126e-02  -3.58711219e-02  -7.71312218e-03   1.56788674e-03\n",
      "   1.51394114e-03  -1.40525392e-03  -1.81897076e-02  -6.54102581e-03\n",
      "   3.83084298e-02   2.08676966e-02   9.91072118e-04  -2.83835462e-02\n",
      "   5.88298074e-03  -9.13243307e-04  -1.87587203e-02  -6.46942312e-03\n",
      "   2.43088008e-03  -1.27820316e-02   4.10696017e-02  -2.02859024e-03\n",
      "  -5.07875735e-03  -1.63303541e-02  -9.08487972e-04   3.68543350e-03\n",
      "   9.33746828e-03  -2.29595516e-03   4.16122986e-03   1.55124235e-02\n",
      "  -4.44285902e-03   2.53287358e-02  -3.86184197e-03  -1.32450705e-02\n",
      "   1.44736830e-02  -7.91983549e-03  -1.66831314e-02   2.97107681e-03\n",
      "   7.03132880e-03  -4.73397266e-04   6.72109117e-03  -1.06829004e-02\n",
      "   9.81429125e-03   1.19889115e-03  -1.56944147e-02  -5.98122241e-03\n",
      "  -1.62869121e-03   1.93872584e-02  -1.52447503e-01   3.10072023e-01\n",
      "   1.19451067e-01   2.46399747e-01  -1.99879590e-03   1.06508274e-01\n",
      "  -1.08225939e-02   3.09981861e-01   1.81175991e-02  -1.50310081e-01\n",
      "  -6.94856914e-02   1.95910733e-02   9.11146134e-02  -1.24920940e-01\n",
      "  -1.91108593e-01   1.23915259e-02   2.81208098e-02  -8.21919534e-03\n",
      "  -2.57048008e-03  -1.33932420e-03   4.72113263e-02   3.59136623e-01\n",
      "  -2.11932201e-02   9.52596790e-03] \n",
      "After Regularization with lambda 0.4:\n",
      " Train MSE:0.219827753792, Valid MSE:0.219467338183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 4 with lambda:0.4_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.4 is: \n",
      " [ -5.92191348e-04  -1.28767256e-04   9.08987649e-03  -1.77626315e-02\n",
      "   1.54424221e-02   1.67432702e-02  -7.88512495e-03  -7.04691348e-03\n",
      "  -2.79944471e-03  -2.40246686e-02   1.92012250e-04  -2.25857211e-02\n",
      "   1.94829943e-02  -5.62167186e-03  -1.32452594e-02   3.36836948e-04\n",
      "   1.77601544e-03   1.50814185e-02  -1.37040701e-02   9.29742266e-03\n",
      "   2.64156226e-03   1.41787449e-02   1.58709397e-02  -1.14462305e-02\n",
      "  -4.24052471e-03   5.31682485e-03   2.53000987e-04  -1.34300862e-03\n",
      "  -6.54656361e-03  -3.93740102e-02  -2.50968726e-02  -3.58671225e-03\n",
      "  -7.91656419e-03  -2.42281631e-02  -1.26371495e-02  -1.73905843e-02\n",
      "   7.73517245e-03   1.24477621e-02  -5.20112470e-03  -3.16434072e-03\n",
      "  -3.34072943e-02   1.26811937e-02   5.93659602e-03   4.68065342e-03\n",
      "  -1.59362024e-03  -2.54623163e-02   1.55177829e-02   1.63039201e-02\n",
      "   5.96705214e-03   1.52950040e-03  -2.75034781e-03   2.84253170e-02\n",
      "   3.79113512e-03  -2.27017847e-02  -1.04105124e-02   3.90993804e-03\n",
      "   6.75279268e-04   3.92239205e-03  -1.62349139e-04  -8.03718085e-03\n",
      "   1.38183000e-02   9.05892087e-03   5.52760970e-03  -8.44896559e-03\n",
      "   8.84058702e-03  -1.07921891e-02  -5.63011028e-03  -9.02816114e-04\n",
      "   4.38105545e-03  -1.62417325e-02   3.62147600e-02   1.38267364e-03\n",
      "  -4.03862414e-03   4.60782711e-04   4.03388220e-03   1.66153711e-02\n",
      "   9.08346401e-03  -2.50088799e-03   2.46772190e-03   1.27617502e-02\n",
      "  -3.73401546e-03  -2.63651911e-03  -1.19087106e-02   1.67146827e-02\n",
      "   1.09068563e-02  -6.86585797e-03  -9.84746279e-03  -2.29547562e-03\n",
      "   9.12154976e-03   1.75171929e-03   7.50492566e-03  -2.41213129e-02\n",
      "   2.00925041e-02   2.87513357e-03  -1.18669360e-02  -1.41552895e-02\n",
      "   6.43615897e-03   1.32267008e-02  -1.44717860e-01   3.17488685e-01\n",
      "   1.21778527e-01   2.40669578e-01  -3.16477334e-02   7.18720275e-02\n",
      "   2.36394520e-03   3.17473695e-01   5.55782659e-03  -1.24782804e-01\n",
      "  -5.62170691e-02   1.80439556e-02   8.42135332e-02  -1.18896778e-01\n",
      "  -1.46956252e-01   1.01904109e-02   4.34105239e-02  -1.60492620e-02\n",
      "  -4.78910796e-04   8.17495722e-04   8.00818656e-02   3.16286610e-01\n",
      "  -3.58964016e-02   6.90357893e-03] \n",
      "After Regularization with lambda 0.4:\n",
      " Train MSE:0.196674689643, Valid MSE:0.196683782413\n",
      "Average validation error over 5 runs 0.107664955416 with lambda:0.4\n",
      "Average training error over 5 runs 0.10725912321 with lambda:0.4\n",
      "\n",
      "_________________________Model Training for run 0 with lambda:0.5_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.5 is: \n",
      " [  3.68644145e-02  -8.74459658e-05  -1.08699160e-03  -1.11016157e-02\n",
      "   1.73656267e-02   2.66963802e-02  -1.15020812e-02   4.28394203e-03\n",
      "   2.02889809e-02  -3.21038506e-02  -8.89767899e-03  -2.75405278e-03\n",
      "   9.77807761e-03  -2.13779373e-03  -3.89684084e-02   7.39521145e-03\n",
      "   3.46706831e-03   1.28485154e-02  -2.29175507e-02   2.20277874e-02\n",
      "  -1.17812920e-03   3.56883948e-02   2.59201161e-02  -1.95107652e-02\n",
      "  -3.94615304e-04   1.86793643e-03   1.30036257e-03  -9.21854335e-04\n",
      "  -1.02593621e-03  -1.14662348e-03  -2.82403099e-02  -7.14965806e-03\n",
      "  -1.88334077e-03  -2.43870897e-02  -1.68045081e-02  -1.13789074e-02\n",
      "   5.46733134e-03   1.42689161e-02   5.56166150e-04   4.37957391e-03\n",
      "  -3.35551076e-02   1.01138136e-02   6.23160168e-03   6.71706402e-03\n",
      "   1.02748491e-02  -2.40653328e-02   1.43715792e-02   2.24656631e-02\n",
      "   4.42651174e-03   3.37450595e-03  -6.82291222e-03   1.72069466e-02\n",
      "   1.46320606e-02  -1.32501427e-02  -6.74849413e-03   3.11694444e-03\n",
      "   1.10152466e-04  -5.04162045e-03  -2.56582503e-02  -6.71421034e-04\n",
      "   2.79430876e-02   2.47202001e-02   8.16508938e-03  -2.62098271e-02\n",
      "   1.60803290e-02  -2.07492606e-02  -1.68686357e-02  -3.65114709e-03\n",
      "   5.90215035e-03  -1.03505752e-02   3.04382776e-02  -1.14248733e-03\n",
      "  -4.02942510e-03  -3.30791538e-03   2.46791899e-03  -1.26090321e-03\n",
      "   6.48156966e-03  -3.92536215e-04  -3.22280566e-03   1.97114332e-02\n",
      "  -6.00685218e-03   1.83560811e-02  -3.89330342e-03  -1.12003380e-02\n",
      "   8.79982667e-03  -2.77229495e-03  -1.53930831e-02  -9.03549948e-04\n",
      "   8.01986857e-03   8.65728372e-03   4.24719097e-03  -3.81841503e-02\n",
      "   2.71730570e-02   1.37252375e-02  -1.07736550e-02  -3.48240996e-03\n",
      "   6.21588733e-04   7.89326636e-03  -8.45469992e-02   2.72377313e-01\n",
      "   1.25062095e-01   2.16212682e-01  -5.93440863e-02   1.91681172e-01\n",
      "  -5.38896659e-02   2.72260437e-01   9.51256715e-03  -1.68863942e-01\n",
      "  -7.40107291e-02  -1.13430576e-03   1.01224913e-01  -1.40836395e-01\n",
      "  -1.43044022e-01  -3.38057955e-03   3.65829692e-02  -2.06680000e-03\n",
      "  -5.08799843e-03  -4.28221797e-03   5.97740945e-02   3.09946835e-01\n",
      "  -3.82738198e-02   1.38968781e-02] \n",
      "After Regularization with lambda 0.5:\n",
      " Train MSE:0.236723483968, Valid MSE:0.237067726702\n",
      "\n",
      "_________________________Model Training for run 1 with lambda:0.5_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.5 is: \n",
      " [ -3.24706023e-02  -7.79846364e-05  -2.80726436e-03   3.24268782e-03\n",
      "   1.55229961e-02   1.67934499e-02  -8.98911868e-03  -9.63583070e-03\n",
      "   2.96623769e-03  -1.86203575e-02  -7.58035779e-04  -1.49706966e-02\n",
      "   5.85287605e-03  -4.20233789e-03  -1.61982683e-02  -1.29653775e-02\n",
      "   1.39379537e-03  -4.83376501e-03  -8.41879433e-04   6.29635145e-03\n",
      "  -5.20078804e-03   4.65373789e-02   3.76286477e-03  -2.82659446e-03\n",
      "  -5.06021196e-04   2.67788051e-03  -9.06206515e-04   1.14003710e-04\n",
      "  -1.40616017e-03  -2.07254286e-02  -8.20326971e-04  -3.96498106e-03\n",
      "  -1.23623570e-02  -2.31681336e-02  -1.06399438e-02  -8.28894451e-03\n",
      "   3.66657053e-03   3.09004805e-03   1.28253295e-02   8.87850953e-03\n",
      "  -1.77554549e-02   7.86843332e-03   7.06544289e-03   1.40384716e-02\n",
      "  -1.38454154e-03  -1.90216584e-02   2.79706809e-02   1.40750122e-02\n",
      "   5.91321644e-03  -4.22510776e-05  -4.39446819e-03   4.87951731e-02\n",
      "   2.81459487e-03  -3.79604255e-02  -2.79177127e-03   3.22048968e-03\n",
      "  -1.53302125e-03  -3.48611375e-03  -9.36698195e-03   6.17143840e-03\n",
      "   6.68264906e-03   3.21621658e-02  -1.08097452e-02  -2.28347411e-02\n",
      "   1.20535667e-03  -1.65133560e-03  -2.13549751e-02   8.40454960e-04\n",
      "  -5.82615789e-03  -2.48357241e-03   3.92331197e-02  -5.80690138e-03\n",
      "  -2.76709066e-03   6.72527445e-03   7.57356573e-03  -6.70059379e-03\n",
      "   8.50205760e-04   3.19614826e-03   5.06235423e-03  -1.57809848e-03\n",
      "   1.17864377e-03   1.76320801e-02  -2.14189370e-02  -3.78185681e-03\n",
      "   1.60887554e-02  -7.95939630e-03  -1.75859418e-02   6.44893515e-03\n",
      "   9.44141446e-03   2.53643718e-03  -1.43252507e-03  -7.68712278e-03\n",
      "   1.75531221e-02  -5.31609387e-03  -1.51216453e-02   6.69160622e-03\n",
      "  -8.50783346e-04   1.02018550e-02  -7.32016480e-02   3.35208593e-01\n",
      "   5.47604247e-02   1.96622018e-01  -5.53701891e-02   1.09880490e-01\n",
      "  -1.47389361e-02   3.35227523e-01   2.66440840e-02  -6.68815946e-02\n",
      "  -3.11412398e-02   4.81569632e-02   1.25700828e-01  -8.64330616e-02\n",
      "  -1.53812074e-01   7.85892993e-03   3.66839820e-02  -9.17937546e-03\n",
      "  -8.74141409e-03  -1.49350326e-03   4.91618107e-02   2.29913197e-01\n",
      "  -2.10413707e-02   7.41255426e-03] \n",
      "After Regularization with lambda 0.5:\n",
      " Train MSE:0.209902884607, Valid MSE:0.2110114785\n",
      "\n",
      "_________________________Model Training for run 2 with lambda:0.5_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.5 is: \n",
      " [  4.04652456e-03   5.97005403e-05   7.88234758e-04  -6.20023804e-03\n",
      "   1.03691897e-02   2.17748120e-02  -1.44724751e-02  -1.04020366e-02\n",
      "   2.01541495e-02  -4.84169248e-02  -8.75734720e-03  -1.17348548e-02\n",
      "   1.01647520e-02  -3.65022384e-03  -3.11743666e-02   2.42367453e-03\n",
      "  -9.10681533e-05   4.54244025e-03  -1.21990911e-02   1.89089862e-02\n",
      "  -5.64994926e-03   3.41807364e-02   1.16311780e-02  -5.66845419e-03\n",
      "  -5.54292145e-04   3.44262195e-03   2.19802252e-03  -6.64486295e-04\n",
      "  -1.96653438e-03  -2.17920948e-02  -2.33524523e-02  -3.59284245e-03\n",
      "  -3.78331711e-03  -2.25489344e-02  -1.50452884e-02  -1.12116612e-02\n",
      "   6.75954011e-03   1.04063689e-02  -3.26825327e-03  -3.59407047e-03\n",
      "  -2.47499270e-02   2.18554009e-02   1.12037186e-02   4.52679675e-03\n",
      "  -9.33898324e-03  -3.28655336e-02   3.93283821e-02   2.63781994e-02\n",
      "   1.02730356e-02   3.58879385e-03  -4.04214203e-03   6.14647018e-02\n",
      "   1.64957668e-02  -1.12826293e-02  -3.31039783e-03  -1.09031181e-03\n",
      "   9.30390278e-05   1.53594826e-04  -6.66199599e-03  -1.30618611e-02\n",
      "   1.95179043e-02   2.33600025e-02   6.94416186e-04  -2.53709391e-02\n",
      "   1.29924484e-02  -1.75944453e-03  -6.36218201e-03  -1.31585596e-02\n",
      "   1.52575231e-03  -1.39440941e-02   2.90873531e-02  -1.09557965e-02\n",
      "  -5.40197187e-03   3.71522019e-03   4.48794239e-03  -6.10914144e-03\n",
      "   2.82996625e-03   1.76133430e-03   3.92451319e-03   2.06471633e-02\n",
      "  -5.64030111e-03   1.40969571e-02  -2.79209488e-03  -5.43159359e-03\n",
      "   1.64946706e-02  -6.37785449e-03  -1.69400102e-02   8.31327156e-04\n",
      "   1.08700873e-02  -5.09273936e-03   5.90221182e-03  -3.32534343e-02\n",
      "   1.93037318e-02   1.37953727e-02  -1.83061263e-02  -8.42704297e-03\n",
      "   1.71679262e-03   1.97039248e-02  -1.34254107e-01   2.82604870e-01\n",
      "   1.13210638e-01   2.70490720e-01  -4.21694014e-03   7.05215684e-02\n",
      "  -1.53228938e-02   2.82683256e-01   2.21229274e-02  -1.21055546e-01\n",
      "  -5.81785614e-02   2.39792440e-02   1.07434034e-01  -1.35911723e-01\n",
      "  -9.38061901e-02  -7.18308108e-03   4.20537414e-02  -1.05076687e-02\n",
      "  -6.82296571e-03  -2.22380538e-03   4.19146382e-02   2.62669030e-01\n",
      "  -3.88163599e-02   1.38619601e-02] \n",
      "After Regularization with lambda 0.5:\n",
      " Train MSE:0.214039704161, Valid MSE:0.213991550972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 3 with lambda:0.5_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.5 is: \n",
      " [  1.15697929e-02  -2.95980107e-05   5.07775281e-03  -5.97773660e-03\n",
      "   1.23764357e-02   2.10251611e-02  -1.01480039e-02  -8.28992130e-03\n",
      "   1.31196005e-02  -3.17286386e-02  -1.25446615e-02  -2.10489386e-02\n",
      "   8.54870227e-03  -4.30332890e-03  -3.97643071e-02   4.89907335e-03\n",
      "   1.72194042e-03   5.77663334e-03  -9.56142379e-03   1.84577100e-02\n",
      "   2.99528757e-03   3.75739406e-02   1.68578436e-02  -7.89827471e-03\n",
      "  -2.56616045e-03   6.07264679e-03   2.30425455e-03  -6.52856244e-04\n",
      "  -5.05690522e-03   2.71232239e-03  -2.28584994e-02  -2.17371371e-03\n",
      "  -8.56696729e-03  -2.64700817e-02  -1.62479716e-02  -9.55568008e-03\n",
      "   1.01355272e-02   1.20697173e-02  -1.20874126e-03   3.35263002e-03\n",
      "  -3.72499950e-02   9.01523595e-03   5.64547451e-03   6.20913645e-03\n",
      "  -7.22462802e-03  -3.09968219e-02   2.16261100e-02   2.37530734e-02\n",
      "   9.10972402e-03   1.76401930e-03  -4.06712835e-03   2.07782461e-02\n",
      "   1.15252634e-02  -3.06822002e-02  -7.94203720e-03   1.20920779e-03\n",
      "   1.91553169e-03  -1.45323250e-03  -1.64483417e-02  -5.40053985e-03\n",
      "   3.45401613e-02   2.11245280e-02   2.48520535e-03  -2.77567206e-02\n",
      "   6.26324898e-03  -1.69713622e-03  -1.71663466e-02  -6.51376270e-03\n",
      "   3.03156654e-03  -1.10152528e-02   3.94769599e-02  -1.34097942e-03\n",
      "  -5.00482432e-03  -1.46287389e-02  -5.52970043e-04   1.58475905e-03\n",
      "   9.31763752e-03  -2.37303337e-03   4.29296832e-03   1.51228977e-02\n",
      "  -4.35683177e-03   2.36045521e-02  -2.88489449e-03  -1.29271071e-02\n",
      "   1.40083752e-02  -7.03017596e-03  -1.68238796e-02   2.10958620e-03\n",
      "   7.39231563e-03  -8.17791552e-04   6.74399753e-03  -9.48495893e-03\n",
      "   1.03878883e-02   2.25847841e-03  -1.58304363e-02  -6.21103922e-03\n",
      "  -1.30608546e-03   1.94884962e-02  -1.26458595e-01   3.03709799e-01\n",
      "   1.09976452e-01   2.46327384e-01  -8.01882288e-04   1.13946980e-01\n",
      "  -1.47966239e-02   3.03603394e-01   1.63159031e-02  -1.42778149e-01\n",
      "  -7.18355507e-02   1.76501049e-02   9.08689714e-02  -1.15716878e-01\n",
      "  -1.75106584e-01   1.24511043e-02   2.91068411e-02  -9.18132600e-03\n",
      "  -2.71054820e-03  -1.28922646e-03   4.79298748e-02   3.09101296e-01\n",
      "  -2.44291853e-02   9.53974298e-03] \n",
      "After Regularization with lambda 0.5:\n",
      " Train MSE:0.243706360582, Valid MSE:0.243329545708\n",
      "\n",
      "_________________________Model Training for run 4 with lambda:0.5_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.5 is: \n",
      " [  3.17724475e-03  -1.21068493e-04   8.28668923e-03  -1.69425520e-02\n",
      "   1.53051589e-02   1.68052912e-02  -7.72439728e-03  -6.36975376e-03\n",
      "  -1.95991221e-03  -2.15344680e-02  -1.88295826e-03  -2.12717469e-02\n",
      "   1.70848641e-02  -5.63934201e-03  -1.13119278e-02   5.98476386e-04\n",
      "   1.82442490e-03   1.53371851e-02  -1.30067867e-02   9.71134202e-03\n",
      "   2.87604801e-03   1.31264514e-02   1.50348321e-02  -1.02012263e-02\n",
      "  -4.37032607e-03   5.42505471e-03   4.52244057e-04  -1.40467090e-03\n",
      "  -6.62144463e-03  -3.55022504e-02  -2.48362449e-02  -3.86574667e-03\n",
      "  -7.64167649e-03  -2.33384268e-02  -1.27787872e-02  -1.62506073e-02\n",
      "   7.69037488e-03   1.25631199e-02  -5.08138465e-03  -3.68417106e-03\n",
      "  -3.15040234e-02   1.27009186e-02   6.74095813e-03   3.18106098e-03\n",
      "  -1.49435191e-03  -2.32951588e-02   1.39797853e-02   1.62480750e-02\n",
      "   5.90012054e-03   1.36586466e-03  -2.91368110e-03   2.16462684e-02\n",
      "   4.23781276e-03  -1.80016895e-02  -1.03881062e-02   3.43390909e-03\n",
      "   1.08727712e-03   3.69741507e-03   5.33320780e-05  -6.91221320e-03\n",
      "   1.23999788e-02   9.34118775e-03   6.46516039e-03  -8.35066902e-03\n",
      "   8.12052829e-03  -1.01412033e-02  -5.21682226e-03  -1.86448215e-03\n",
      "   4.99796779e-03  -1.34529791e-02   3.50552801e-02   1.94163708e-03\n",
      "  -3.96892429e-03   5.49359262e-04   4.09533693e-03   1.36040270e-02\n",
      "   9.14639496e-03  -2.51954613e-03   2.72050983e-03   1.25082026e-02\n",
      "  -3.74614376e-03  -2.87503203e-03  -9.98379627e-03   1.45071299e-02\n",
      "   1.05492291e-02  -6.50760618e-03  -9.98935996e-03  -2.62962247e-03\n",
      "   9.58364104e-03   1.77910819e-03   7.49941701e-03  -2.20543131e-02\n",
      "   2.07655530e-02   3.16591589e-03  -1.20113444e-02  -1.39062915e-02\n",
      "   6.40996453e-03   1.33216466e-02  -1.20545101e-01   3.10503383e-01\n",
      "   1.10642674e-01   2.40717737e-01  -2.72997295e-02   8.15398666e-02\n",
      "  -3.43486715e-03   3.10489799e-01   3.92547739e-03  -1.17583332e-01\n",
      "  -5.81791070e-02   1.56824139e-02   8.43369415e-02  -1.10123750e-01\n",
      "  -1.34481157e-01   1.09081892e-02   4.47143534e-02  -1.62339160e-02\n",
      "  -4.61841126e-04   8.72548178e-04   7.96845947e-02   2.70445481e-01\n",
      "  -3.90865273e-02   7.03563594e-03] \n",
      "After Regularization with lambda 0.5:\n",
      " Train MSE:0.219534636788, Valid MSE:0.219544782325\n",
      "Average validation error over 5 runs 0.12442553562 with lambda:0.5\n",
      "Average training error over 5 runs 0.124048021897 with lambda:0.5\n",
      "\n",
      "_________________________Model Training for run 0 with lambda:0.6_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.6 is: \n",
      " [  3.59125041e-02  -8.66523191e-05  -4.92021242e-04  -1.11502611e-02\n",
      "   1.74055593e-02   2.72098184e-02  -1.12676224e-02   4.62486142e-03\n",
      "   1.93430126e-02  -2.91551227e-02  -9.52521374e-03  -2.34349771e-03\n",
      "   8.90724968e-03  -2.20397256e-03  -3.47490492e-02   7.34391218e-03\n",
      "   3.57258486e-03   1.32815254e-02  -2.13124025e-02   2.23649012e-02\n",
      "  -9.59914572e-04   3.29337539e-02   2.35537944e-02  -1.74432359e-02\n",
      "  -2.42678591e-04   1.97086146e-03   1.48093631e-03  -9.64308336e-04\n",
      "  -1.03531396e-03  -1.48626098e-04  -2.73924526e-02  -7.05648594e-03\n",
      "  -2.16228566e-03  -2.35654393e-02  -1.66945304e-02  -1.04297410e-02\n",
      "   5.34052291e-03   1.42404248e-02   6.91794652e-04   4.08564267e-03\n",
      "  -3.18487661e-02   1.02133580e-02   6.97097467e-03   5.58034472e-03\n",
      "   8.97060048e-03  -2.20172017e-02   1.30452089e-02   2.23091051e-02\n",
      "   4.30433328e-03   3.28034796e-03  -6.74478274e-03   1.38462903e-02\n",
      "   1.48477496e-02  -1.04514360e-02  -7.12635938e-03   2.78782295e-03\n",
      "   4.85056675e-04  -4.91539365e-03  -2.36146154e-02  -4.47431146e-04\n",
      "   2.64222179e-02   2.42237651e-02   9.23724860e-03  -2.54952406e-02\n",
      "   1.46186237e-02  -1.90293073e-02  -1.55331239e-02  -3.11345111e-03\n",
      "   6.14692750e-03  -9.35539016e-03   2.93090439e-02  -1.25990440e-04\n",
      "  -3.74061574e-03  -3.87039720e-03   2.22008830e-03  -2.21392596e-03\n",
      "   6.61057081e-03  -2.82149461e-04  -3.09098135e-03   1.94480323e-02\n",
      "  -5.91533194e-03   1.67996051e-02  -3.13121495e-03  -1.08187780e-02\n",
      "   8.35987213e-03  -2.54967734e-03  -1.54469527e-02  -9.71449803e-04\n",
      "   8.38739660e-03   8.49041829e-03   4.14908969e-03  -3.66052324e-02\n",
      "   2.66482139e-02   1.39184627e-02  -1.09855559e-02  -3.65229983e-03\n",
      "   7.96045945e-04   8.19221449e-03  -7.25204640e-02   2.68760663e-01\n",
      "   1.13462890e-01   2.17996571e-01  -5.11359254e-02   1.90986791e-01\n",
      "  -5.45513020e-02   2.68651168e-01   7.68781022e-03  -1.59024377e-01\n",
      "  -7.56776628e-02  -1.63156055e-03   1.00838470e-01  -1.30380684e-01\n",
      "  -1.31863226e-01  -3.38731378e-03   3.69394436e-02  -3.21151499e-03\n",
      "  -5.17223552e-03  -4.17479430e-03   6.03867369e-02   2.70024609e-01\n",
      "  -3.98245656e-02   1.39673541e-02] \n",
      "After Regularization with lambda 0.6:\n",
      " Train MSE:0.258133792459, Valid MSE:0.258463503764\n",
      "\n",
      "_________________________Model Training for run 1 with lambda:0.6_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.6 is: \n",
      " [ -2.74257271e-02  -7.88000588e-05  -3.12008927e-03   2.73294742e-03\n",
      "   1.54075916e-02   1.63907183e-02  -8.87722108e-03  -9.23871250e-03\n",
      "   2.88698054e-03  -1.70117074e-02  -1.54754396e-03  -1.42078290e-02\n",
      "   4.50136919e-03  -4.27086686e-03  -1.35990435e-02  -1.30421509e-02\n",
      "   1.44233863e-03  -4.34726214e-03  -1.17161845e-03   6.48966479e-03\n",
      "  -5.29481833e-03   4.33478982e-02   4.23503767e-03  -2.92468320e-03\n",
      "  -6.44721261e-04   2.74792161e-03  -7.53832498e-04   1.31879328e-04\n",
      "  -1.32709973e-03  -1.78875058e-02  -1.45409096e-03  -4.03199795e-03\n",
      "  -1.20354715e-02  -2.19940455e-02  -1.09125453e-02  -8.51424613e-03\n",
      "   3.66638628e-03   3.11199511e-03   1.25615087e-02   7.82680914e-03\n",
      "  -1.65213453e-02   8.16932478e-03   7.72290080e-03   1.25755645e-02\n",
      "  -1.66650884e-03  -1.69736410e-02   2.56056022e-02   1.39458058e-02\n",
      "   5.72455987e-03  -1.45317966e-04  -4.44351116e-03   4.28925562e-02\n",
      "   2.66216719e-03  -3.51364842e-02  -2.99919694e-03   3.10127063e-03\n",
      "  -1.53762502e-03  -3.21682873e-03  -8.31612312e-03   5.92040725e-03\n",
      "   7.33604918e-03   2.98222304e-02  -9.89692332e-03  -2.20834959e-02\n",
      "   1.61259555e-03  -2.06113483e-03  -1.93423435e-02   8.14757923e-04\n",
      "  -5.94734512e-03  -2.20642580e-03   3.78018928e-02  -5.30458643e-03\n",
      "  -2.67637540e-03   6.93655063e-03   7.52908543e-03  -6.85096841e-03\n",
      "   8.03888590e-04   3.14872050e-03   4.91187442e-03  -1.74871504e-03\n",
      "   1.10829283e-03   1.55518760e-02  -1.88223060e-02  -4.44218968e-03\n",
      "   1.55726691e-02  -6.97228942e-03  -1.73034127e-02   5.34659269e-03\n",
      "   9.70720210e-03   2.40826254e-03  -1.37637360e-03  -5.84569915e-03\n",
      "   1.76681447e-02  -4.47151209e-03  -1.53143471e-02   6.11767012e-03\n",
      "  -5.52386221e-04   1.03134328e-02  -6.33836572e-02   3.26161624e-01\n",
      "   5.00887622e-02   2.04508924e-01  -4.98146948e-02   1.14387966e-01\n",
      "  -1.84730116e-02   3.26167293e-01   2.55559448e-02  -6.27340369e-02\n",
      "  -3.41632453e-02   4.40157090e-02   1.25099810e-01  -7.85658493e-02\n",
      "  -1.41858483e-01   7.88100443e-03   3.73697519e-02  -9.85751900e-03\n",
      "  -8.83350194e-03  -1.42310589e-03   5.01912584e-02   2.01063246e-01\n",
      "  -2.47870964e-02   7.58149294e-03] \n",
      "After Regularization with lambda 0.6:\n",
      " Train MSE:0.233455465169, Valid MSE:0.234565021614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 2 with lambda:0.6_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.6 is: \n",
      " [  6.56691390e-03   6.79230804e-05   4.22283768e-04  -6.00959897e-03\n",
      "   1.02388367e-02   2.18935950e-02  -1.40755500e-02  -9.95388171e-03\n",
      "   1.96105189e-02  -4.44526966e-02  -1.02453055e-02  -1.06819008e-02\n",
      "   8.72320281e-03  -3.71509094e-03  -2.75708998e-02   1.98694200e-03\n",
      "  -1.12237857e-05   4.92648699e-03  -1.15317672e-02   1.88309433e-02\n",
      "  -5.70105452e-03   3.14542451e-02   1.13947607e-02  -5.23174591e-03\n",
      "  -6.00268434e-04   3.53474189e-03   2.32406558e-03  -6.06398287e-04\n",
      "  -1.98281026e-03  -1.90363591e-02  -2.29540161e-02  -3.62807114e-03\n",
      "  -3.98158130e-03  -2.18816198e-02  -1.52641281e-02  -1.07919179e-02\n",
      "   6.64571086e-03   1.02821390e-02  -3.11165717e-03  -3.77017391e-03\n",
      "  -2.33145760e-02   2.18816001e-02   1.13573020e-02   3.88826376e-03\n",
      "  -8.41979826e-03  -2.98726564e-02   3.62619374e-02   2.61751944e-02\n",
      "   1.02611259e-02   3.47369592e-03  -4.03824519e-03   5.53333153e-02\n",
      "   1.67761314e-02  -8.24585184e-03  -3.30105679e-03  -1.61691596e-03\n",
      "   2.30309367e-04   2.07236452e-04  -6.11702934e-03  -1.15430038e-02\n",
      "   1.79779015e-02   2.24001596e-02   1.64753154e-03  -2.46567229e-02\n",
      "   1.25496736e-02  -1.96645340e-03  -6.45219912e-03  -1.25981624e-02\n",
      "   1.98423943e-03  -1.33236163e-02   2.79082912e-02  -1.01557666e-02\n",
      "  -5.22980682e-03   4.35712719e-03   4.67037127e-03  -6.63131091e-03\n",
      "   2.62873086e-03   1.74672357e-03   4.00552111e-03   2.03269642e-02\n",
      "  -5.67880841e-03   1.24260821e-02  -2.38581621e-03  -4.66610419e-03\n",
      "   1.59400115e-02  -6.11991552e-03  -1.67334768e-02   4.44457290e-04\n",
      "   1.11975519e-02  -5.04553580e-03   5.84182789e-03  -3.16866934e-02\n",
      "   1.95828139e-02   1.43287151e-02  -1.84538008e-02  -8.44438658e-03\n",
      "   2.11732990e-03   1.97353873e-02  -1.14889441e-01   2.78660320e-01\n",
      "   1.02954524e-01   2.65139876e-01  -3.32619535e-03   7.74138979e-02\n",
      "  -1.92117913e-02   2.78705958e-01   2.06948169e-02  -1.13102770e-01\n",
      "  -6.03000386e-02   2.12343000e-02   1.06784312e-01  -1.25568614e-01\n",
      "  -8.72023016e-02  -6.90856568e-03   4.30324955e-02  -1.09564749e-02\n",
      "  -6.88190136e-03  -2.18638465e-03   4.14408379e-02   2.30577940e-01\n",
      "  -4.09018974e-02   1.37878370e-02] \n",
      "After Regularization with lambda 0.6:\n",
      " Train MSE:0.234302239289, Valid MSE:0.234261022835\n",
      "\n",
      "_________________________Model Training for run 3 with lambda:0.6_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.6 is: \n",
      " [  1.27142978e-02  -2.01007370e-05   4.07633025e-03  -6.03916716e-03\n",
      "   1.21993632e-02   2.08446697e-02  -9.96711790e-03  -7.72429176e-03\n",
      "   1.25454950e-02  -2.90580182e-02  -1.33505841e-02  -1.95256145e-02\n",
      "   7.43241665e-03  -4.35370936e-03  -3.54459763e-02   4.64252959e-03\n",
      "   1.87601749e-03   6.19075403e-03  -8.98689095e-03   1.86660863e-02\n",
      "   2.86943121e-03   3.48301579e-02   1.61993022e-02  -7.89861184e-03\n",
      "  -2.63661636e-03   6.11034111e-03   2.50071122e-03  -6.26832768e-04\n",
      "  -5.01958647e-03   3.11468688e-03  -2.21878004e-02  -2.48181348e-03\n",
      "  -8.43698983e-03  -2.51816980e-02  -1.62300833e-02  -9.02731890e-03\n",
      "   9.98166603e-03   1.20883658e-02  -1.08010946e-03   2.61168833e-03\n",
      "  -3.51996792e-02   9.24530274e-03   6.44191284e-03   4.75648115e-03\n",
      "  -7.08775375e-03  -2.79881728e-02   1.93755443e-02   2.35107046e-02\n",
      "   8.73322347e-03   1.64925406e-03  -4.00697819e-03   1.79503433e-02\n",
      "   1.13733711e-02  -2.66984366e-02  -8.13019099e-03   9.05569764e-04\n",
      "   2.18939659e-03  -1.43648308e-03  -1.49334655e-02  -4.48504820e-03\n",
      "   3.16821131e-02   2.09994596e-02   3.72819667e-03  -2.70859595e-02\n",
      "   6.31957684e-03  -2.16244170e-03  -1.58140903e-02  -6.31810014e-03\n",
      "   3.46531154e-03  -9.81604785e-03   3.80293384e-02  -7.25732803e-04\n",
      "  -4.92565386e-03  -1.33386728e-02  -2.80222854e-04   2.17289544e-04\n",
      "   9.30429403e-03  -2.42696256e-03   4.42356520e-03   1.47530564e-02\n",
      "  -4.28109977e-03   2.21016399e-02  -2.19231080e-03  -1.24668388e-02\n",
      "   1.35928197e-02  -6.31015208e-03  -1.68401724e-02   1.40095248e-03\n",
      "   7.64884993e-03  -1.09518859e-03   6.73723639e-03  -8.52903279e-03\n",
      "   1.07547183e-02   3.13050318e-03  -1.58863837e-02  -6.37850896e-03\n",
      "  -1.04450658e-03   1.94877216e-02  -1.08329709e-01   2.98387397e-01\n",
      "   1.01867946e-01   2.45855548e-01  -4.36085620e-04   1.19796273e-01\n",
      "  -1.77561482e-02   2.98274645e-01   1.47942177e-02  -1.35966567e-01\n",
      "  -7.33135142e-02   1.64108491e-02   9.03513076e-02  -1.07993566e-01\n",
      "  -1.61269193e-01   1.24138554e-02   2.98469675e-02  -1.00136585e-02\n",
      "  -2.80446802e-03  -1.22923820e-03   4.84281803e-02   2.71602333e-01\n",
      "  -2.70711857e-02   9.53144086e-03] \n",
      "After Regularization with lambda 0.6:\n",
      " Train MSE:0.266383430552, Valid MSE:0.265992672356\n",
      "\n",
      "_________________________Model Training for run 4 with lambda:0.6_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.6 is: \n",
      " [  5.72803288e-03  -1.13737409e-04   7.63575392e-03  -1.61646465e-02\n",
      "   1.51073287e-02   1.68126373e-02  -7.56964183e-03  -5.77686449e-03\n",
      "  -1.51318790e-03  -1.95401401e-02  -3.24600072e-03  -2.00768988e-02\n",
      "   1.52600112e-02  -5.65487916e-03  -9.81443286e-03   8.56917241e-04\n",
      "   1.87126055e-03   1.55323140e-02  -1.23783671e-02   9.98137221e-03\n",
      "   3.09136335e-03   1.22413512e-02   1.42912914e-02  -9.14109781e-03\n",
      "  -4.47854718e-03   5.52309785e-03   6.38688811e-04  -1.45093572e-03\n",
      "  -6.67301523e-03  -3.25018408e-02  -2.44971603e-02  -4.07638318e-03\n",
      "  -7.43878529e-03  -2.25483866e-02  -1.28778789e-02  -1.52583112e-02\n",
      "   7.63645687e-03   1.26232251e-02  -4.94428542e-03  -4.02226271e-03\n",
      "  -2.98818331e-02   1.26073861e-02   7.20426303e-03   2.19022040e-03\n",
      "  -1.39354912e-03  -2.14335412e-02   1.25411393e-02   1.61584326e-02\n",
      "   5.80103638e-03   1.24300014e-03  -3.05029232e-03   1.64353701e-02\n",
      "   4.57341847e-03  -1.45129900e-02  -1.03480873e-02   3.01738079e-03\n",
      "   1.41860251e-03   3.52474028e-03   2.38986473e-04  -5.98005855e-03\n",
      "   1.13743728e-02   9.41874369e-03   7.27422803e-03  -8.17794973e-03\n",
      "   7.46557814e-03  -9.55514104e-03  -4.87700441e-03  -2.42808308e-03\n",
      "   5.43974530e-03  -1.14879954e-02   3.39771040e-02   2.44917144e-03\n",
      "  -3.89251275e-03   4.94383750e-04   4.11733549e-03   1.15468961e-02\n",
      "   9.19233233e-03  -2.52898990e-03   2.95020356e-03   1.22329881e-02\n",
      "  -3.75637861e-03  -3.04671025e-03  -8.59328302e-03   1.29080593e-02\n",
      "   1.02356022e-02  -6.20136684e-03  -1.00665348e-02  -2.92214190e-03\n",
      "   9.92337710e-03   1.80580814e-03   7.48332332e-03  -2.03384361e-02\n",
      "   2.11592617e-02   3.39214063e-03  -1.20824207e-02  -1.36759498e-02\n",
      "   6.37022766e-03   1.33439647e-02  -1.03485039e-01   3.04684250e-01\n",
      "   1.01472433e-01   2.40317199e-01  -2.41829622e-02   8.92037758e-02\n",
      "  -7.95116630e-03   3.04671649e-01   2.58709930e-03  -1.11177930e-01\n",
      "  -5.94359679e-02   1.40612924e-02   8.40797343e-02  -1.02777282e-01\n",
      "  -1.23642103e-01   1.13555575e-02   4.56312566e-02  -1.63630320e-02\n",
      "  -4.15811825e-04   9.30837547e-04   7.89673171e-02   2.36650715e-01\n",
      "  -4.15931655e-02   7.16277836e-03] \n",
      "After Regularization with lambda 0.6:\n",
      " Train MSE:0.24164410421, Valid MSE:0.241655004532\n",
      "Average validation error over 5 runs 0.139745774295 with lambda:0.6\n",
      "Average training error over 5 runs 0.139389994952 with lambda:0.6\n",
      "\n",
      "_________________________Model Training for run 0 with lambda:0.7_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.7 is: \n",
      " [  3.46573527e-02  -8.50186502e-05  -1.22682295e-04  -1.10922490e-02\n",
      "   1.73253668e-02   2.75340365e-02  -1.10862229e-02   4.82525844e-03\n",
      "   1.82970682e-02  -2.67159917e-02  -9.76665960e-03  -2.05506378e-03\n",
      "   8.17689667e-03  -2.26467041e-03  -3.13398102e-02   7.25351634e-03\n",
      "   3.66451729e-03   1.35691090e-02  -1.99085926e-02   2.25636108e-02\n",
      "  -7.63301742e-04   3.06417901e-02   2.15997707e-02  -1.56623708e-02\n",
      "  -1.19813537e-04   2.07101551e-03   1.65221484e-03  -9.95273523e-04\n",
      "  -1.03689320e-03   4.66741744e-04  -2.66073229e-02  -6.98128223e-03\n",
      "  -2.38791610e-03  -2.28168199e-02  -1.66261739e-02  -9.61604897e-03\n",
      "   5.22621886e-03   1.41763453e-02   8.00852615e-04   3.84842620e-03\n",
      "  -3.03563632e-02   1.02005278e-02   7.45408911e-03   4.72245297e-03\n",
      "   7.92547680e-03  -2.02741522e-02   1.17605144e-02   2.20906579e-02\n",
      "   4.15436958e-03   3.21503160e-03  -6.68423393e-03   1.12465859e-02\n",
      "   1.49361673e-02  -8.26801481e-03  -7.45228995e-03   2.49746815e-03\n",
      "   7.92944604e-04  -4.77746075e-03  -2.18071929e-02  -1.93125287e-04\n",
      "   2.51104501e-02   2.36651285e-02   1.00612037e-02  -2.48140674e-02\n",
      "   1.34107696e-02  -1.75800584e-02  -1.43770007e-02  -2.61527932e-03\n",
      "   6.29733483e-03  -8.60122214e-03   2.82677737e-02   7.14567457e-04\n",
      "  -3.50508235e-03  -4.29645373e-03   2.01422796e-03  -2.86459102e-03\n",
      "   6.72092769e-03  -1.93071166e-04  -2.96089709e-03   1.91619333e-02\n",
      "  -5.83033377e-03   1.54796549e-02  -2.62943601e-03  -1.03265497e-02\n",
      "   7.99291221e-03  -2.35746063e-03  -1.54287159e-02  -1.06558093e-03\n",
      "   8.66141325e-03   8.34276852e-03   4.05410287e-03  -3.50887951e-02\n",
      "   2.60745036e-02   1.40618634e-02  -1.11341337e-02  -3.77966228e-03\n",
      "   9.32956088e-04   8.39410864e-03  -6.36863694e-02   2.65597576e-01\n",
      "   1.04192099e-01   2.19032817e-01  -4.50300011e-02   1.90196183e-01\n",
      "  -5.47426415e-02   2.65493545e-01   6.13005635e-03  -1.50267419e-01\n",
      "  -7.65653519e-02  -1.61007256e-03   1.00240755e-01  -1.21655193e-01\n",
      "  -1.22323318e-01  -3.36239702e-03   3.71188689e-02  -4.19064513e-03\n",
      "  -5.23649955e-03  -4.07017683e-03   6.07638623e-02   2.39508410e-01\n",
      "  -4.10937971e-02   1.40019715e-02] \n",
      "After Regularization with lambda 0.7:\n",
      " Train MSE:0.278970107029, Valid MSE:0.279287967845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 1 with lambda:0.7_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.7 is: \n",
      " [ -2.36028464e-02  -7.93287572e-05  -3.34464644e-03   2.32291822e-03\n",
      "   1.52406504e-02   1.60209944e-02  -8.78135189e-03  -8.87931593e-03\n",
      "   2.75514769e-03  -1.56948238e-02  -2.07589304e-03  -1.34895069e-02\n",
      "   3.44910925e-03  -4.32937232e-03  -1.15638190e-02  -1.29967094e-02\n",
      "   1.48929915e-03  -3.87402383e-03  -1.39770586e-03   6.62381005e-03\n",
      "  -5.37528474e-03   4.06516377e-02   4.57823509e-03  -2.94734781e-03\n",
      "  -7.70053465e-04   2.81736886e-03  -6.04775707e-04   1.49043755e-04\n",
      "  -1.25764648e-03  -1.56863447e-02  -1.96986985e-03  -4.09665216e-03\n",
      "  -1.17340009e-02  -2.09611507e-02  -1.11371922e-02  -8.66443922e-03\n",
      "   3.66224174e-03   3.12899866e-03   1.23155976e-02   6.94192411e-03\n",
      "  -1.54714272e-02   8.35859323e-03   8.16544532e-03   1.14858197e-02\n",
      "  -1.82629485e-03  -1.52641310e-02   2.35295582e-02   1.37911700e-02\n",
      "   5.53346380e-03  -2.29787211e-04  -4.51261653e-03   3.80964098e-02\n",
      "   2.52648595e-03  -3.27162722e-02  -3.18226423e-03   2.97343995e-03\n",
      "  -1.52665448e-03  -2.98176512e-03  -7.37529338e-03   5.74181496e-03\n",
      "   7.71423928e-03   2.79118342e-02  -9.07742179e-03  -2.13629502e-02\n",
      "   1.88753026e-03  -2.31643090e-03  -1.76886175e-02   8.62143468e-04\n",
      "  -6.02906493e-03  -2.01924793e-03   3.64361692e-02  -4.86280294e-03\n",
      "  -2.59433377e-03   6.98275488e-03   7.46488590e-03  -6.88354348e-03\n",
      "   7.79348490e-04   3.09839425e-03   4.79817815e-03  -1.90382285e-03\n",
      "   1.05657989e-03   1.39390885e-02  -1.68148561e-02  -4.90215613e-03\n",
      "   1.50972383e-02  -6.18317138e-03  -1.69991879e-02   4.44305125e-03\n",
      "   9.90077802e-03   2.30204826e-03  -1.33128212e-03  -4.40554300e-03\n",
      "   1.76973245e-02  -3.71588485e-03  -1.54526885e-02   5.62918487e-03\n",
      "  -3.15059449e-04   1.03676890e-02  -5.62140562e-02   3.18806853e-01\n",
      "   4.64204193e-02   2.09828821e-01  -4.54769987e-02   1.18125472e-01\n",
      "  -2.14364169e-02   3.18803225e-01   2.45968125e-02  -5.90677617e-02\n",
      "  -3.62835900e-02   4.08429579e-02   1.24238381e-01  -7.22143525e-02\n",
      "  -1.31533154e-01   7.89150956e-03   3.78481223e-02  -1.04532454e-02\n",
      "  -8.91607276e-03  -1.35216333e-03   5.09490051e-02   1.78801914e-01\n",
      "  -2.77368802e-02   7.72430727e-03] \n",
      "After Regularization with lambda 0.7:\n",
      " Train MSE:0.256414673007, Valid MSE:0.257523812051\n",
      "\n",
      "_________________________Model Training for run 2 with lambda:0.7_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.7 is: \n",
      " [  8.25534880e-03   7.53615411e-05   1.45736204e-04  -5.81717789e-03\n",
      "   1.01029532e-02   2.19823889e-02  -1.37279035e-02  -9.58549547e-03\n",
      "   1.89152565e-02  -4.11350707e-02  -1.11515871e-02  -9.74290827e-03\n",
      "   7.57810985e-03  -3.77152182e-03  -2.46920611e-02   1.63501207e-03\n",
      "   6.33782394e-05   5.25638917e-03  -1.09352034e-02   1.87018566e-02\n",
      "  -5.72220139e-03   2.92159829e-02   1.11435155e-02  -4.77033605e-03\n",
      "  -6.48156437e-04   3.61777568e-03   2.43818217e-03  -5.47602000e-04\n",
      "  -2.00183796e-03  -1.69322083e-02  -2.25177281e-02  -3.66799359e-03\n",
      "  -4.12197793e-03  -2.12885097e-02  -1.54476447e-02  -1.04020665e-02\n",
      "   6.54436558e-03   1.01611688e-02  -2.96836541e-03  -3.86584506e-03\n",
      "  -2.20278267e-02   2.17739523e-02   1.13910457e-02   3.44891636e-03\n",
      "  -7.60211956e-03  -2.73852644e-02   3.35990948e-02   2.59537361e-02\n",
      "   1.02203626e-02   3.39055171e-03  -4.04922302e-03   5.03201731e-02\n",
      "   1.69582540e-02  -5.88565923e-03  -3.29576054e-03  -2.07384037e-03\n",
      "   3.32351639e-04   2.68736253e-04  -5.60566705e-03  -1.03052577e-02\n",
      "   1.67836976e-02   2.15179859e-02   2.45566470e-03  -2.39301216e-02\n",
      "   1.21075900e-02  -2.10333836e-03  -6.41992858e-03  -1.20001956e-02\n",
      "   2.33587493e-03  -1.28369970e-02   2.68177949e-02  -9.42221992e-03\n",
      "  -5.07033943e-03   4.79849706e-03   4.79348918e-03  -6.95805753e-03\n",
      "   2.45885306e-03   1.73738201e-03   4.08900695e-03   2.00004132e-02\n",
      "  -5.70774870e-03   1.10108499e-02  -2.10939749e-03  -3.94017221e-03\n",
      "   1.54333867e-02  -5.86723519e-03  -1.65002401e-02   1.14040161e-04\n",
      "   1.14386316e-02  -4.98876783e-03   5.78062502e-03  -3.02447875e-02\n",
      "   1.97067011e-02   1.47110783e-02  -1.85248740e-02  -8.43119796e-03\n",
      "   2.45219456e-03   1.96898230e-02  -1.00337593e-01   2.75041992e-01\n",
      "   9.44430382e-02   2.60670241e-01  -2.73462661e-03   8.28368018e-02\n",
      "  -2.22135440e-02   2.75065833e-01   1.95005419e-02  -1.06099446e-01\n",
      "  -6.16833088e-02   1.92656883e-02   1.05976010e-01  -1.16909321e-01\n",
      "  -8.11758576e-02  -6.70626173e-03   4.37614760e-02  -1.13646316e-02\n",
      "  -6.88946094e-03  -2.15624946e-03   4.09869621e-02   2.05751531e-01\n",
      "  -4.27001897e-02   1.37157616e-02] \n",
      "After Regularization with lambda 0.7:\n",
      " Train MSE:0.253822444932, Valid MSE:0.253788220805\n",
      "\n",
      "_________________________Model Training for run 3 with lambda:0.7_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.7 is: \n",
      " [  1.34319577e-02  -1.15856182e-05   3.32510799e-03  -6.03464521e-03\n",
      "   1.19944953e-02   2.06762346e-02  -9.79604503e-03  -7.24928840e-03\n",
      "   1.18522199e-02  -2.68386450e-02  -1.37443823e-02  -1.81733529e-02\n",
      "   6.51418156e-03  -4.39315986e-03  -3.19425691e-02   4.41199929e-03\n",
      "   2.01585347e-03   6.54214670e-03  -8.48509063e-03   1.87638816e-02\n",
      "   2.76065582e-03   3.25234799e-02   1.55098314e-02  -7.72258715e-03\n",
      "  -2.69328036e-03   6.14677031e-03   2.68331358e-03  -5.95452441e-04\n",
      "  -4.98413724e-03   3.31008020e-03  -2.15634441e-02  -2.76537831e-03\n",
      "  -8.31328471e-03  -2.40671128e-02  -1.62090661e-02  -8.56715011e-03\n",
      "   9.84251955e-03   1.20807602e-02  -9.51007544e-04   2.02947663e-03\n",
      "  -3.34348514e-02   9.36606430e-03   6.95926662e-03   3.71997450e-03\n",
      "  -6.88409233e-03  -2.54949405e-02   1.74140447e-02   2.32466015e-02\n",
      "   8.36786912e-03   1.55678577e-03  -3.95693747e-03   1.56620481e-02\n",
      "   1.12037797e-02  -2.35080682e-02  -8.28661877e-03   6.41977114e-04\n",
      "   2.38517850e-03  -1.38920261e-03  -1.36044948e-02  -3.72936360e-03\n",
      "   2.94195888e-02   2.06905146e-02   4.76746575e-03  -2.64002996e-02\n",
      "   6.22471786e-03  -2.45207186e-03  -1.46401916e-02  -6.00990133e-03\n",
      "   3.78911000e-03  -8.94098965e-03   3.67061248e-02  -1.80471985e-04\n",
      "  -4.84834089e-03  -1.23291410e-02  -6.26204941e-05  -7.22063034e-04\n",
      "   9.29403039e-03  -2.46991493e-03   4.55084630e-03   1.44053904e-02\n",
      "  -4.21079799e-03   2.07757053e-02  -1.68132562e-03  -1.19409670e-02\n",
      "   1.32098715e-02  -5.72419414e-03  -1.67679463e-02   8.14360352e-04\n",
      "   7.83483579e-03  -1.32255347e-03   6.71214724e-03  -7.73135966e-03\n",
      "   1.09924502e-02   3.84562746e-03  -1.58922016e-02  -6.49728639e-03\n",
      "  -8.27821764e-04   1.94183736e-02  -9.49255358e-02   2.93808045e-01\n",
      "   9.49479988e-02   2.45138238e-01  -4.58667833e-04   1.24482199e-01\n",
      "  -2.00143748e-02   2.93693647e-01   1.34846164e-02  -1.29790171e-01\n",
      "  -7.41903688e-02   1.56219803e-02   8.96723103e-02  -1.01417548e-01\n",
      "  -1.49278616e-01   1.23246783e-02   3.04124297e-02  -1.07400516e-02\n",
      "  -2.86960100e-03  -1.16444447e-03   4.87662366e-02   2.42396101e-01\n",
      "  -2.92745233e-02   9.50819466e-03] \n",
      "After Regularization with lambda 0.7:\n",
      " Train MSE:0.288241041315, Valid MSE:0.287838198232\n",
      "\n",
      "_________________________Model Training for run 4 with lambda:0.7_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.7 is: \n",
      " [  7.50554350e-03  -1.06791106e-04   7.07710425e-03  -1.54363514e-02\n",
      "   1.48722321e-02   1.67786558e-02  -7.42376748e-03  -5.25944669e-03\n",
      "  -1.30082470e-03  -1.78987886e-02  -4.15873144e-03  -1.90045623e-02\n",
      "   1.37959690e-02  -5.66711244e-03  -8.62214806e-03   1.09937235e-03\n",
      "   1.91657828e-03   1.56756721e-02  -1.18034573e-02   1.01533316e-02\n",
      "   3.28928859e-03   1.14851267e-02   1.36359850e-02  -8.23312640e-03\n",
      "  -4.57454391e-03   5.61381799e-03   8.14341847e-04  -1.48581934e-03\n",
      "  -6.70956748e-03  -3.01094272e-02  -2.41248460e-02  -4.24297612e-03\n",
      "  -7.28006029e-03  -2.18403784e-02  -1.29550903e-02  -1.43870120e-02\n",
      "   7.57759746e-03   1.26504048e-02  -4.80040686e-03  -4.24351067e-03\n",
      "  -2.84538665e-02   1.24439347e-02   7.45628030e-03   1.49761335e-03\n",
      "  -1.30368791e-03  -1.98344013e-02   1.12264305e-02   1.60433373e-02\n",
      "   5.68055011e-03   1.14500560e-03  -3.16619311e-03   1.23141014e-02\n",
      "   4.83340234e-03  -1.18049100e-02  -1.02995026e-02   2.65083695e-03\n",
      "   1.69407257e-03   3.38340225e-03   4.05364647e-04  -5.19966108e-03\n",
      "   1.05953595e-02   9.39706319e-03   7.97153491e-03  -7.96202472e-03\n",
      "   6.89678708e-03  -9.04038548e-03  -4.57879386e-03  -2.76818102e-03\n",
      "   5.76530322e-03  -1.00189795e-02   3.29787806e-02   2.90314963e-03\n",
      "  -3.81578182e-03   3.61463763e-04   4.11796582e-03   1.00636266e-02\n",
      "   9.22498148e-03  -2.53546358e-03   3.15929595e-03   1.19505917e-02\n",
      "  -3.76169132e-03  -3.18253103e-03  -7.54290259e-03   1.17097868e-02\n",
      "   9.95171459e-03  -5.94252123e-03  -1.00937057e-02  -3.17484497e-03\n",
      "   1.01772047e-02   1.83259325e-03   7.46130890e-03  -1.88746291e-02\n",
      "   2.13824327e-02   3.56266280e-03  -1.21095565e-02  -1.34598039e-02\n",
      "   6.31915776e-03   1.33192790e-02  -9.07278249e-02   2.99694165e-01\n",
      "   9.38217987e-02   2.39642764e-01  -2.18283101e-02   9.53987258e-02\n",
      "  -1.15523079e-02   2.99682301e-01   1.47141766e-03  -1.05448415e-01\n",
      "  -6.02041992e-02   1.29262250e-02   8.35919190e-02  -9.65288592e-02\n",
      "  -1.14202070e-01   1.16323610e-02   4.62857196e-02  -1.64598583e-02\n",
      "  -3.55804161e-04   9.89512492e-04   7.80895712e-02   2.10698547e-01\n",
      "  -4.36212619e-02   7.28355526e-03] \n",
      "After Regularization with lambda 0.7:\n",
      " Train MSE:0.263205102558, Valid MSE:0.26321653364\n",
      "Average validation error over 5 runs 0.154033015652 with lambda:0.7\n",
      "Average training error over 5 runs 0.153694514821 with lambda:0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 0 with lambda:0.8_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.8 is: \n",
      " [  3.33373163e-02  -8.28759932e-05   9.35349127e-05  -1.09660324e-02\n",
      "   1.71704720e-02   2.77262570e-02  -1.09423960e-02   4.92917730e-03\n",
      "   1.72330930e-02  -2.46506815e-02  -9.78373637e-03  -1.84626008e-03\n",
      "   7.53199249e-03  -2.31957226e-03  -2.85287789e-02   7.14188462e-03\n",
      "   3.74632320e-03   1.37545575e-02  -1.86667673e-02   2.26665455e-02\n",
      "  -5.84649493e-04   2.87004031e-02   1.99743566e-02  -1.41213913e-02\n",
      "  -2.03788607e-05   2.16798500e-03   1.81476724e-03  -1.01780080e-03\n",
      "  -1.03384332e-03   8.40764898e-04  -2.58831188e-02  -6.91968046e-03\n",
      "  -2.57168523e-03  -2.21324649e-02  -1.65882119e-02  -8.90863018e-03\n",
      "   5.12155075e-03   1.40913658e-02   8.91282348e-04   3.65060487e-03\n",
      "  -2.90243019e-02   1.01198989e-02   7.76135992e-03   4.05320216e-03\n",
      "   7.06789133e-03  -1.87841237e-02   1.05558559e-02   2.18310133e-02\n",
      "   3.98659256e-03   3.16819235e-03  -6.63644559e-03   9.17137755e-03\n",
      "   1.49444572e-02  -6.50649854e-03  -7.73682302e-03   2.24040775e-03\n",
      "   1.04841165e-03  -4.63666283e-03  -2.01975552e-02   6.65478613e-05\n",
      "   2.39691315e-02   2.30902102e-02   1.07041973e-02  -2.41625493e-02\n",
      "   1.24034616e-02  -1.63457154e-02  -1.33666763e-02  -2.16171149e-03\n",
      "   6.38441004e-03  -8.00344640e-03   2.73099910e-02   1.42026033e-03\n",
      "  -3.30971858e-03  -4.62944826e-03   1.83863947e-03  -3.32075592e-03\n",
      "   6.81589298e-03  -1.20524456e-04  -2.83532760e-03   1.88659777e-02\n",
      "  -5.74962740e-03   1.43360384e-02  -2.28450036e-03  -9.78843459e-03\n",
      "   7.67631364e-03  -2.19533076e-03  -1.53590123e-02  -1.16823847e-03\n",
      "   8.86936395e-03   8.21103836e-03   3.96179850e-03  -3.36473119e-02\n",
      "   2.54877333e-02   1.41585624e-02  -1.12385445e-02  -3.87443978e-03\n",
      "   1.03898823e-03   8.52535219e-03  -5.68855221e-02   2.62757740e-01\n",
      "   9.65768243e-02   2.19555798e-01  -4.03151387e-02   1.89350304e-01\n",
      "  -5.46402865e-02   2.62658108e-01   4.78633832e-03  -1.42425663e-01\n",
      "  -7.69418355e-02  -1.29434263e-03   9.95110597e-02  -1.14231738e-01\n",
      "  -1.14074480e-01  -3.30989068e-03   3.71756152e-02  -5.04882664e-03\n",
      "  -5.28744203e-03  -3.96939681e-03   6.09797194e-02   2.15415592e-01\n",
      "  -4.21546812e-02   1.40119597e-02] \n",
      "After Regularization with lambda 0.8:\n",
      " Train MSE:0.299289876887, Valid MSE:0.299597835395\n",
      "\n",
      "_________________________Model Training for run 1 with lambda:0.8_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.8 is: \n",
      " [ -2.05963141e-02  -7.96638783e-05  -3.52217229e-03   1.98811987e-03\n",
      "   1.50465205e-02   1.56846844e-02  -8.69600478e-03  -8.55241937e-03\n",
      "   2.59823505e-03  -1.45907915e-02  -2.43232353e-03  -1.28152285e-02\n",
      "   2.59733608e-03  -4.37946592e-03  -9.93500031e-03  -1.28721418e-02\n",
      "   1.53482159e-03  -3.42299678e-03  -1.54894842e-03   6.71473559e-03\n",
      "  -5.44354924e-03   3.83357120e-02   4.82835098e-03  -2.92222173e-03\n",
      "  -8.86158794e-04   2.88545550e-03  -4.59101868e-04   1.66072740e-04\n",
      "  -1.19588128e-03  -1.39392134e-02  -2.39811388e-03  -4.15768750e-03\n",
      "  -1.14540323e-02  -2.00446743e-02  -1.13275856e-02  -8.75796100e-03\n",
      "   3.65493109e-03   3.14366814e-03   1.20869184e-02   6.18900509e-03\n",
      "  -1.45532117e-02   8.47028956e-03   8.46654926e-03   1.06444634e-02\n",
      "  -1.91161374e-03  -1.38255069e-02   2.17003696e-02   1.36196978e-02\n",
      "   5.34307814e-03  -3.04668087e-04  -4.59100314e-03   3.41167319e-02\n",
      "   2.40636249e-03  -3.05946019e-02  -3.34408892e-03   2.84258777e-03\n",
      "  -1.50804965e-03  -2.77412901e-03  -6.53317305e-03   5.60896894e-03\n",
      "   7.92726249e-03   2.63130028e-02  -8.33914721e-03  -2.06752762e-02\n",
      "   2.08143256e-03  -2.47561716e-03  -1.63003338e-02   9.49824442e-04\n",
      "  -6.08240417e-03  -1.88101745e-03   3.51489410e-02  -4.46874762e-03\n",
      "  -2.51897194e-03   6.92271880e-03   7.39081385e-03  -6.84957464e-03\n",
      "   7.69441432e-04   3.04624770e-03   4.71002202e-03  -2.04610589e-03\n",
      "   1.01855530e-03   1.26478547e-02  -1.52155692e-02  -5.22556043e-03\n",
      "   1.46549519e-02  -5.54094506e-03  -1.66856535e-02   3.69278780e-03\n",
      "   1.00441033e-02   2.21323304e-03  -1.29489304e-03  -3.25349948e-03\n",
      "   1.76775718e-02  -3.04016169e-03  -1.55521316e-02   5.20696253e-03\n",
      "  -1.23090475e-04   1.03804687e-02  -5.07166178e-02   3.12616942e-01\n",
      "   4.34537564e-02   2.13499272e-01  -4.19798303e-02   1.21242463e-01\n",
      "  -2.38174399e-02   3.12606591e-01   2.37446806e-02  -5.58019530e-02\n",
      "  -3.77888450e-02   3.83326623e-02   1.23210320e-01  -6.69627924e-02\n",
      "  -1.22545062e-01   7.90315216e-03   3.81805834e-02  -1.09822817e-02\n",
      "  -8.99001703e-03  -1.28197332e-03   5.15092739e-02   1.61105577e-01\n",
      "  -3.01333168e-02   7.84744916e-03] \n",
      "After Regularization with lambda 0.8:\n",
      " Train MSE:0.278851096379, Valid MSE:0.27995887785\n",
      "\n",
      "_________________________Model Training for run 2 with lambda:0.8_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.8 is: \n",
      " [  9.42084916e-03   8.21783496e-05  -8.45096633e-05  -5.62767626e-03\n",
      "   9.96533117e-03   2.20453866e-02  -1.34207064e-02  -9.27827669e-03\n",
      "   1.81570201e-02  -3.83047120e-02  -1.16845445e-02  -8.90701305e-03\n",
      "   6.63160287e-03  -3.82041485e-03  -2.23379172e-02   1.34576223e-03\n",
      "   1.33460489e-04   5.53656342e-03  -1.03942033e-02   1.85402742e-02\n",
      "  -5.72307770e-03   2.73376150e-02   1.09025832e-02  -4.31267215e-03\n",
      "  -6.98966763e-04   3.69358013e-03   2.54286252e-03  -4.89181813e-04\n",
      "  -2.02336369e-03  -1.52851628e-02  -2.20687046e-02  -3.71023276e-03\n",
      "  -4.21848142e-03  -2.07564790e-02  -1.56064670e-02  -1.00409430e-02\n",
      "   6.45309637e-03   1.00465054e-02  -2.83674062e-03  -3.91085614e-03\n",
      "  -2.08590378e-02   2.15826734e-02   1.13477357e-02   3.13802144e-03\n",
      "  -6.87974090e-03  -2.52891562e-02   3.12721162e-02   2.57175373e-02\n",
      "   1.01581461e-02   3.32838415e-03  -4.06981689e-03   4.61310706e-02\n",
      "   1.70716817e-02  -3.99279711e-03  -3.29249107e-03  -2.47546937e-03\n",
      "   4.08347141e-04   3.32597209e-04  -5.12796611e-03  -9.27541407e-03\n",
      "   1.58258264e-02   2.07215094e-02   3.14510361e-03  -2.32102817e-02\n",
      "   1.16894016e-02  -2.19586772e-03  -6.31293560e-03  -1.14030318e-02\n",
      "   2.60868227e-03  -1.24362148e-02   2.58088087e-02  -8.75166165e-03\n",
      "  -4.92295486e-03   5.10487667e-03   4.87543593e-03  -7.16590036e-03\n",
      "   2.31059933e-03   1.72968660e-03   4.17229977e-03   1.96738305e-02\n",
      "  -5.72835986e-03   9.79431908e-03  -1.91500890e-03  -3.26452490e-03\n",
      "   1.49643026e-02  -5.62837257e-03  -1.62504152e-02  -1.65774996e-04\n",
      "   1.16188244e-02  -4.92722293e-03   5.72095736e-03  -2.89123129e-02\n",
      "   1.97355473e-02   1.49795503e-02  -1.85434433e-02  -8.39372267e-03\n",
      "   2.73522288e-03   1.95899527e-02  -8.89531455e-02   2.71699571e-01\n",
      "   8.72681060e-02   2.56796183e-01  -2.31197189e-03   8.71803240e-02\n",
      "  -2.45753457e-02   2.71708147e-01   1.84892352e-02  -9.98809720e-02\n",
      "  -6.25521831e-02   1.78222147e-02   1.05071538e-01  -1.09534785e-01\n",
      "  -7.57251824e-02  -6.54835635e-03   4.43096305e-02  -1.17406021e-02\n",
      "  -6.86278562e-03  -2.13110994e-03   4.05527835e-02   1.85977468e-01\n",
      "  -4.42743482e-02   1.36464338e-02] \n",
      "After Regularization with lambda 0.8:\n",
      " Train MSE:0.272733728297, Valid MSE:0.272706576911\n",
      "\n",
      "_________________________Model Training for run 3 with lambda:0.8_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.8 is: \n",
      " [  1.38731703e-02  -3.84718137e-06   2.72367245e-03  -5.98689629e-03\n",
      "   1.17725412e-02   2.05123887e-02  -9.63500667e-03  -6.84695898e-03\n",
      "   1.11146245e-02  -2.49540408e-02  -1.38853824e-02  -1.69734680e-02\n",
      "   5.73163443e-03  -4.42360319e-03  -2.90462831e-02   4.20359917e-03\n",
      "   2.14346345e-03   6.84057686e-03  -8.03359054e-03   1.87830714e-02\n",
      "   2.66440557e-03   3.05498943e-02   1.48419089e-02  -7.45671039e-03\n",
      "  -2.74179912e-03   6.18182698e-03   2.85439462e-03  -5.60185037e-04\n",
      "  -4.95186206e-03   3.37886296e-03  -2.09866545e-02  -3.02623078e-03\n",
      "  -8.19784921e-03  -2.30897786e-02  -1.61901975e-02  -8.16584769e-03\n",
      "   9.71576726e-03   1.20572966e-02  -8.23902390e-04   1.56081645e-03\n",
      "  -3.18811435e-02   9.41214533e-03   7.29286964e-03   2.95161392e-03\n",
      "  -6.65282832e-03  -2.34018606e-02   1.56948416e-02   2.29672800e-02\n",
      "   8.01427489e-03   1.47844917e-03  -3.91421358e-03   1.37631470e-02\n",
      "   1.10277570e-02  -2.08730271e-02  -8.41775440e-03   4.09119390e-04\n",
      "   2.52952508e-03  -1.32789651e-03  -1.24299400e-02  -3.09224968e-03\n",
      "   2.75727392e-02   2.02945881e-02   5.64275372e-03  -2.57165705e-02\n",
      "   6.05996978e-03  -2.63729558e-03  -1.36077560e-02  -5.65056452e-03\n",
      "   4.03642548e-03  -8.26787370e-03   3.54911408e-02   3.02600686e-04\n",
      "  -4.77551157e-03  -1.15219489e-02   1.15498843e-04  -1.39169419e-03\n",
      "   9.28507856e-03  -2.50727372e-03   4.67401944e-03   1.40791571e-02\n",
      "  -4.14355865e-03   1.95945606e-02  -1.29249587e-03  -1.13899224e-02\n",
      "   1.28500429e-02  -5.24226882e-03  -1.66337071e-02   3.24757965e-04\n",
      "   7.97109429e-03  -1.51132528e-03   6.67532315e-03  -7.04469496e-03\n",
      "   1.11455574e-02   4.43243807e-03  -1.58647997e-02  -6.57792670e-03\n",
      "  -6.45879815e-04   1.93011068e-02  -8.45816279e-02   2.89781774e-01\n",
      "   8.90019459e-02   2.44264529e-01  -6.54277344e-04   1.28289688e-01\n",
      "  -2.17675623e-02   2.89668097e-01   1.23428677e-02  -1.24166236e-01\n",
      "  -7.46447838e-02   1.51274337e-02   8.88939585e-02  -9.57405331e-02\n",
      "  -1.38825641e-01   1.22082044e-02   3.08492537e-02  -1.13804660e-02\n",
      "  -2.91532664e-03  -1.09754902e-03   4.89881952e-02   2.18984881e-01\n",
      "  -3.11438154e-02   9.47432513e-03] \n",
      "After Regularization with lambda 0.8:\n",
      " Train MSE:0.309472557673, Valid MSE:0.309059069107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 4 with lambda:0.8_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.8 is: \n",
      " [  8.78116422e-03  -1.00205157e-04   6.57851636e-03  -1.47584019e-02\n",
      "   1.46150569e-02   1.67154543e-02  -7.28690021e-03  -4.80584910e-03\n",
      "  -1.23124027e-03  -1.65182689e-02  -4.77597352e-03  -1.80421910e-02\n",
      "   1.25750812e-02  -5.67591538e-03  -7.65137772e-03   1.32268123e-03\n",
      "   1.96047929e-03   1.57773018e-02  -1.12729014e-02   1.02559885e-02\n",
      "   3.47266957e-03   1.08295352e-02   1.30577035e-02  -7.44767862e-03\n",
      "  -4.66283902e-03   5.69883012e-03   9.80802582e-04  -1.51211933e-03\n",
      "  -6.73596237e-03  -2.81598447e-02  -2.37416768e-02  -4.37942495e-03\n",
      "  -7.14927347e-03  -2.12004878e-02  -1.30202264e-02  -1.36147970e-02\n",
      "   7.51626665e-03   1.26572393e-02  -4.65463558e-03  -4.38629504e-03\n",
      "  -2.71715627e-02   1.22381298e-02   7.57111503e-03   9.93724272e-04\n",
      "  -1.22674494e-03  -1.84538561e-02   1.00367145e-02   1.59090319e-02\n",
      "   5.54551612e-03   1.06314229e-03  -3.26542023e-03   8.98692570e-03\n",
      "   5.04039790e-03  -9.63140827e-03  -1.02466787e-02   2.32597509e-03\n",
      "   1.92782747e-03   3.26284775e-03   5.56577842e-04  -4.53811823e-03\n",
      "   9.98275047e-03   9.32552253e-03   8.57513203e-03  -7.72195544e-03\n",
      "   6.40983170e-03  -8.58929720e-03  -4.30959182e-03  -2.97410952e-03\n",
      "   6.01000561e-03  -8.87275513e-03   3.20552483e-02   3.30827010e-03\n",
      "  -3.74128156e-03   1.83902141e-04   4.10618889e-03   8.95192843e-03\n",
      "   9.24710346e-03  -2.54118469e-03   3.35081318e-03   1.16684372e-02\n",
      "  -3.76134199e-03  -3.29681680e-03  -6.72245183e-03   1.07889642e-02\n",
      "   9.68924920e-03  -5.72326869e-03  -1.00832624e-02  -3.39249370e-03\n",
      "   1.03684716e-02   1.85970314e-03   7.43583017e-03  -1.76003800e-02\n",
      "   2.14961602e-02   3.68784348e-03  -1.21085749e-02  -1.32550121e-02\n",
      "   6.25864796e-03   1.32623636e-02  -8.07756648e-02   2.95319019e-01\n",
      "   8.73430806e-02   2.38796068e-01  -1.99712159e-02   1.00480817e-01\n",
      "  -1.44761200e-02   2.95307734e-01   5.30371476e-04  -1.00292157e-01\n",
      "  -6.06263054e-02   1.21190573e-02   8.29582139e-02  -9.11388408e-02\n",
      "  -1.05931488e-01   1.17970370e-02   4.67536160e-02  -1.65379755e-02\n",
      "  -2.89169386e-04   1.04734387e-03   7.71381026e-02   1.90148407e-01\n",
      "  -4.52992159e-02   7.39762748e-03] \n",
      "After Regularization with lambda 0.8:\n",
      " Train MSE:0.284303941234, Valid MSE:0.284315726016\n",
      "Average validation error over 5 runs 0.167542475793 with lambda:0.8\n",
      "Average training error over 5 runs 0.167218087348 with lambda:0.8\n",
      "\n",
      "_________________________Model Training for run 0 with lambda:0.9_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.9 is: \n",
      " [  3.20555159e-02  -8.04146081e-05   2.01611899e-04  -1.07960381e-02\n",
      "   1.69689799e-02   2.78251832e-02  -1.08256845e-02   4.96621095e-03\n",
      "   1.61927062e-02  -2.28703100e-02  -9.66855654e-03  -1.68937165e-03\n",
      "   6.94417006e-03  -2.36899852e-03  -2.61715920e-02   7.02027893e-03\n",
      "   3.82024380e-03   1.38670625e-02  -1.75574857e-02   2.27012504e-02\n",
      "  -4.20639453e-04   2.70312829e-02   1.86095786e-02  -1.27771913e-02\n",
      "   6.02462814e-05   2.26167435e-03   1.96940577e-03  -1.03400094e-03\n",
      "  -1.02795979e-03   1.05590984e-03  -2.52143672e-02  -6.86837351e-03\n",
      "  -2.72162860e-03  -2.15038521e-02  -1.65723306e-02  -8.28624964e-03\n",
      "   5.02454625e-03   1.39940720e-02   9.68336631e-04   3.48149070e-03\n",
      "  -2.78178045e-02   9.99856290e-03   7.94509491e-03   3.51820106e-03\n",
      "   6.35118590e-03  -1.75016620e-02   9.44386014e-03   2.15437170e-02\n",
      "   3.80732535e-03   3.13364076e-03  -6.59788279e-03   7.47618807e-03\n",
      "   1.49015453e-02  -5.04849110e-03  -7.98740150e-03   2.01151962e-03\n",
      "   1.26199999e-03  -4.49744238e-03  -1.87552010e-02   3.20397062e-04\n",
      "   2.29677394e-02   2.25222267e-02   1.12126266e-02  -2.35383504e-02\n",
      "   1.15545258e-02  -1.52825439e-02  -1.24763678e-02  -1.75087036e-03\n",
      "   6.42801300e-03  -7.51301138e-03   2.64284435e-02   2.02104209e-03\n",
      "  -3.14540566e-03  -4.89764314e-03   1.68537616e-03  -3.64557146e-03\n",
      "   6.89800675e-03  -6.06276397e-05  -2.71504090e-03   1.85673484e-02\n",
      "  -5.67203179e-03   1.33301229e-02  -2.03908828e-03  -9.23789980e-03\n",
      "   7.39593439e-03  -2.05950785e-03  -1.52529060e-02  -1.27096177e-03\n",
      "   9.02895576e-03   8.09262018e-03   3.87200363e-03  -3.22833248e-02\n",
      "   2.49061420e-02   1.42149537e-02  -1.13103598e-02  -3.94384801e-03\n",
      "   1.11955931e-03   8.60324181e-03  -5.14570710e-02   2.60159417e-01\n",
      "   9.01858014e-02   2.19714716e-01  -3.65604014e-02   1.88471209e-01\n",
      "  -5.43485929e-02   2.60063523e-01   3.61808472e-03  -1.35361189e-01\n",
      "  -7.69733191e-02  -8.14337905e-04   9.86967132e-02  -1.07816875e-01\n",
      "  -1.06858529e-01  -3.23385982e-03   3.71446589e-02  -5.81494103e-03\n",
      "  -5.32850803e-03  -3.87270472e-03   6.10826035e-02   1.95911947e-01\n",
      "  -4.30549705e-02   1.40042382e-02] \n",
      "After Regularization with lambda 0.9:\n",
      " Train MSE:0.319121525663, Valid MSE:0.319421072634\n",
      "\n",
      "_________________________Model Training for run 1 with lambda:0.9_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.9 is: \n",
      " [ -1.81603605e-02  -7.98640343e-05  -3.67400786e-03   1.71112274e-03\n",
      "   1.48390932e-02   1.53804513e-02  -8.61794070e-03  -8.25316876e-03\n",
      "   2.43187276e-03  -1.36473828e-02  -2.67218912e-03  -1.21820051e-02\n",
      "   1.88596945e-03  -4.42251052e-03  -8.60761632e-03  -1.26954758e-02\n",
      "   1.57903052e-03  -2.99703277e-03  -1.64466721e-03   6.77323249e-03\n",
      "  -5.50063894e-03   3.63198711e-02   5.01071081e-03  -2.86694834e-03\n",
      "  -9.95452515e-04   2.95189072e-03  -3.16805725e-04   1.83151536e-04\n",
      "  -1.14040812e-03  -1.25274923e-02  -2.75897474e-03  -4.21450776e-03\n",
      "  -1.11924586e-02  -1.92250383e-02  -1.14922914e-02  -8.80778664e-03\n",
      "   3.64520533e-03   3.15743297e-03   1.18742325e-02   5.54224800e-03\n",
      "  -1.37347369e-02   8.52710798e-03   8.67091237e-03   9.97733666e-03\n",
      "  -1.94975203e-03  -1.26041952e-02   2.00802684e-02   1.34371753e-02\n",
      "   5.15501186e-03  -3.74741517e-04  -4.67234889e-03   3.07602874e-02\n",
      "   2.30080245e-03  -2.87045591e-02  -3.48755813e-03   2.71176680e-03\n",
      "  -1.48606029e-03  -2.58914957e-03  -5.77804336e-03   5.50666164e-03\n",
      "   8.03775273e-03   2.49497874e-02  -7.67074132e-03  -2.00209568e-02\n",
      "   2.22333086e-03  -2.57254929e-03  -1.51152451e-02   1.05938375e-03\n",
      "  -6.11456762e-03  -1.77091447e-03   3.39424960e-02  -4.11281598e-03\n",
      "  -2.44879823e-03   6.79139207e-03   7.31210810e-03  -6.77654212e-03\n",
      "   7.69584643e-04   2.99327341e-03   4.64054658e-03  -2.17742743e-03\n",
      "   9.90858459e-04   1.15875018e-02  -1.39107580e-02  -5.45285702e-03\n",
      "   1.42404792e-02  -5.00982242e-03  -1.63704637e-02   3.06261054e-03\n",
      "   1.01510971e-02   2.13846208e-03  -1.26532987e-03  -2.31503275e-03\n",
      "   1.76294188e-02  -2.43431816e-03  -1.56224648e-02   4.83756716e-03\n",
      "   3.39781741e-05   1.03622362e-02  -4.63396356e-02   3.07270014e-01\n",
      "   4.09950433e-02   2.16053632e-01  -3.90835973e-02   1.23855644e-01\n",
      "  -2.57490688e-02   3.07254689e-01   2.29822621e-02  -5.28712462e-02\n",
      "  -3.88588640e-02   3.62934163e-02   1.22073798e-01  -6.25371105e-02\n",
      "  -1.14655450e-01   7.92128660e-03   3.84065766e-02  -1.14567156e-02\n",
      "  -9.05601228e-03  -1.21310830e-03   5.19229537e-02   1.46707908e-01\n",
      "  -3.21263927e-02   7.95518117e-03] \n",
      "After Regularization with lambda 0.9:\n",
      " Train MSE:0.300798421244, Valid MSE:0.301904181456\n",
      "\n",
      "_________________________Model Training for run 2 with lambda:0.9_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.9 is: \n",
      " [  1.02472430e-02   8.84949819e-05  -2.90254029e-04  -5.44341133e-03\n",
      "   9.82851712e-03   2.20876964e-02  -1.31468398e-02  -9.01817222e-03\n",
      "   1.73845194e-02  -3.58527881e-02  -1.19715012e-02  -8.15983889e-03\n",
      "   5.82484018e-03  -3.86273022e-03  -2.03761994e-02   1.10528622e-03\n",
      "   1.99590237e-04   5.77354943e-03  -9.89840288e-03   1.83579735e-02\n",
      "  -5.70930327e-03   2.57334674e-02   1.06810030e-02  -3.87089601e-03\n",
      "  -7.52658201e-04   3.76336743e-03   2.63989347e-03  -4.31743843e-04\n",
      "  -2.04696259e-03  -1.39708078e-02  -2.16200577e-02  -3.75286427e-03\n",
      "  -4.28114106e-03  -2.02747626e-02  -1.57465193e-02  -9.70571162e-03\n",
      "   6.37011201e-03   9.93906533e-03  -2.71503456e-03  -3.92302799e-03\n",
      "  -1.97873334e-02   2.13394270e-02   1.12542826e-02   2.91445223e-03\n",
      "  -6.24153626e-03  -2.35005038e-02   2.92254599e-02   2.54701436e-02\n",
      "   1.00797806e-02   3.28042719e-03  -4.09647136e-03   4.25720669e-02\n",
      "   1.71360327e-02  -2.43707365e-03  -3.29010728e-03  -2.83217149e-03\n",
      "   4.64172446e-04   3.96255270e-04  -4.68282775e-03  -8.40322178e-03\n",
      "   1.50378929e-02   2.00058759e-02   3.73781890e-03  -2.25076808e-02\n",
      "   1.13025553e-02  -2.25860231e-03  -6.16118027e-03  -1.08243271e-02\n",
      "   2.82216108e-03  -1.20945872e-02   2.48739986e-02  -8.13768127e-03\n",
      "  -4.78642844e-03   5.31569265e-03   4.92752609e-03  -7.29729838e-03\n",
      "   2.17800148e-03   1.72217596e-03   4.25426120e-03   1.93508845e-02\n",
      "  -5.74183395e-03   8.73634873e-03  -1.77470594e-03  -2.64109809e-03\n",
      "   1.45258944e-02  -5.40562809e-03  -1.59911656e-02  -4.02151244e-04\n",
      "   1.17545713e-02  -4.86351265e-03   5.66390736e-03  -2.76763203e-02\n",
      "   1.97044736e-02   1.51615482e-02  -1.85245733e-02  -8.33712981e-03\n",
      "   2.97644771e-03   1.94508379e-02  -7.97680080e-02   2.68589308e-01\n",
      "   8.11337806e-02   2.53348848e-01  -1.98548577e-03   9.07062865e-02\n",
      "  -2.64607414e-02   2.68586798e-01   1.76244736e-02  -9.43176857e-02\n",
      "  -6.30537067e-02   1.67438441e-02   1.04108422e-01  -1.03163638e-01\n",
      "  -7.08027354e-02  -6.41841651e-03   4.47224766e-02  -1.20905722e-02\n",
      "  -6.81228029e-03  -2.10948433e-03   4.01392969e-02   1.69864437e-01\n",
      "  -4.56689884e-02   1.35799800e-02] \n",
      "After Regularization with lambda 0.9:\n",
      " Train MSE:0.291107815029, Valid MSE:0.291087824868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 3 with lambda:0.9_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.9 is: \n",
      " [  1.41359982e-02   3.26314711e-06   2.21754860e-03  -5.91048700e-03\n",
      "   1.15413084e-02   2.03513639e-02  -9.48333511e-03  -6.50260222e-03\n",
      "   1.03735874e-02  -2.33269223e-02  -1.38683624e-02  -1.59042959e-02\n",
      "   5.04634638e-03  -4.44660052e-03  -2.66137898e-02   4.01480778e-03\n",
      "   2.26056854e-03   7.09521464e-03  -7.61957449e-03   1.87453759e-02\n",
      "   2.57831535e-03   2.88369709e-02   1.42160114e-02  -7.14658286e-03\n",
      "  -2.78525652e-03   6.21546214e-03   3.01578023e-03  -5.22133653e-04\n",
      "  -4.92307978e-03   3.36678529e-03  -2.04542921e-02  -3.26631678e-03\n",
      "  -8.09052895e-03  -2.22229432e-02  -1.61751559e-02  -7.81419698e-03\n",
      "   9.59943672e-03   1.20240669e-02  -6.99625700e-04   1.17634200e-03\n",
      "  -3.04911887e-02   9.40681195e-03   7.50086448e-03   2.36568408e-03\n",
      "  -6.41362828e-03  -2.16237759e-02   1.41793354e-02   2.26776202e-02\n",
      "   7.67227046e-03   1.40960614e-03  -3.87682341e-03   1.21591467e-02\n",
      "   1.08516606e-02  -1.86453399e-02  -8.52839637e-03   2.00761711e-04\n",
      "   2.63804716e-03  -1.26100453e-03  -1.13849153e-02  -2.54607255e-03\n",
      "   2.60300404e-02   1.98618920e-02   6.38564929e-03  -2.50445918e-02\n",
      "   5.86591570e-03  -2.75690588e-03  -1.26913276e-02  -5.27181539e-03\n",
      "   4.22843794e-03  -7.72901846e-03   3.43710981e-02   7.32099225e-04\n",
      "  -4.70793226e-03  -1.08666983e-02   2.63873739e-04  -1.88173926e-03\n",
      "   9.27647429e-03  -2.54151288e-03   4.79290607e-03   1.37726550e-02\n",
      "  -4.07815944e-03   1.85340706e-02  -9.89151223e-04  -1.08359228e-02\n",
      "   1.25078916e-02  -4.84124680e-03  -1.64565327e-02  -8.72804991e-05\n",
      "   8.07109924e-03  -1.66959830e-03   6.63086001e-03  -6.44028072e-03\n",
      "   1.12411758e-02   4.91492569e-03  -1.58144826e-02  -6.62853971e-03\n",
      "  -4.91735094e-04   1.91494880e-02  -7.63327937e-02   2.86180623e-01\n",
      "   8.38454676e-02   2.43288802e-01  -9.14394243e-04   1.31417429e-01\n",
      "  -2.31454306e-02   2.86068889e-01   1.13378897e-02  -1.19022223e-01\n",
      "  -7.47966833e-02   1.48280216e-02   8.80536036e-02  -9.07795849e-02\n",
      "  -1.29647407e-01   1.20785560e-02   3.11885810e-02  -1.19507371e-02\n",
      "  -2.94711319e-03  -1.03003024e-03   4.91257457e-02   1.99793831e-01\n",
      "  -3.27518885e-02   9.43264610e-03] \n",
      "After Regularization with lambda 0.9:\n",
      " Train MSE:0.330183172279, Valid MSE:0.329760174526\n",
      "\n",
      "_________________________Model Training for run 4 with lambda:0.9_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 0.9 is: \n",
      " [  9.72170760e-03  -9.39438045e-05   6.12171990e-03  -1.41286156e-02\n",
      "   1.43457818e-02   1.66325139e-02  -7.15833544e-03  -4.40536583e-03\n",
      "  -1.24938712e-03  -1.53367796e-02  -5.19354580e-03  -1.71745004e-02\n",
      "   1.15271323e-02  -5.68155287e-03  -6.84628176e-03   1.52711582e-03\n",
      "   2.00306262e-03   1.58459824e-02  -1.07803377e-02   1.03083605e-02\n",
      "   3.64412612e-03   1.02539815e-02   1.25449640e-02  -6.76088020e-03\n",
      "  -4.74578649e-03   5.77914936e-03   1.13931755e-03  -1.53183417e-03\n",
      "  -6.75521397e-03  -2.65431728e-02  -2.33592633e-02  -4.49413582e-03\n",
      "  -7.03662485e-03  -2.06177868e-02  -1.30782059e-02  -1.29243695e-02\n",
      "   7.45396321e-03   1.26512157e-02  -4.50935645e-03  -4.47466125e-03\n",
      "  -2.60046976e-02   1.20077453e-02   7.59430175e-03   6.16290871e-04\n",
      "  -1.16145434e-03  -1.72539715e-02   8.96402089e-03   1.57602524e-02\n",
      "   5.40057436e-03   9.92254983e-04  -3.35083215e-03   6.25857954e-03\n",
      "   5.20940927e-03  -7.84165049e-03  -1.01918790e-02   2.03603055e-03\n",
      "   2.12885932e-03   3.15718345e-03   6.94690402e-04  -3.97046052e-03\n",
      "   9.48835819e-03   9.22908531e-03   9.10088179e-03  -7.46969976e-03\n",
      "   5.99416932e-03  -8.19206903e-03  -4.06335253e-03  -3.09552054e-03\n",
      "   6.19656092e-03  -7.94897744e-03   3.12002201e-02   3.67079974e-03\n",
      "  -3.66994537e-03  -2.00275256e-05   4.08682469e-03   8.09415720e-03\n",
      "   9.26077436e-03  -2.54683067e-03   3.52754086e-03   1.13904828e-02\n",
      "  -3.75547127e-03  -3.39673550e-03  -6.06460046e-03   1.00674437e-02\n",
      "   9.44312439e-03  -5.53625295e-03  -1.00448556e-02  -3.58009645e-03\n",
      "   1.05128446e-02   1.88715528e-03   7.40828602e-03  -1.64737608e-02\n",
      "   2.15370792e-02   3.77690059e-03  -1.20886914e-02  -1.30596899e-02\n",
      "   6.19033752e-03   1.31824282e-02  -7.27569333e-02   2.91415463e-01\n",
      "   8.17799585e-02   2.37838710e-01  -1.84534010e-02   1.04698395e-01\n",
      "  -1.68842852e-02   2.91404653e-01  -2.70354971e-04  -9.56241991e-02\n",
      "  -6.07981944e-02   1.15381440e-02   8.22299536e-02  -8.64318316e-02\n",
      "  -9.86352320e-02   1.18857840e-02   4.70840986e-02  -1.66055841e-02\n",
      "  -2.19689150e-04   1.10383928e-03   7.61618858e-02   1.73481456e-01\n",
      "  -4.67117346e-02   7.50511611e-03] \n",
      "After Regularization with lambda 0.9:\n",
      " Train MSE:0.304980751655, Valid MSE:0.304992743981\n",
      "Average validation error over 5 runs 0.180441632493 with lambda:0.9\n",
      "Average training error over 5 runs 0.18012901915 with lambda:0.9\n",
      "\n",
      "_________________________Model Training for run 0 with lambda:1_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 1 is: \n",
      " [  3.08545190e-02  -7.77497877e-05   2.31227445e-04  -1.05982156e-02\n",
      "   1.67388628e-02   2.78575279e-02  -1.07288903e-02   4.95659440e-03\n",
      "   1.51965444e-02  -2.13138680e-02  -9.47562443e-03  -1.56625153e-03\n",
      "   6.39771157e-03  -2.41347723e-03  -2.41666676e-02   6.89568195e-03\n",
      "   3.88781952e-03   1.39268097e-02  -1.65582449e-02   2.26862741e-02\n",
      "  -2.68579083e-04   2.55781952e-02   1.74526973e-02  -1.15942489e-02\n",
      "   1.25709117e-04   2.35213518e-03   2.11692412e-03  -1.04538182e-03\n",
      "  -1.02030749e-03   1.16325170e-03  -2.45949431e-02  -6.82482075e-03\n",
      "  -2.84375363e-03  -2.09235225e-02  -1.65724181e-02  -7.73310517e-03\n",
      "   4.93378797e-03   1.38897016e-02   1.03559227e-03   3.33414697e-03\n",
      "  -2.67130200e-02   9.85345669e-03   8.04057231e-03   3.08255801e-03\n",
      "   5.74344866e-03  -1.63894857e-02   8.42530461e-03   2.12380152e-02\n",
      "   3.62082757e-03   3.10745934e-03  -6.56592035e-03   6.06680504e-03\n",
      "   1.48258862e-02  -3.81720015e-03  -8.20957893e-03   1.80643522e-03\n",
      "   1.44157887e-03  -4.36205105e-03  -1.74555739e-02   5.63497505e-04\n",
      "   2.20822715e-02   2.19728758e-02   1.16192626e-02  -2.29397650e-02\n",
      "   1.08317175e-02  -1.43571192e-02  -1.16859085e-02  -1.37879888e-03\n",
      "   6.44129446e-03  -7.09958489e-03   2.56154710e-02   2.53897270e-03\n",
      "  -3.00559560e-03  -5.11993413e-03   1.54894023e-03  -3.87831122e-03\n",
      "   6.96927703e-03  -1.03769295e-05  -2.59999987e-03   1.82702255e-02\n",
      "  -5.59686953e-03   1.24353316e-02  -1.85939970e-03  -8.69272815e-03\n",
      "   7.14260185e-03  -1.94571969e-03  -1.51213095e-02  -1.36969366e-03\n",
      "   9.15212199e-03   7.98543640e-03   3.78464866e-03  -3.09950522e-02\n",
      "   2.43391581e-02   1.42378129e-02  -1.13571343e-02  -3.99325908e-03\n",
      "   1.17903205e-03   8.63976645e-03  -4.69982641e-02   2.57748449e-01\n",
      "   8.47286238e-02   2.19608511e-01  -3.34916299e-02   1.87572121e-01\n",
      "  -5.39324959e-02   2.57655848e-01   2.59602854e-03  -1.28961395e-01\n",
      "  -7.67669886e-02  -2.47620226e-04   9.78276355e-02  -1.02202299e-01\n",
      "  -1.00482042e-01  -3.13797774e-03   3.70495213e-02  -6.50864001e-03\n",
      "  -5.36166564e-03  -3.78000748e-03   6.11050390e-02   1.79804697e-01\n",
      "  -4.38275168e-02   1.39833265e-02] \n",
      "After Regularization with lambda 1:\n",
      " Train MSE:0.338483011073, Valid MSE:0.338775330531\n",
      "\n",
      "_________________________Model Training for run 1 with lambda:1_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 1 is: \n",
      " [ -1.61384577e-02  -7.99673167e-05  -3.81172315e-03   1.47931612e-03\n",
      "   1.46265572e-02   1.51062279e-02  -8.54516830e-03  -7.97748750e-03\n",
      "   2.26498294e-03  -1.28286839e-02  -2.83110034e-03  -1.15862759e-02\n",
      "   1.27662168e-03  -4.45963702e-03  -7.50922271e-03  -1.24844331e-02\n",
      "   1.62202355e-03  -2.59629826e-03  -1.69835680e-03   6.80690437e-03\n",
      "  -5.54746178e-03   3.45455299e-02   5.14313031e-03  -2.79286689e-03\n",
      "  -1.09942077e-03   3.01659156e-03  -1.77837970e-04   2.00305580e-04\n",
      "  -1.09016922e-03  -1.13703810e-02  -3.06652657e-03  -4.26686964e-03\n",
      "  -1.09468079e-02  -1.84867135e-02  -1.16369889e-02  -8.82338254e-03\n",
      "   3.63368602e-03   3.17109176e-03   1.16761567e-02   4.98212941e-03\n",
      "  -1.29950296e-02   8.54459172e-03   8.80726093e-03   9.43739559e-03\n",
      "  -1.95712277e-03  -1.15584630e-02   1.86373744e-02   1.32476107e-02\n",
      "   4.97012547e-03  -4.42588286e-04  -4.75289006e-03   2.78921009e-02\n",
      "   2.20883127e-03  -2.70007919e-02  -3.61520799e-03   2.58269694e-03\n",
      "  -1.46302410e-03  -2.42327651e-03  -5.09904740e-03   5.42568291e-03\n",
      "   8.08313378e-03   2.37704450e-02  -7.06239541e-03  -1.93993886e-02\n",
      "   2.33054509e-03  -2.62789413e-03  -1.40898503e-02   1.17996474e-03\n",
      "  -6.13049394e-03  -1.67773688e-03   3.28143576e-02  -3.78778039e-03\n",
      "  -2.38270971e-03   6.61085163e-03   7.23170680e-03  -6.68011231e-03\n",
      "   7.76725066e-04   2.94025526e-03   4.58531251e-03  -2.29915805e-03\n",
      "   9.71122325e-04   1.06987272e-02  -1.28254462e-02  -5.61080254e-03\n",
      "   1.38499749e-02  -4.56431029e-03  -1.60583581e-02   2.52796456e-03\n",
      "   1.02310092e-02   2.07519694e-03  -1.24112942e-03  -1.53902809e-03\n",
      "   1.75648358e-02  -1.88905900e-03  -1.56703085e-02   4.51119745e-03\n",
      "   1.63476527e-04   1.03202728e-02  -4.27492611e-02   3.02557085e-01\n",
      "   3.89152567e-02   2.17822045e-01  -3.66301904e-02   1.26056130e-01\n",
      "  -2.73276366e-02   3.02538017e-01   2.22958141e-02  -5.02233223e-02\n",
      "  -3.96118890e-02   3.45998398e-02   1.20866720e-01  -5.87488665e-02\n",
      "  -1.07674018e-01   7.94786613e-03   3.85526428e-02  -1.18858683e-02\n",
      "  -9.11461961e-03  -1.14579016e-03   5.22259573e-02   1.34773660e-01\n",
      "  -3.38144239e-02   8.05046851e-03] \n",
      "After Regularization with lambda 1:\n",
      " Train MSE:0.322274955516, Valid MSE:0.323378214135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________Model Training for run 2 with lambda:1_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 1 is: \n",
      " [  1.08475060e-02   9.44036037e-05  -4.82862774e-04  -5.26545959e-03\n",
      "   9.69412569e-03   2.21139305e-02  -1.29006976e-02  -8.79469722e-03\n",
      "   1.66245867e-02  -3.37021071e-02  -1.20931937e-02  -7.48782172e-03\n",
      "   5.12057323e-03  -3.89936819e-03  -1.87158261e-02   9.04030863e-04\n",
      "   2.62226073e-04   5.97387850e-03  -9.44031075e-03   1.81626867e-02\n",
      "  -5.68444030e-03   2.43439419e-02   1.04810479e-02  -3.44982832e-03\n",
      "  -8.08824659e-04   3.82800084e-03   2.73058309e-03  -3.75628430e-04\n",
      "  -2.07219251e-03  -1.29054869e-02  -2.11788077e-02  -3.79445713e-03\n",
      "  -4.31737381e-03  -1.98347940e-02  -1.58713985e-02  -9.39331838e-03\n",
      "   6.29403584e-03   9.83882539e-03  -2.60169513e-03  -3.91345794e-03\n",
      "  -1.87976052e-02   2.10647221e-02   1.11283058e-02   2.75271455e-03\n",
      "  -5.67597319e-03  -2.19569803e-02   2.74141157e-02   2.52146196e-02\n",
      "   9.98920701e-03   3.24229427e-03  -4.12670305e-03   3.95083886e-02\n",
      "   1.71647750e-02  -1.13314940e-03  -3.28799668e-03  -3.15163581e-03\n",
      "   5.03934748e-04   4.58449296e-04  -4.26839982e-03  -7.65328788e-03\n",
      "   1.43769039e-02   1.93625530e-02   4.25147636e-03  -2.18279283e-02\n",
      "   1.09480417e-02  -2.30046886e-03  -5.98365821e-03  -1.02719460e-02\n",
      "   2.99022637e-03  -1.17959877e-02   2.40063561e-02  -7.57351168e-03\n",
      "  -4.65945132e-03   5.45619225e-03   4.95726330e-03  -7.37746469e-03\n",
      "   2.05720652e-03   1.71432896e-03   4.33444163e-03   1.90337424e-02\n",
      "  -5.74922297e-03   7.80751539e-03  -1.67124815e-03  -2.06802298e-03\n",
      "   1.41134669e-02  -5.19888201e-03  -1.57275244e-02  -6.01840513e-04\n",
      "   1.18569376e-02  -4.79913083e-03   5.60994403e-03  -2.65259085e-02\n",
      "   1.96350091e-02   1.52774095e-02  -1.84782346e-02  -8.26558396e-03\n",
      "   3.18338025e-03   1.92829124e-02  -7.21749706e-02   2.65675463e-01\n",
      "   7.58238520e-02   2.50221411e-01  -1.71346680e-03   9.35979941e-02\n",
      "  -2.79824975e-02   2.65664679e-01   1.68791619e-02  -8.93070109e-02\n",
      "  -6.32874965e-02   1.59251191e-02   1.03110404e-01  -9.75923311e-02\n",
      "  -6.63498071e-02  -6.30633035e-03   4.50314118e-02  -1.24191231e-02\n",
      "  -6.74463604e-03  -2.09036023e-03   3.97472902e-02   1.56491266e-01\n",
      "  -4.69164661e-02   1.35162861e-02] \n",
      "After Regularization with lambda 1:\n",
      " Train MSE:0.308987616856, Valid MSE:0.308974876045\n",
      "\n",
      "_________________________Model Training for run 3 with lambda:1_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 1 is: \n",
      " [  1.42833120e-02   9.85567318e-06   1.77533641e-03  -5.81503206e-03\n",
      "   1.13063015e-02   2.01933921e-02  -9.34014433e-03  -6.20466301e-03\n",
      "   9.65136871e-03  -2.19035832e-02  -1.37517420e-02  -1.49459282e-02\n",
      "   4.43364348e-03  -4.46342709e-03  -2.45432185e-02   3.84361283e-03\n",
      "   2.36860941e-03   7.31387418e-03  -7.23512525e-03   1.86659921e-02\n",
      "   2.50098965e-03   2.73326408e-02   1.36385303e-02  -6.81727297e-03\n",
      "  -2.82541943e-03   6.24767354e-03   3.16888498e-03  -4.82125609e-04\n",
      "  -4.89769522e-03   3.30161632e-03  -1.99619880e-02  -3.48747951e-03\n",
      "  -7.99047935e-03  -2.14465565e-02  -1.61641364e-02  -7.50414834e-03\n",
      "   9.49192117e-03   1.19847953e-02  -5.78381843e-04   8.56095317e-04\n",
      "  -2.92327434e-02   9.36594406e-03   7.62071644e-03   1.90919754e-03\n",
      "  -6.17648765e-03  -2.00970830e-02   1.28359776e-02   2.23813041e-02\n",
      "   7.34142539e-03   1.34745096e-03  -3.84331848e-03   1.07861910e-02\n",
      "   1.06791603e-02  -1.67276966e-02  -8.62222228e-03   1.24884674e-05\n",
      "   2.72053115e-03  -1.19299141e-03  -1.04495127e-02  -2.07148321e-03\n",
      "   2.47179928e-02   1.94193464e-02   7.02091921e-03  -2.43900317e-02\n",
      "   5.66350597e-03  -2.83346688e-03  -1.18719086e-02  -4.89074049e-03\n",
      "   4.37928782e-03  -7.28406027e-03   3.33348789e-02   1.11595579e-03\n",
      "  -4.64557983e-03  -1.03289979e-02   3.89007177e-04  -2.24707514e-03\n",
      "   9.26767826e-03  -2.57378208e-03   4.90759546e-03   1.34840122e-02\n",
      "  -4.01394529e-03   1.75756194e-02  -7.47536304e-04  -1.02913704e-02\n",
      "   1.21801807e-02  -4.50371682e-03  -1.62500990e-02  -4.36704411e-04\n",
      "   8.14394478e-03  -1.80330324e-03   6.58142158e-03  -5.89949583e-03\n",
      "   1.12966856e-02   5.31252746e-03  -1.57479147e-02  -6.65537643e-03\n",
      "  -3.60350970e-04   1.89728471e-02  -6.95811901e-02   2.82914884e-01\n",
      "   7.93316078e-02   2.42245755e-01  -1.18364715e-03   1.34008570e-01\n",
      "  -2.42373452e-02   2.82805713e-01   1.04466778e-02  -1.14296531e-01\n",
      "  -7.47284400e-02   1.46591497e-02   8.71749751e-02  -8.63984625e-02\n",
      "  -1.21529590e-01   1.19440754e-02   3.14522774e-02  -1.24632596e-02\n",
      "  -2.96835800e-03  -9.62709005e-04   4.92016130e-02   1.83776862e-01\n",
      "  -3.41510456e-02   9.38508890e-03] \n",
      "After Regularization with lambda 1:\n",
      " Train MSE:0.350433818453, Valid MSE:0.350002223314\n",
      "\n",
      "_________________________Model Training for run 4 with lambda:1_____________________\n",
      "\n",
      "\n",
      "Parameters W_ridge after L2 regularization corresponding to Lambda 1 is: \n",
      " [  1.04323682e-02  -8.79708087e-05   5.69586237e-03  -1.35436360e-02\n",
      "   1.40710226e-02   1.65368940e-02  -7.03719472e-03  -4.04903452e-03\n",
      "  -1.32117012e-03  -1.43113083e-02  -5.47298579e-03  -1.63873447e-02\n",
      "   1.06076821e-02  -5.68441612e-03  -6.16825502e-03   1.71410597e-03\n",
      "   2.04441610e-03   1.58888295e-02  -1.03209050e-02   1.03235221e-02\n",
      "   3.80579437e-03   9.74327342e-03   1.20877121e-02  -6.15411138e-03\n",
      "  -4.82468939e-03   5.85546903e-03   1.29086357e-03  -1.54642381e-03\n",
      "  -6.76927599e-03  -2.51830430e-02  -2.29837446e-02  -4.59245070e-03\n",
      "  -6.93602181e-03  -2.00836218e-02  -1.31315916e-02  -1.23021533e-02\n",
      "   7.39161092e-03   1.26369738e-02  -4.36576816e-03  -4.52435045e-03\n",
      "  -2.49328688e-02   1.17643849e-02   7.55525904e-03   3.27453232e-04\n",
      "  -1.10576471e-03  -1.62036053e-02   7.99741009e-03   1.56006379e-02\n",
      "   5.24899978e-03   9.29138401e-04  -3.42454916e-03   3.99365275e-03\n",
      "   5.35068169e-03  -6.33791095e-03  -1.01363665e-02   1.77557676e-03\n",
      "   2.30335739e-03   3.06282798e-03   8.21093185e-04  -3.47790135e-03\n",
      "   9.08123095e-03   9.12105234e-03   9.56190581e-03  -7.21289992e-03\n",
      "   5.63881780e-03  -7.83975836e-03  -3.83653749e-03  -3.16177182e-03\n",
      "   6.34025075e-03  -7.18537148e-03   3.04072761e-02   3.99681530e-03\n",
      "  -3.60200037e-03  -2.39642678e-04   4.06265060e-03   7.41721505e-03\n",
      "   9.26757907e-03  -2.55248799e-03   3.69183731e-03   1.11188573e-02\n",
      "  -3.74454604e-03  -3.48613417e-03  -5.52584478e-03   9.49326419e-03\n",
      "   9.21010307e-03  -5.37534716e-03  -9.98581530e-03  -3.74215338e-03\n",
      "   1.06212377e-02   1.91487920e-03   7.37951844e-03  -1.54653353e-02\n",
      "   2.15282808e-02   3.83741577e-03  -1.20556215e-02  -1.28725343e-02\n",
      "   6.11563348e-03   1.30855736e-02  -6.61298454e-02   2.87883435e-01\n",
      "   7.69438616e-02   2.36809500e-01  -1.71756366e-02   1.08230782e-01\n",
      "  -1.88909129e-02   2.87873026e-01  -9.56263128e-04  -9.13750213e-02\n",
      "  -6.07860537e-02   1.11161993e-02   8.14399143e-02  -8.22774760e-02\n",
      "  -9.21538591e-02   1.19220254e-02   4.73105351e-02  -1.66677169e-02\n",
      "  -1.49361672e-04   1.15884933e-03   7.51891266e-02   1.59701041e-01\n",
      "  -4.79171613e-02   7.60632873e-03] \n",
      "After Regularization with lambda 1:\n",
      " Train MSE:0.325256069123, Valid MSE:0.325268144123\n",
      "Average validation error over 5 runs 0.192844809588 with lambda:1\n",
      "Average training error over 5 runs 0.192542192071 with lambda:1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnIAAAHwCAYAAAAivoLbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11158c9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X,Y = load_data('hwk1_datasets/Datasets/communities.data.txt')\n",
    "new_X = fill_data(X)\n",
    "split_data(new_X,5)\n",
    "model_1 = LinearRegression(X,Y)\n",
    "valid_mse = []\n",
    "train_mse = []\n",
    "colors=['r', 'c', 'g', 'k', '#F9690E']\n",
    "\n",
    "for i in range(5):\n",
    "    valid_x, valid_y = load_new_data(\"hwk1_datasets/Datasets/Q3.2_dataset/k{}/CandC_valid{}.csv\".format(i,i))\n",
    "    train_x, train_y = load_new_data(\"hwk1_datasets/Datasets/Q3.2_dataset/k{}/CandC_train{}.csv\".format(i,i))\n",
    "    print \"\\n_________________________Model Training for run {}_____________________\\n\".format(i)\n",
    "    t_mse,v_mse = model_1.train_closeform(train_x, train_y, valid_x, valid_y)\n",
    "    print \"\\n\\n______________________________________________________________________________________\"\n",
    "    valid_mse.append(v_mse)\n",
    "    train_mse.append(t_mse)\n",
    "ax, fig = plt.subplots(0,figsize=(10,8))\n",
    "x_range = np.arange(5)\n",
    "plt.plot(x_range, valid_mse, color='b', label=\"validation mse\")\n",
    "plt.plot(x_range, train_mse, color='c', label=\"training mse\")\n",
    "plt.ylabel('trining | validation MSE')\n",
    "plt.xlabel('runs')\n",
    "plt.title('Learning Curve')\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.show()\n",
    "    \n",
    "print \"\\n\\n_________________________________________________________________________________________\"\n",
    "print \"\\n_______The Average of Training MSE of 5 folds cross validation is: {}\" .format(np.mean(train_mse))\n",
    "print \"\\n_______The Average of Validation MSE of 5 folds cross validation is: {}\" .format(np.mean(valid_mse))\n",
    "\n",
    "print \"\\n\\n__________________________________Ridge Regularization____________________________________________\"\n",
    "model_2 = RidgeReg(new_X,Y)\n",
    "lambda_ = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "vmse_lambda = []\n",
    "tmse_lambda = []\n",
    "for j in range(len(lambda_)):\n",
    "    for i in range(5):\n",
    "        valid_x, valid_y = load_new_data(\"hwk1_datasets/Datasets/Q3.2_dataset/k{}/CandC_valid{}.csv\".format(i,i))\n",
    "        train_x, train_y = load_new_data(\"hwk1_datasets/Datasets/Q3.2_dataset/k{}/CandC_train{}.csv\".format(i,i))   \n",
    "        print \"\\n_________________________Model Training for run {} with lambda:{}_____________________\\n\".format(i,lambda_[j])\n",
    "        model_2.l2_regularization(train_x,train_y,lambda_[j])\n",
    "        t_mse,v_mse = model_2.compute_reg_loss(train_x, train_y, valid_x, valid_y,lambda_[j])\n",
    "        valid_mse.append(v_mse)\n",
    "        train_mse.append(t_mse)\n",
    "    vmse_lambda.append(np.mean(valid_mse))\n",
    "    tmse_lambda.append(np.mean(train_mse))\n",
    "    print \"Average validation error over 5 runs {} with lambda:{}\".format(np.mean(valid_mse),lambda_[j])\n",
    "    print \"Average training error over 5 runs {} with lambda:{}\".format(np.mean(train_mse),lambda_[j])\n",
    "ax, fig = plt.subplots(0,figsize=(10,8))   \n",
    "plt.plot(lambda_, vmse_lambda, color='y', label=\"Average validation mse with different lambda\",linewidth=4.0 )\n",
    "plt.plot(lambda_, tmse_lambda, color='b', label=\"Average training mse with different lambda\", linewidth=2.0)\n",
    "plt.ylabel('training | validation MSE')\n",
    "plt.xlabel('lambda')\n",
    "plt.title('Learning Curve')\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
